---
layout: post
comments: true
title: "PyTorch"
date: 2022-02-09 01:09:00

---

> 这是关于PyTorch的学习内容.


<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 总览

首先列出PyTorch的参考资料：

1. PyTorch官网：https://pytorch.org/
这里包含了tutorial、API文档等等。
2. 一个不错的介绍PyTorch的博客：https://github.com/jcjohnson/pytorch-examples
3. PyTorch源码的Github页：https://github.com/pytorch/pytorch
在PyTorch官网上也可以找到网页版的源码介绍，是一模一样的。
4. 这里开了个介绍PyTorch源码的博客：https://zhuanlan.zhihu.com/p/328674159
5. 这是PyTorch的一个作者写的介绍内部机理的博客：http://blog.ezyang.com/2019/05/pytorch-internals/


本post不会重复参考资料里的内容，但可能会对其进行补充。本post主要会对重要的PyTorch内容进行补充介绍，其中可能会综合汇总其他人博客里的内容（会标明出处），以达到对PyTorch常用内容可以迅速复习的目的。

本post内容会与post：CUDA学习的内容联动。

## 0. PyTorch优雅模板

### 0.1 版本一：

```python
### 一、导入包以及设置随机种子
import numpy as np
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

import random
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)

### 二、以类的方式定义超参数
class argparse():
    pass

args = argparse()
args.epochs, args.learning_rate, args.patience = [30, 0.001, 4]
args.hidden_size, args.input_size= [40, 30]
args.device, = [torch.device("cuda:0" if torch.cuda.is_available() else "cpu"),]

### 三、定义自己的模型
class Your_model(nn.Module):
    def __init__(self):
        super(Your_model, self).__init__()
        pass
        
    def forward(self,x):
        pass
        return x

### 四、定义早停类(此步骤可以省略)
class EarlyStopping():
    def __init__(self,patience=7,verbose=False,delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta
    def __call__(self,val_loss,model,path):
        print("val_loss={}".format(val_loss))
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss,model,path)
        elif score < self.best_score+self.delta:
            self.counter+=1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter>=self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss,model,path)
            self.counter = 0
    def save_checkpoint(self,val_loss,model,path):
        if self.verbose:
            print(
                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), path+'/'+'model_checkpoint.pth')
        self.val_loss_min = val_loss

### 五、定义自己的数据集Dataset,DataLoader
class Dataset_name(Dataset):
    def __init__(self, flag='train'):
        assert flag in ['train', 'test', 'valid']
        self.flag = flag
        self.__load_data__()

    def __getitem__(self, index):
        pass
    def __len__(self):
        pass

    def __load_data__(self, csv_paths: list):
        pass
        print(
            "train_X.shape:{}\ntrain_Y.shape:{}\nvalid_X.shape:{}\nvalid_Y.shape:{}\n"
            .format(self.train_X.shape, self.train_Y.shape, self.valid_X.shape, self.valid_Y.shape))

train_dataset = Dataset_name(flag='train')
train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
valid_dataset = Dataset_name(flag='valid')
valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=True)

### 六、实例化模型，设置loss，优化器等
model = Your_model().to(args.device)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(Your_model.parameters(),lr=args.learning_rate)

train_loss = []
valid_loss = []
train_epochs_loss = []
valid_epochs_loss = []

early_stopping = EarlyStopping(patience=args.patience,verbose=True)

### 七、开始训练以及调整lr
for epoch in range(args.epochs):
    Your_model.train()
    train_epoch_loss = []
    for idx,(data_x,data_y) in enumerate(train_dataloader,0):
        data_x = data_x.to(torch.float32).to(args.device)
        data_y = data_y.to(torch.float32).to(args.device)
        outputs = Your_model(data_x)
        optimizer.zero_grad()
        loss = criterion(data_y,outputs)
        loss.backward()
        optimizer.step()
        train_epoch_loss.append(loss.item())
        train_loss.append(loss.item())
        if idx%(len(train_dataloader)//2)==0:
            print("epoch={}/{},{}/{}of train, loss={}".format(
                epoch, args.epochs, idx, len(train_dataloader),loss.item()))
    train_epochs_loss.append(np.average(train_epoch_loss))
    
    #=====================valid============================
    Your_model.eval()
    valid_epoch_loss = []
    for idx,(data_x,data_y) in enumerate(valid_dataloader,0):
        data_x = data_x.to(torch.float32).to(args.device)
        data_y = data_y.to(torch.float32).to(args.device)
        outputs = Your_model(data_x)
        loss = criterion(data_y,outputs)
        valid_epoch_loss.append(loss.item())
        valid_loss.append(loss.item())
    valid_epochs_loss.append(np.average(valid_epoch_loss))
    #==================early stopping======================
    early_stopping(valid_epochs_loss[-1],model=Your_model,path=r'c:\\your_model_to_save')
    if early_stopping.early_stop:
        print("Early stopping")
        break
    #====================adjust lr========================
    lr_adjust = {
            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,
            10: 5e-7, 15: 1e-7, 20: 5e-8
        }
    if epoch in lr_adjust.keys():
        lr = lr_adjust[epoch]
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        print('Updating learning rate to {}'.format(lr))

### 八、绘图
plt.figure(figsize=(12,4))
plt.subplot(121)
plt.plot(train_loss[:])
plt.title("train_loss")
plt.subplot(122)
plt.plot(train_epochs_loss[1:],'-o',label="train_loss")
plt.plot(valid_epochs_loss[1:],'-o',label="valid_loss")
plt.title("epochs_loss")
plt.legend()
plt.show()

### 九、预测
# 此处可定义一个预测集的Dataloader。也可以直接将你的预测数据reshape,添加batch_size=1
Your_model.eval()
predict = Your_model(data)
```

下面是一个完整的例子，以18个数训练了一个分类网络，判断一个数字是否大于8（在dataset中设置），具有完整的训练和预测流程。网络是最简单的全连接，输入为1，输出为2（2分类）。

```python
import random

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# 设置随机数种子保证论文可复现
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
torch.cuda.manual_seed_all(seed)

# 以类的方式定义参数，还有很多方法，config文件等等
class Args:
    def __init__(self) -> None:
        self.batch_size = 1
        self.lr = 0.001
        self.epochs = 10
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.data_train = np.array([-2, -1, 0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18, 20])
        self.data_val = np.array([15, 16, 17, 0.1, -3, -4])


args = Args()

# 定义一个简单的全连接
class Net(nn.Module):
    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):
        super().__init__()
        self.layer1 = nn.Sequential(
            nn.Linear(in_dim, n_hidden_1), nn.ReLU(True))
        self.layer2 = nn.Sequential(
            nn.Linear(n_hidden_1, n_hidden_2), nn.ReLU(True))
        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim))

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return x


# 定义数据集，判断一个数字是否大于8
class Dataset_num(Dataset):
    def __init__(self, flag='train') -> None:
        self.flag = flag
        assert self.flag in ['train', 'val'], 'not implement!'

        if self.flag == 'train':
            self.data = args.data_train
        else:
            self.data = args.data_val

    def __getitem__(self, index: int):
        val = self.data[index]

        if val > 8:
            label = 1
        else:
            label = 0

        return torch.tensor(label, dtype=torch.long), torch.tensor([val], dtype=torch.float32)

    def __len__(self) -> int:
        return len(self.data)


def train():
    train_dataset = Dataset_num(flag='train')
    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)
    val_dataset = Dataset_num(flag='val')
    val_dataloader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=True)

    model = Net(1, 32, 16, 2).to(args.device) # 网路参数设置，输入为1，输出为2，即判断一个数是否大于8
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  # , eps=1e-8)

    train_epochs_loss = []
    valid_epochs_loss = []
    train_acc = []
    val_acc = []

    for epoch in range(args.epochs):
        model.train()
        train_epoch_loss = []
        acc, nums = 0, 0
        # =========================train=======================
        for idx, (label, inputs) in enumerate(tqdm(train_dataloader)):
            inputs = inputs.to(args.device)
            label = label.to(args.device)
            outputs = model(inputs)
            optimizer.zero_grad()
            loss = criterion(outputs, label)
            loss.backward()
            # torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0) #用来梯度裁剪
            optimizer.step()
            train_epoch_loss.append(loss.item())
            acc += sum(outputs.max(axis=1)[1] == label).cpu()
            nums += label.size()[0]
        train_epochs_loss.append(np.average(train_epoch_loss))
        train_acc.append(100 * acc / nums)
        print("train acc = {:.3f}%, loss = {}".format(100 * acc / nums, np.average(train_epoch_loss)))
        # =========================val=========================
        with torch.no_grad():
            model.eval()
            val_epoch_loss = []
            acc, nums = 0, 0

            for idx, (label, inputs) in enumerate(tqdm(val_dataloader)):
                inputs = inputs.to(args.device)  # .to(torch.float)
                label = label.to(args.device)
                outputs = model(inputs)
                loss = criterion(outputs, label)
                val_epoch_loss.append(loss.item())

                acc += sum(outputs.max(axis=1)[1] == label).cpu()
                nums += label.size()[0]

            valid_epochs_loss.append(np.average(val_epoch_loss))
            val_acc.append(100 * acc / nums)

            print("epoch = {}, valid acc = {:.2f}%, loss = {}".format(epoch, 100 * acc / nums, np.average(val_epoch_loss)))

    # =========================plot==========================
    plt.figure(figsize=(12, 4))
    plt.subplot(121)
    plt.plot(train_epochs_loss[:])
    plt.title("train_loss")
    plt.subplot(122)
    plt.plot(train_epochs_loss, '-o', label="train_loss")
    plt.plot(valid_epochs_loss, '-o', label="valid_loss")
    plt.title("epochs_loss")
    plt.legend()
    plt.show()
    # =========================save model=====================
    torch.save(model.state_dict(), 'model.pth')


def pred(val):
    model = Net(1, 32, 16, 2)
    model.load_state_dict(torch.load('model.pth'))
    model.eval()
    val = torch.tensor(val).reshape(1, -1).float()
    # 需要转换成相应的输入shape，而且得带上batch_size，因此转换成shape=(1,1)这样的形状
    res = model(val)
    # real: tensor([[-5.2095, -0.9326]], grad_fn=<AddmmBackward0>) 需要找到最大值所在的列数，就是标签
    res = res.max(axis=1)[1].item()
    print("predicted label is {}, {} {} 8".format(res, val.item(), ('>' if res == 1 else '<')))



if __name__ == '__main__':
    train()
    pred(24)
    pred(3.14)
    pred(7.8)  # 这个会预测错误，所以数据量对于深度学习很重要
```



## 1. PyTorch函数

### 1.1 scatter_ 和 scatter method

这个method是tensor所拥有的，而scartter和scatter_的用法是一样的，区别仅在于，scatter method会返回一个新的tensor，而scatter_仅改变原tensor，而不返回新的tensor。在PyTorch里，所有带一个下划线结束的tensor的method都是类似的效果。所以我们仅介绍scatter method的用法即可。

scatter method的用法为：

```python
target.scatter(dim, index, src)
```
> * target：即目标张量，将在该张量上进行映射
> * src：即源张量，将把该张量上的元素逐个映射到目标张量上
> * dim：指定轴方向，定义了填充方式。对于二维张量，dim=0表示逐列进行行填充，而dim=1表示逐列进行行填充
> * index: 按照轴方向，在target张量中需要填充的位置

为了保证scatter填充的有效性，需要注意：
* （1）target张量在dim方向上的长度不小于source张量，且在其它轴方向的长度与source张量一般相同。这里的一般是指：scatter操作本身有broadcast机制。
* （2）index张量的shape一般与source相同，从而定义了每个source元素的填充位置。这里的一般是指broadcast机制下的例外情况，也就是和第一点里的情况相同。

例子1：
```python
a = torch.arange(10).reshape((2, 5)).float()
b = torch.zeros(3, 5)
b_ = b.scatter(0, index=torch.tensor([[1,2,1,1,2], [2,0,2,1,0]], dtype=torch.int64), a)
print(b_)

# torch.tensor([[0., 6., 0., 0., 9.],
                [0., 0., 2., 8., 0.],
                [5., 1., 7., 0., 4.]])
```

例子2：
scatter函数的一个典型应用就是在分类问题中，将目标标签转换为one-hot编码形式，如：

```python
labels = torch.tensor([1, 3], dtype=torch.int64)
targets = torch.zeros(2, 5)
targets_ = targets.scatter(1, labels.unsqueeze(-1), 1.0)
print(targets_)

# torch.tensor([[0., 1., 0., 0., 0.],
                [0., 0., 0., 1., 0.]])
```

### 1.2 torch.mul()，torch.mm()以及torch.bmm

torch.mul(a,b)是矩阵$$a$$和$$b$$对应位置的元素相乘，需要$$a$$和$$b$$维度相同（或者满足broadcasting），得到的结果也和$$a$$以及$$b$$维度相同。

torch.mm(a,b)是矩阵$$a$$和$$b$$的矩阵相乘，注意$$a$$和$$b$$都必须是2维矩阵。

注意$$\ast$$和torch.mul()用法一样，$$@$$和torch.mm()用法一样。

torch.bmm(a,b)是专门用来进行batch的矩阵乘法的，也就是说$$a$$尺寸为$$b \times n \times m$$，$$b$$的尺寸为$$b \times m \times p$$，结果尺寸为$$b \times n \times p$$，具体计算为对第一个维度的每个2维矩阵，进行矩阵乘法。torch.bmm的作用就是为了第一个维度为batch来及逆行的矩阵乘法，因为torch.mm只能针对2维矩阵乘法，不方便。


## 2. PyTorch机制

### 2.1 model.eval()和torch.no_grad()的区别

在PyTorch代码里，这两个一般都用在推理阶段，但它们的作用是完全不同的，也没有重叠，可以一起使用。

对于model.eval()来说，经常在模型推理代码的前面，都会添加model.eval()，主要有3个作用：

* 不进行dropout
* 不更新batchnorm的mean和var参数
* 不进行梯度反向传播，**但梯度仍然会计算**

torch.no_grad的一般使用方法是，在代码块外面用with torch.no_grad()给包起来。如下面这样：

```python
with torch.no_grad():
    ## your code
```

它的主要作用有2个：

* 不进行梯度的计算(当然也就没办法反向传播了)，节约显存和算力
* dropout和batchnorm还是会正常更新


## 3. torchvision模块

### 3.1 torchvision.transforms模块

此模块的作用是对输入的图片进行某种preprocessing，比如resize，flip等。

#### 3.1.1 transforms.ToTensor()函数
`ToTensor()`将shape为$$(H, W, C)$$的`numpy.ndarray`或由`PIL.Image.open()`读入的`Image`类型的图片数据，转为shape为$$(C, H, W)$$的`Tensor`数据类型，并且还对输入的数据执行除以255的操作。

```python
import torchvision.transforms as transforms
import numpy as np
from PIL import Image
from __future__ import print_function

## 定义转换方式，transforms.Compose将多个转换函数组合起来使用
transform = transforms.Compose([transforms.ToTensor()])

## 定义一个numpy数组
np_array = np.random.rand(4,6,3)

## 直接使用函数归一化
np_tran = transform(np_array)

## 手动归一化
np_temp = torch.from_numpy(np_array.transpose((2,0,1)))
np_manu = np_temp.float().div(255)
print(np_tran.equal(np_manu))

## 输出为True

## 用Image读取一张图片
img = Image.open("xxx.png").convert("RGB")
img_tran = transform(img)
print(np.asarray(img).shape, img_tran.shape, img_tran.type)

## 输出分别为：Height x Width x 3，3 x Height x Width，Tensor
```

#### 3.1.2 transforms.Normalize()函数

在`transforms.Compose([transforms.ToTensor()])`中加入`transforms.Normalize()`，如下所示：`transforms.Compose([transforms.ToTensor(),transforms.Normalize(std=(0.5,0.5,0.5),mean=(0.5,0.5,0.5))])`，则其作用就是先将输入归一化到$$(0,1)$$，再使用公式$$(x-mean)/std$$，将每个元素分布到$$(-1,1)$$

```python
import torchvision.transforms as transforms

## 归一化到(0,1)之后，再执行(x-mean)/std，归一化到(-1,1)
transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(std=(0.5,0.5,0.5),mean=(0.5,0.5,0.5))]) 

np_array = np.random.rand(4,6,3)
np_tran = transform(np_array)

np_temp1 = torch.from_numpy(np_array.transpose((2,0,1)))
np_temp2 = np_temp1.float().div(255)
np.manu = np_temp2.sub_(0.5).div_(0.5)

print(np_tran.equal(manu))
## 输出为True
```

很多代码里面使用的`torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`，这一组值`mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`是从Imagenet训练集计算而来的，所以酌情使用该`mean`和`std`。

#### 3.1.3 transforms.Resize()函数

该函数用来调整由`PIL.Image`库读取的`PILImage`对象的尺寸，注意不能是用`scipy.io.imread`或者`cv2.imread`读取的图片，这两种方法得到的是`ndarray`，无法调整尺寸了。

具体用法是`torchvision.transforms.Resize(a)`或者`torchvision.transforms.Resize([h, w])`，其中前者表示将短边变成$$a$$，长宽比不变，后者指定变换后的长和宽（前者在输入图片长宽一样的情况就不能使用了）。

```python
from PIL import Image
from torchvision import transforms

img = Image.open('xxx.jpg')
w, h = img.size
resize = transforms.Resize([224,244])
img1 = resize(img)
img1.save('xxx1.jpg')
```

> 注意，`img1`是`img`在经过`transforms.Resize()`函数处理之后的对象，`img`是`PILImage`对象，可以看到`img1`仍然是`PILImage`对象，`transforms.Resize()`并没有将其变成`pytorch`的`Tensor`对象。

> 需要注意的一点是`PILImage`对象`size`属性返回的是`w, h`，而`transforms.Resize()`函数的的参数顺序是`h, w`。

---
