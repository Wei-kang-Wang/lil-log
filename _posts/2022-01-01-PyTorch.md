---
layout: post
comments: false
title: "PyTorch"
date: 2022-02-09 01:09:00

---

> 这是关于PyTorch的学习内容.


<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 总览

首先列出PyTorch的参考资料：

1. PyTorch官网：https://pytorch.org/
这里包含了tutorial、API文档等等。
2. 一个不错的介绍PyTorch的博客：https://github.com/jcjohnson/pytorch-examples
3. PyTorch源码的Github页：https://github.com/pytorch/pytorch
在PyTorch官网上也可以找到网页版的源码介绍，是一模一样的。
4. 这里开了个介绍PyTorch源码的博客：https://zhuanlan.zhihu.com/p/328674159

本post不会重复参考资料里的内容，但可能会对其进行补充。本post主要会对重要的PyTorch内容进行补充介绍，其中可能会综合汇总其他人博客里的内容（会标明出处），以达到对PyTorch常用内容可以迅速复习的目的。

本post内容会与post：CUDA学习的内容联动。


### PyTorch函数：scatter_ 和 scatter method

这个method是tensor所拥有的，而scartter和scatter_的用法是一样的，区别仅在于，scatter method会返回一个新的tensor，而scatter_仅改变原tensor，而不返回新的tensor。在PyTorch里，所有带一个下划线结束的tensor的method都是类似的效果。所以我们仅介绍scatter method的用法即可。

scatter method的用法为：

```python
target.scatter(dim, index, src)
```
> * target：即目标张量，将在该张量上进行映射
> * src：即源张量，将把该张量上的元素逐个映射到目标张量上
> * dim：指定轴方向，定义了填充方式。对于二维张量，dim=0表示逐列进行行填充，而dim=1表示逐列进行行填充
> * index: 按照轴方向，在target张量中需要填充的位置

为了保证scatter填充的有效性，需要注意：
* （1）target张量在dim方向上的长度不小于source张量，且在其它轴方向的长度与source张量一般相同。这里的一般是指：scatter操作本身有broadcast机制。
* （2）index张量的shape一般与source相同，从而定义了每个source元素的填充位置。这里的一般是指broadcast机制下的例外情况，也就是和第一点里的情况相同。

例子1：
```python
a = torch.arange(10).reshape((2, 5)).float()
b = torch.zeros(3, 5)
b_ = b.scatter(0, index=torch.tensor([[1,2,1,1,2], [2,0,2,1,0]], dtype=torch.int64), a)
print(b_)

# torch.tensor([[0., 6., 0., 0., 9.],
                [0., 0., 2., 8., 0.],
                [5., 1., 7., 0., 4.]])
```

例子2：
scatter函数的一个典型应用就是在分类问题中，将目标标签转换为one-hot编码形式，如：

```python
labels = torch.tensor([1, 3], dtype=torch.int64)
targets = torch.zeros(2, 5)
targets_ = targets.scatter(1, labels.unsqueeze(-1), 1.0)
print(targets_)

# torch.tensor([[0., 1., 0., 0., 0.],
                [0., 0., 0., 1., 0.]])
```





---
