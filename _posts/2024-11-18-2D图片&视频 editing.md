---
layout: post
comments: True
title: "2D图片/视频 editing"
date: 2024-07-30 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

## 风格迁移

### \[**ECCV 2024**\] [InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser](https://cuixing100876.github.io/instastyle.github.io/)

### \[**CVPR 2024**\] [DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing](https://kevin-thu.github.io/DiffMorpher_page/)

### \[**Siggraph 2024**\] [Cross-Image Attention for Zero-Shot Appearance Transfer](https://garibida.github.io/cross-image-attention/)

### \[**CVPR 2021**\] [Learning to Warp for Style Transfer](https://github.com/xch-liu/learning-warp-st)

### \[**CVPR 2020 Oral**\] [Cross-Domain Correspondence Learning for Exemplar-Based Image Translation](https://panzhang0212.github.io/CoCosNet/)

### \[**ICCV 2023**\] [StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation](https://github.com/AIRI-Institute/StyleDomain)

### \[**ECCV 2020**\] [Deformable Style Transfer (DST)](https://github.com/sunniesuhyoung/DST?tab=readme-ov-file)



## 图片/视频修复（inpainting）

ICCV2023的[ProPainter: Improving Propagation and Transformer for Video Inpainting](https://shangchenzhou.com/projects/ProPainter/)


## 动作迁移

### \[**CVPR 2020**\] [Transferring Dense Pose to Proximal Animal Classes](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sanakoyeu_Transferring_Dense_Pose_to_Proximal_Animal_Classes_CVPR_2020_paper.pdf)

[POST](https://gdude.de/densepose-evolution/)
[CODE](https://github.com/asanakoy/densepose-evolution)

### \[**CVPR 2024**\] [DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing](https://kevin-thu.github.io/DiffMorpher_page/)

## Animation

### \[**WACV 2022 Best Paper**\] [Few-Shot Keypoint Character Animation and Reposing](https://github.com/tohinz/CharacterGAN)

### \[**Siggraph 2022**\] [Rewriting Geometric Rules of a GAN](https://peterwang512.github.io/GANWarping/)

### \[**Arxiv 2024**\] [SwiftEdit: Lightning Fast Text-guided Image Editing via One-step Diffusion](https://swift-edit.github.io/)

### \[**CVPR 2020**\] [Barycenters of Natural Images Constrained Wasserstein Barycenters for Image Morphing](https://github.com/drorsimon/image_barycenters)

## editing生成模型

### \[**Arxiv 2021**\] [LatentKeypointGAN: Controlling GANs via Latent Keypoints](https://xingzhehe.github.io/LatentKeypointGAN/)

这篇文章提出的想法是，将使用auto encoder以keypoint作为latent representation的unsupervised 2d keypoint detection的框架，和GAN结合起来，从而使用keypoint来控制生成图片，可以做到不仅控制appearance，还可以通过控制keypoint的位置来控制keypoint表示的区域的位置（比如将生成人脸的左眼区域上下移动，或者删掉，或者将A的嘴替换为B的嘴），效果如下图所示：

![unsuper1]({{ '/assets/images/latentkpgan_1.png' | relative_url }}){: width=800px style="float:center"} 

这篇文章的设计思路是，首先使用三个独立采样的标准高斯分布随机变量$$z_{kp-pose}, z_{kp-app}, z_{bg-emb}$$来分别通过三个独立的MLP，得到对应的结果，其中由$$z_{kp-pose}$$输出2d keypoint matrix $$P \in \left( 0,1 \right)^{K \times 2}$$，由$$z_{kp-app}$$生成描述image物体部分的整体appearance特征$$W_{global} \in \mathbb{R}^{D_{embed}}$$，由$$z_{bg-emb}$$生成描述image背景部分的appearance的特征$$W^{bg} \in \mathbb{R}^{D_{embed}}$$。

随后，还有$$K$$个global的vector $$W_{constant}^i, \ i=1,2,\cdots, K$$需要被optimized，$$f_i = W_{global} \otimes W_{constant}^i$$被用来表示每个keypoint的feature，$$i=1,2,\cdots,K$$，其中$$\otimes$$表示element-wise multplication。

对于$$P$$里的第$$j$$个keypoint，以该keypoint为中心，构造一个大小为$$H \times W$$的Gaussian heatmap，记为$$S_j$$，再补充一个维度，即$$S_j \in \mathbb{R}_{+}^{H \times W \times 1}$$。将之前得到的第$$j$$个keypoint的feature $$f_j \in \mathbb{R}^{D_{embed}}$$增加一个维度，变成$$f_j \in \mathbb{R}^{1 \times D_{embed}}$$，最后$$H_j = S_j \cdot f_j \in \mathbb{R}^{H \times W \times D_{embed}}$$即是第$$j$$个keypoint的feature map。而background的feature map是$$H_{bg}(p) = 1 - \max\limits_{j=1,2,\cdots,K} H_j(p)$$，其中$$p$$表示$$H \times W$$ image grid上的每个像素点。最后，将这$$K+1$$个feature map沿着一个新的维度连接起来，得到一个$$H \times W \times D_{embed} \times (K+1)$$的feature map，输入给generator，生成最后的图片。

![unsuper1]({{ '/assets/images/latentkpgan_2.png' | relative_url }}){: width=800px style="float:center"}

> 这篇文章还可以用来进行unsupervised 2D keypoint detection。具体做法是，在训练上述网络的同时（或者已经有了某个预训练好的上述网络），再训练一个2d keypoint detector，以生成的图片为输入，以keypoint matrix $$P$$为输出。


