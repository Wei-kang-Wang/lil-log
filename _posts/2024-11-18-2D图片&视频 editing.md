---
layout: post
comments: True
title: "2D图片/视频 editing"
date: 2024-07-30 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

## 风格迁移

### \[**ECCV 2024**\] [InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser](https://cuixing100876.github.io/instastyle.github.io/)

### \[**CVPR 2024**\] [DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing](https://kevin-thu.github.io/DiffMorpher_page/)

### \[**Siggraph 2024**\] [Cross-Image Attention for Zero-Shot Appearance Transfer](https://garibida.github.io/cross-image-attention/)

### \[**CVPR 2021**\] [Learning to Warp for Style Transfer](https://github.com/xch-liu/learning-warp-st)

### \[**CVPR 2020 Oral**\] [Cross-Domain Correspondence Learning for Exemplar-Based Image Translation](https://panzhang0212.github.io/CoCosNet/)

### \[**ICCV 2023**\] [StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation](https://github.com/AIRI-Institute/StyleDomain)

### \[**ECCV 2020**\] [Deformable Style Transfer (DST)](https://github.com/sunniesuhyoung/DST?tab=readme-ov-file)

### \[**ACM SIGGRAPH 2024**\] [Cross-Image Attention for Zero-Shot Appearance Transfer](https://garibida.github.io/cross-image-attention/)

### \[**CVPR 2024**\] [PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding](https://photo-maker.github.io/)


## 试衣

### [Outfit Anyone: Ultra-high quality virtual try-on for Any Clothing and Any Person](https://humanaigc.github.io/outfit-anyone/)


## 图片/视频修复（inpainting）

### \[**ICCV 2023**\] [ProPainter: Improving Propagation and Transformer for Video Inpainting](https://shangchenzhou.com/projects/ProPainter/)

### \[**CVPR 2024**\] [BrushNet : A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion](https://tencentarc.github.io/BrushNet/)

### [BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion](https://tencentarc.github.io/BrushNet/)


## 图片/视频扩展（outpainting）

### [Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific Adaptation](https://be-your-outpainter.github.io/)


## 视频interpolation

### \[**NeurIPS 2024**\] [Generalizable Implicit Motion Modeling for Video Frame Interpolation](https://gseancdat.github.io/projects/GIMMVFI)


## 图片/视频 super-resolution

### [KEEP: Kalman-Inspired FEaturE Propagation for Video Face Super-Resolution](https://jnjaby.github.io/projects/KEEP/)

### \[**ECCV 2024**\] [SuperGaussian: Repurposing Video Models for 3D Super Resolution](https://supergaussian.github.io/)

### \[**CVPR 2024 Oral**\] [FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring](https://kaist-viclab.github.io/fmanet-site/)

### [Large Kernel Distillation Network for Efficient Single Image Super-Resolution](https://github.com/stella-von/LKDN)

### \[**CVPR 2022**\] [Investigating Tradeoffs in Real-World Video Super-Resolution](https://github.com/ckkelvinchan/RealBasicVSR)

### [XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution](https://github.com/qyp2000/XPSR)

## 动作迁移

### \[**CVPR 2020**\] [Transferring Dense Pose to Proximal Animal Classes](https://openaccess.thecvf.com/content_CVPR_2020/papers/Sanakoyeu_Transferring_Dense_Pose_to_Proximal_Animal_Classes_CVPR_2020_paper.pdf)

[POST](https://gdude.de/densepose-evolution/)
[CODE](https://github.com/asanakoy/densepose-evolution)

### \[**CVPR 2024**\] [DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing](https://kevin-thu.github.io/DiffMorpher_page/)

### \[**CVPR 2020**\] [Transferring Dense Pose to Proximal Animal Classes](https://gdude.de/densepose-evolution/)

## Animation

### \[**AAAI 2025**\] [Move and Act: Enhanced Object Manipulation and Background Integrity for Image Editing](https://github.com/mobiushy/move-act)

### \[**ECCV 2024**\] [InstructGIE: Towards Generalizable Image Editing](https://cr8br0ze.github.io/InstructGIE/)

### [Reference-Based 3D-Aware Image Editing with Triplanes](https://three-bee.github.io/triplane_edit/)

### \[**ECCV 2024**\] [RegionDrag: Fast Region-Based Image Editing with Diffusion Models](https://visual-ai.github.io/regiondrag/)

### \[**NeurIPS 2019**\] [First Order Motion Model for Image Animation](https://aliaksandrsiarohin.github.io/first-order-model-website/)

### \[**ACM SIGGRAPH ASIA 2021**\] [Layered Neural Atlases for Consistent Video Editing](https://layered-neural-atlases.github.io/#paper)

### \[**WACV 2022 Best Paper**\] [Few-Shot Keypoint Character Animation and Reposing](https://github.com/tohinz/CharacterGAN)

### \[**Siggraph 2022**\] [Rewriting Geometric Rules of a GAN](https://peterwang512.github.io/GANWarping/)

### \[**Arxiv 2024**\] [SwiftEdit: Lightning Fast Text-guided Image Editing via One-step Diffusion](https://swift-edit.github.io/)

### \[**CVPR 2020**\] [Barycenters of Natural Images Constrained Wasserstein Barycenters for Image Morphing](https://github.com/drorsimon/image_barycenters)

### \[**Arxiv 2024**\] [Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts](https://follow-your-click.github.io/)

### \[**CVPR 2024**\] [VidToMe: Video Token Merging for Zero-Shot Video Editing](https://vidtome-diffusion.github.io/)

### \[**ACM SIGGRAPH ASIA 2024**\] [TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models](https://turboedit-paper.github.io/)

### \[**ECCV 2024**\] [TurboEdit: Instant text-based image editing](https://betterze.github.io/TurboEdit/)

### \[**ECCV 2024**\] [SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing](https://swap-anything.github.io/)

### \[**ECCV 2024**\] [Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models](https://maxin-cn.github.io/cinemo_project/)

### \[**ECCV 2024**\] [FlexiEdit: Frequency-Aware Latent Refinement for Enhanced Non-Rigid Editing](https://kookie12.github.io/FlexiEdit-Project-Page/#)

### \[**ECCV 2024**\] [Real-time 3D-aware Portrait Editing from a Single Image](https://ezioby.github.io/3dpe/)

## editing生成模型

### \[**Arxiv 2021**\] [LatentKeypointGAN: Controlling GANs via Latent Keypoints](https://xingzhehe.github.io/LatentKeypointGAN/)

这篇文章提出的想法是，将使用auto encoder以keypoint作为latent representation的unsupervised 2d keypoint detection的框架，和GAN结合起来，从而使用keypoint来控制生成图片，可以做到不仅控制appearance，还可以通过控制keypoint的位置来控制keypoint表示的区域的位置（比如将生成人脸的左眼区域上下移动，或者删掉，或者将A的嘴替换为B的嘴），效果如下图所示：

![unsuper1]({{ '/assets/images/latentkpgan_1.png' | relative_url }}){: width=800px style="float:center"} 

这篇文章的设计思路是，首先使用三个独立采样的标准高斯分布随机变量$$z_{kp-pose}, z_{kp-app}, z_{bg-emb}$$来分别通过三个独立的MLP，得到对应的结果，其中由$$z_{kp-pose}$$输出2d keypoint matrix $$P \in \left( 0,1 \right)^{K \times 2}$$，由$$z_{kp-app}$$生成描述image物体部分的整体appearance特征$$W_{global} \in \mathbb{R}^{D_{embed}}$$，由$$z_{bg-emb}$$生成描述image背景部分的appearance的特征$$W^{bg} \in \mathbb{R}^{D_{embed}}$$。

随后，还有$$K$$个global的vector $$W_{constant}^i, \ i=1,2,\cdots, K$$需要被optimized，$$f_i = W_{global} \otimes W_{constant}^i$$被用来表示每个keypoint的feature，$$i=1,2,\cdots,K$$，其中$$\otimes$$表示element-wise multplication。

对于$$P$$里的第$$j$$个keypoint，以该keypoint为中心，构造一个大小为$$H \times W$$的Gaussian heatmap，记为$$S_j$$，再补充一个维度，即$$S_j \in \mathbb{R}_{+}^{H \times W \times 1}$$。将之前得到的第$$j$$个keypoint的feature $$f_j \in \mathbb{R}^{D_{embed}}$$增加一个维度，变成$$f_j \in \mathbb{R}^{1 \times D_{embed}}$$，最后$$H_j = S_j \cdot f_j \in \mathbb{R}^{H \times W \times D_{embed}}$$即是第$$j$$个keypoint的feature map。而background的feature map是$$H_{bg}(p) = 1 - \max\limits_{j=1,2,\cdots,K} H_j(p)$$，其中$$p$$表示$$H \times W$$ image grid上的每个像素点。最后，将这$$K+1$$个feature map沿着一个新的维度连接起来，得到一个$$H \times W \times D_{embed} \times (K+1)$$的feature map，输入给generator，生成最后的图片。

![unsuper1]({{ '/assets/images/latentkpgan_2.png' | relative_url }}){: width=800px style="float:center"}

> 这篇文章还可以用来进行unsupervised 2D keypoint detection。具体做法是，在训练上述网络的同时（或者已经有了某个预训练好的上述网络），再训练一个2d keypoint detector，以生成的图片为输入，以keypoint matrix $$P$$为输出。


