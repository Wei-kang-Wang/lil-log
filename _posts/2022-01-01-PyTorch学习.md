---
layout: post
comments: false
title: "PyTorch学习"
date: 2022-02-09 01:09:00

---

> This post of different PyTorch functions.


<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

## 1. torch


## 2. torch.nn

### 2.1 Conv2d function

卷积和反卷积是图片计算在深度学习中常用的上采样和下采样操作。相比其他采样操作，卷积层可以通过更新参数拟合图像特征（通过梯度反向传递)。

另外作为特征提取的常用操作，卷积在计算中可以改变图片计算后的通道，把参数压缩为数量更少的卷积核。相比上一代的全连接操作，能降低计算量的同时，充分整合图像的局部特征。

在torch.nn中,卷积操作是一个函数，输入为一组图片或特征变量$$\left[ N, C_{in}, W, H \right]$$,输出也为一组变量$$ \left[ N, C_{out}, W_{out}, H_{out} \right]$$.变量类型为tensor。其中$$N$$表示的是batch size，因此不变。

```python
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
```

这个函数在pytorch里被定义为了一个class，因此每次需要传入参数实例化之后才是一个可以正常使用的函数。

最简单的情况，对于输入大小为$$\left[ N, C_{in}, W, H \right]$$，输出大小为$$ \left[ N, C_{out}, W_{out}, H_{out} \right]$$的情况，如果没有再改变其他的参数的话，输出层则表示为：

$$out(N_i, C_{out_j}) = bias(C_{out_j}) + \Sigma_{k=0}^{C_{in} - 1} weight(C_{out_j}, k) \ast input(N_i, k)$$

其中$$N_i$$表示这是这个batch里第几个元素，$$\ast$$表示的是二维卷积操作，weight则是卷积核。


**torch.nn.Conv2d的参数**：

* in_channels (int) – 输入图片的通道数，Number of channels in the input image
* out_channels (int) – 输出图片的通道数，Number of channels produced by the convolution
* kernel_size (int or tuple) – 卷积核的大小，Size of the convolving kernel
* stride (int or tuple, optional) – 卷积操作里步长的大小，Stride of the convolution. Default: 1。控制的是卷积核每次移动的距离。
* padding (int, tuple or str, optional) – 卷积操作里padding的大小，Padding added to all four sides of the input. Default: 0。其可以是一个string（'valid'或者'same'）也可以直接是int或者tuple，来表示每边需要padding的值。
* padding_mode (string, optional) – 卷积操作里padding的方式，'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'
* dilation (int or tuple, optional) – 卷积操作里dilation的大小，Spacing between kernel elements. Default: 1。很难讲的清楚，但是[这个视频](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)可以很清晰地说明。
* groups (int, optional) – 卷积操作里groupings的值，Number of blocked connections from input channels to output channels. Default: 1。groups控制的是输入和输出之间的连接，in_channels和out_channels都必须能够整除groups。比如说$$groups=1$$，所有的输入都被卷积计算为输出；$$groups=2$$，就等价为有两个卷积层，每个卷积层只操作输入的一半的通道，然后也只输出out_channels/2的通道数，再将这两个输出按照通道这个维度连起来。$$groups= in_channels$$，输入的每个通道都使用某个filters集合来进行计算，计算出的输出的通道数为out_channels/inchannels，再将它们连接起来。
* bias (bool, optional) – 卷积操作里，上述计算式子里是否要加上bias，If True, adds a learnable bias to the output. Default: True。


注意，kernel_size，stride，padding，dilation这几个参数即可以是一个int，也可以是由两个int构成的tuple（因为这是Conv2d函数，所以确定是2个）:
* a single int – 对于width和height维度使用这个同样的int。
* a tuple of two ints – 第一个int用于height维度，第二个int用于width维度。

注意，只有前三个是必须要传递的参数，后面都是optional的了，所以说如果并未指定参数名直接传入值，那么就会按顺序给前三个，剩下的再按顺序给optional的参数，比如说torch.nn.Conv2d(3, 64, 3, 1, 1)表示的就是输入通道为3，输出通道为64，卷积核大小为3，步长为1，padding为1的卷积函数。


**torch.nn.Conv2d的输入输出shape**：

* 输入：$$(N, C_{in}, H_{in}, W_{in})$$或者$$(C_{in}, H_{in}, W_{in})$$
* 输出：$$(N, C_{out}, H_{out}, W_{out})$$或者$$(C_{out}, H_{out}, W_{out})$$，其中：
* $$H_{out} = \lfloor \frac{H_{in} + 2 \times padding \left[0 \right] - dilation \left[0 \right] \times (kernel size \left[0 \right] - 1) - 1}{stride \left[0 \right]} + 1 \rfloor$$
* $$W_{out} = \lfloor \frac{W_{in} + 2 \times padding \left[0 \right] - dilation \left[0 \right] \times (kernel size \left[0 \right] - 1) - 1}{stride \left[0 \right]} + 1 \rfloor$$


**torch.nn.Conv2d的variables**

* torch.nn.Conv2d.weights：是一个Tensor，是可以学习的，大小为$$(out_channels, \frac{in_channels}{groups}, kernel_size \left[0 \right], kernel_size \left[1 \right])。如果没有指定，初始化的参数是从某个概率分布随机采样得到的。
* torch.nn.Conv2d.bias：是一个Tensor，是可以学习的，大小为$$(out_channels)。如果没有指定，初始化的参数是从某个概率分布随机采样得到的。

如果不加指定，这个函数默认数据类型为TensorFloat32。


>当dilation=in_channels，而outchannels = in_channels * K，其中K是一个正整数，这种情况叫做depthwise convolution。

>padding的值如果是'valid'，就等于表示不需要padding。padding的值如果是'same'，就表示输出的尺寸和输入一样，但这只能适用于stride=1的情况，其他情况不行。

>torch.nn.Conv2d这个module还支持复数，complex32，complex64和complex128三种数据类型。


### 2.2 ConvTranspose2d function

转置卷积，也称为反卷积(deconvlution)和分部卷积(fractionally-strided convolution)。为卷积的逆操作，即把特征的维度压缩，但尺寸放大。

函数形式如下：

```python
torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)
```

torch.nn.ConvTranpose2d这个module依然被定义为一个类，所以每次使用也是传入参数将其实例化为一个函数再使用。这个module可以被看成一个Conv2d function关于它的输入的梯度。它也叫fractionally-strided convolution或者deconvolution（但它实际上并不是一个真正的deconvolution操作，因为它并不真的是一个convolution的逆运算）。 [这个网页](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)还有[这个网页](https://blog.csdn.net/disanda/article/details/105762054)可以看到它的可视化。


**torch.nn.ConvTranspose2d的parameters**

* in_channels (int) –输入通道数，Number of channels in the input image
* out_channels (int) – 输出通道数，Number of channels produced by the convolution
* kernel_size (int or tuple) – 卷积核的大小，Size of the convolving kernel
* stride (int or tuple, optional) – 步长，Stride of the convolution. Default: 1
* padding (int or tuple, optional) – Default: 0。数量为$$dilation \times (kernel_size - 1) - padding$$的 zero-padding将会被加到输入的每个维度上。
* output_padding (int or tuple, optional) – 在卷积计算之后，加在输出各个维度上的padding值。Default: 0
* groups (int, optional) – 连接输入和输出的blocks的数量，Number of blocked connections from input channels to output channels. Default: 1
* bias (bool, optional) – 是否在卷积计算的时候增加一个偏置，If True, adds a learnable bias to the output. Default: True
* dilation (int or tuple, optional) – 卷积计算的时候输入是否进行膨胀，Spacing between kernel elements. Default: 1

值得注意的是，除了stride，padding和output_padding以外，其余的参数的意义与torch.nn.Conv2d里的对应参数的意义是一样的。而stride在这里表示的就不是卷积的步长了，它表示的是对于输入来说，每行之间插入$$stride \left[1 \right] - 1$$行0，每列之间插入$$stride \left[1 \right] - 1$$列0。而如上面所说，输入的每个维度都加上大小为$$dilation \times (kernel_size - 1) - padding$$的值为0的padding。而output_padding是新的参数，意思如上所示。


**torch.nn.ConvTranspose2d的输入输出shape**


* 输入：$$(N, C_{in}, H_{in}, W_{in})$$或者$$(C_{in}, H_{in}, W_{in})$$
* 输出：$$(N, C_{out}, H_{out}, W_{out})$$或者$$(C_{out}, H_{out}, W_{out})$$，其中：
* $$H_{out} = (H_{in} − 1) \times stride \left[0 \right] − 2 \times padding \left[ 0 \right] + dilation \left[ 0 \right] \times (kernel size \left[ 0 \right]−1) + output padding \left[ 0 \right] + 1$$
* $$W_{out} = (W_{in} − 1) \times stride \left[0 \right] − 2 \times padding \left[ 0 \right] + dilation \left[ 0 \right] \times (kernel size \left[ 0 \right]−1) + output padding \left[ 0 \right] + 1$$


**torch.nn.ConvTranspose2d的variables**

和torch.nn.Conv2d里的一样。


>实际上，torch.nn.ConvTranspose2d里padding和stride这样设置，是为了让两个具有相同参数的torch.nn.Conv2d和torch.nn.ConvTranspose2d串连起来的时候，输入的尺寸在经过两个模块之后保持不变，也就是输出尺寸等于输入尺寸。


**参考文献**

* https://blog.csdn.net/disanda/article/details/105762054


### 2.3 GroupNorm function

```python
torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True, device=None, dtype=None)
```

上述定义的是一个python里的类，所以每次需要实例化才能是一个正常的Pytorch函数。这个函数会在一个mini-batch上应用Group normalization操作，正如[Group Normalization](https://arxiv.org/pdf/1803.08494.pdf)里所描述的那样。

一个mini-batch的输入的通道会被分为num_groups个部分，每个部分含有num_channels / num_groups个通道。所以要求num_channels必须要能够被num_groups整除。而针对每个部分，都要计算它们的均值和标准差，然后按照下列式子对每个部分进行标准化。式子里的$$\gamma$$和$$\beta$$和batch normalization里一样，为了增加模型的可学习能力。

$$output = \frac{input - E\left[input\right]}{\sqrt{Var\left[input\right] + \epsilon}} \ast \gamma + \beta$$



## 3. torch.nn.functional


## 4. torch.Tensor

### 4.1 scatter_ 和 scatter method

这个method是tensor所拥有的，而scartter和scatter_的用法是一样的，区别仅在于，scatter method会返回一个新的tensor，而scatter_仅改变原tensor，而不返回新的tensor。在PyTorch里，所有带一个
下划线结束的tensor的method都是类似的效果。所以我们仅介绍scatter method的用法即可。

scatter method的用法为：

```python
target.scatter(dim, index, src)
```
> * target：即目标张量，将在该张量上进行映射
> * src：即源张量，将把该张量上的元素逐个映射到目标张量上
> * dim：指定轴方向，定义了填充方式。对于二维张量，dim=0表示逐列进行行填充，而dim=1表示逐列进行行填充
> * index: 按照轴方向，在target张量中需要填充的位置

为了保证scatter填充的有效性，需要注意：
* （1）target张量在dim方向上的长度不小于source张量，且在其它轴方向的长度与source张量一般相同。这里的一般是指：scatter操作本身有broadcast机制。
* （2）index张量的shape一般与source相同，从而定义了每个source元素的填充位置。这里的一般是指broadcast机制下的例外情况，也就是和第一点里的情况相同。

例子1：
```python
a = torch.arange(10).reshape((2, 5)).float()
b = torch.zeros(3, 5)
b_ = b.scatter(0, index=torch.tensor([[1,2,1,1,2], [2,0,2,1,0]], dtype=torch.int64), a)
print(b_)

# torch.tensor([[0., 6., 0., 0., 9.],
                [0., 0., 2., 8., 0.],
                [5., 1., 7., 0., 4.]])
```

例子2：
scatter函数的一个典型应用就是在分类问题中，将目标标签转换为one-hot编码形式，如：

```python
labels = torch.tensor([1, 3], dtype=torch.int64)
targets = torch.zeros(2, 5)
targets_ = targets.scatter(1, labels.unsqueeze(-1), 1.0)
print(targets_)

# torch.tensor([[0., 1., 0., 0., 0.],
                [0., 0., 0., 1., 0.]])
```


## 5. Tensor Attributes


## 6. Tensor Views


## 7. torch.amp


## 8. torch.autograd


## 9. torch.library


## 10. torch.cuda


## 11. torch.backends


## 12. torch.distributed


## 13. torch.distributed.algorithms.join


## 14. torch.distributed.elastic


## 15. torch.distributed.fsdp


## 16. torch.distributed.optim


## 17. torch.distributions


## 18. torch.fft


## 19. torch.futures


## 20. torch.fx


## 21. torch.hub


## 22. torch.jit


## 23. torch.linalg


## 24. torch.monitor


## 25. torch.special


## 26. torch.overrides


## 27. torch.package


## 28. torch.profiler


## 29. torch.nn.init


## 30. torch.onnx


## 31. torch.optim


## 32. Complex Numbers


## 33. DDP Communication Hooks


## 34. Pipeline Parallelism


## 35. Quantization


## 36. Distributed RPC Framework


## 37. torch.random


## 38. torch.nested


## 39. torch.sparse


## 40. torch.Storage


## 41. torch.testing


## 42. torch.utils.benchmark


## 43. torch.utils.bottleneck


## 44. torch.utils.checkpoint


## 45. torch.utils.cpp_extension


## 46. torch.utils.data


## 47. torch.utils.dlpack


## 48. torch.utils.mobile_optimizer


## 49. torch.utils.model_zoo


## 50. torch.utils.tensorboard

Tensorboard是TensorFlow最先提出的概念，所以最全面的内容可以在这里找到：https://www.tensorflow.org/tensorboard/

因为Tensorboard使用的是存储的log来表示信息而不是存储的tensor，所以pytorch里也可以很容易的使用Tensorboard。

一旦已经安装了TensorBoard，我们就可以将PyTorch模型以及计算的数值都存储为log的形式，从而利用Tensorboard的UI进行可视化操作。Pytorch模型里的scalars，images，histograms，graphs以及embeddings的可视化都是可以的。

>安装Tensorboard只需要一句指令：pip install tensorboard

SummaryWriter这个类是用来记录所需要Tensorboard进行可视化的那些log的类。举个例子说明：

```python

import torch
import torchvision
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms

# Writer will output to ./runs/ directory by default
writer = SummaryWriter()

transform = transforms.Compose([transform.ToTensor(), transforms.Normalize((0.5, ), (0.5, ))])
trainset = datasets.MINIST('mnist_train', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle=True)
model = torchvision.models.resnet50(False)
# Have ResNet model take in grayscale rather than RGB
model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
image, labels = next(iter(trainloader)

grid = torchvision.utils.make_grid(images)
writer.add_image('images', grid, 0)
writer.add_graph(model, images)
writer.close()

```

之后可以用一句指令打开Tensorboard：

```python
tensorboard --logdir=runs
```

我们可以在一个实验里存储很多数据用来可视化。为了避免这些数据可视化会在Tensorboard的UI里显得杂乱无章，我们还可以给这些可视化的plots分组命名。比如说：Loss/train以及Loss/test就表示在Loss这个分组下，有train和test这两张图，而Accuracy/train和Accuracy/test同理。举个例子如下：

```python
from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
```

![torchutilstensorboard1]({{ '/assets/images/TORCHUTILSTENSORBOARD.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}


下面详细介绍torch.utils.tensorboard这个类。

```python

CLASS  torch.utils.tensorboard.writer.SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')

```

这个类会将各种类型的数值写到存放在log_dir目录下的event file（也就是log文件）里，从而可以被Tensorboard所使用。

SummaryWriter类提供了一个high-level的API来在log_dir下创建一个event file，并且将数值添加到这个file里。这个类非同步的更新event file里的内容（也就是说先创建一个event file，然后再逐步往里添加内容），从而我们可以在一个训练程序里随时给这个event file添加内容。

接下来我们来看这个class里定义的各种methods。

```python

__init__(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')

```

创建一个SummaryWriter实例，将所需要的内容写入一个event file里。

参数：

* log_dir (string) – 所创建的SummaryWriter，也是event files存放的目录。默认的值是runs/CURRENT_DATETIME_HOSTNAME, 其会在每次运行的时候都变化。使用层次化的文件夹命名规则会使得在不同的runs之间比较实验效果更加容易，e.g. ‘runs/exp1’, ‘runs/exp2’。
* comment (string) – 这个comment会加在默认的log_dir之后，共同构成目录。如果log_dir已经被赋值了，那么这个argument没有任何作用。
* purge_step (int) – When logging crashes at step T+X and restarts at step T, any events whose global_step larger or equal to T will be purged and hidden from TensorBoard. Note that crashed and resumed experiments should have the same log_dir.
* max_queue (int) – 在任何一个add还没被调用之前（add会迫使某个值被存储到disk里），由还在等待的events和summaries构成的queue的大小，默认是10。
* flush_secs (int) – 多长时间会将还在等待的events和summaries存储到disk里，默认值是120，单位是秒。
* filename_suffix (string) – 加在所有log_dir目录下的event file名字的后面。更多细节参考tensorboard.summary.writer.event_file_writer.EventFileWriter。


接下来看看torch.utils.tensorboard.writer.SummaryWriter()这个类的源码。

>值得注意的是一般都用from torch.utils.tensorboard import SummaryWriter，因为在torch.utils.tensorboard这个文件夹下只有SummaryWriter这一个叫这个名字的文件，实际上SummaryWriter是存放在torch.utils.tensorboard.writer下的，所以from torch.utils.tensorboard.writer import SummaryWriter也是一样的。


如下代码是torch.utils.tensorboard.writer文件的一部分：

```python

class SummaryWriter(object):
    """Writes entries directly to event files in the log_dir to be
    consumed by TensorBoard.

    The `SummaryWriter` class provides a high-level API to create an event file
    in a given directory and add summaries and events to it. The class updates the
    file contents asynchronously. This allows a training program to call methods
    to add data to the file directly from the training loop, without slowing down
    training.
    """
    
    def __init__(
        self,
        log_dir=None,
        comment="",
        purge_step=None,
        max_queue=10,
        flush_secs=120,
        filename_suffix="",
    ):
        """Creates a `SummaryWriter` that will write out events and summaries
        to the event file.

        Args:
            log_dir (string): Save directory location. Default is
              runs/**CURRENT_DATETIME_HOSTNAME**, which changes after each run.
              Use hierarchical folder structure to compare
              between runs easily. e.g. pass in 'runs/exp1', 'runs/exp2', etc.
              for each new experiment to compare across them.
            comment (string): Comment log_dir suffix appended to the default
              ``log_dir``. If ``log_dir`` is assigned, this argument has no effect.
            purge_step (int):
              When logging crashes at step :math:`T+X` and restarts at step :math:`T`,
              any events whose global_step larger or equal to :math:`T` will be
              purged and hidden from TensorBoard.
              Note that crashed and resumed experiments should have the same ``log_dir``.
            max_queue (int): Size of the queue for pending events and
              summaries before one of the 'add' calls forces a flush to disk.
              Default is ten items.
            flush_secs (int): How often, in seconds, to flush the
              pending events and summaries to disk. Default is every two minutes.
            filename_suffix (string): Suffix added to all event filenames in
              the log_dir directory. More details on filename construction in
              tensorboard.summary.writer.event_file_writer.EventFileWriter.

        Examples::

            from torch.utils.tensorboard import SummaryWriter

            # create a summary writer with automatically generated folder name.
            writer = SummaryWriter()
            # folder location: runs/May04_22-14-54_s-MacBook-Pro.local/

            # create a summary writer using the specified folder name.
            writer = SummaryWriter("my_experiment")
            # folder location: my_experiment

            # create a summary writer with comment appended.
            writer = SummaryWriter(comment="LR_0.1_BATCH_16")
            # folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/

        """
        torch._C._log_api_usage_once("tensorboard.create.summarywriter")
        if not log_dir:
            import socket
            from datetime import datetime

            current_time = datetime.now().strftime("%b%d_%H-%M-%S")
            log_dir = os.path.join(
                "runs", current_time + "_" + socket.gethostname() + comment
            )
        self.log_dir = log_dir
        self.purge_step = purge_step
        self.max_queue = max_queue
        self.flush_secs = flush_secs
        self.filename_suffix = filename_suffix

        # Initialize the file writers, but they can be cleared out on close
        # and recreated later as needed.
        self.file_writer = self.all_writers = None
        self._get_file_writer()

        # Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard
        v = 1e-12
        buckets = []
        neg_buckets = []
        while v < 1e20:
            buckets.append(v)
            neg_buckets.append(-v)
            v *= 1.1
        self.default_bins = neg_buckets[::-1] + [0] + buckets


```



## 51. Type Info


## 52. Named Tensors


## 53. Named Tensors operator coverage


## 54. torch.__config__





---
