---
layout: post
comments: false
title: "[论文]Keypoint"
date: 2021-12-08 01:09:00
tags: paper-reading
---

> This post is a summary of keypoint related papers.


<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 1. 3D Keypoint Learning

### [KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control](https://openaccess.thecvf.com/content/CVPR2021/papers/Jakab_KeypointDeformer_Unsupervised_3D_Keypoint_Discovery_for_Shape_Control_CVPR_2021_paper.pdf)

*Tomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu, Noah Snavely, Angjoo Kanazawa*

*CVPR 2021*

[page](http://tomasjakab.github.io/KeypointDeformer)

**Abstract**

我们提出了KeypointDeformer，一个利用自动检测3D keypoints实现shape control的新颖的unsupervsied方法。我们将这个问题描述为将一个source 3D object和一个target 3D object对齐的过程，这两个object是同一类的。我们的方法通过比较它们的latent representations来分析这两个objects的shapes的区别。这个latent representation是由3D keypoints来表示的，而这些3D keypoints是通过unsupervised的方式学出来的。source和target的3D keypoints之间的差异会让shape deformation algorithm明白如何将source object变形为target object。整个模型是end-to-end的，模型对于检测3D keypoints和学习利用3D keypoints来deform object shapes这两个任务是同时进行的。我们的方法对于shape deformations提供了intuitive和semantically consistent的control。而且，我们所发现的3D keypoints对于object category instances是consistent的，即使它们有很大的shape variations。因为我们的方法是unsupervised的，其可以轻易的被部署在任何一个新的object类别上，而不需要任何的3D keypoints或者deformation的标注。

**1. Introduction**

现在在Internet上有非常多的3D shapes，给用户提供intuitive和简单的interface，让他们可以在保留关键shape性质的情况下对3D object做semantically manipulating，在AI辅助的3D content creation领域有着非常多的应用。在这篇文章里，我们为interative editing提出自动检测intuitive和semantically有意义的control points，让对于每个object类别的3D模型保留细节的shape deformation成为可能。

更准确的说，我们将3D keypoints作为shape editing的intuitive和simple interface。Keypoints是那些在一个object类别所有的3D shape间都semantically consistent的3D points。我们提出一个学习框架使用非监督学习的方式来找到这样的keypoints，并且设计一个deformation model来利用这些keypoints在保留局部形状特征的前提下改变物体的shape。我们叫这个模型KeypointDeformer。

Fig 1描述了KeypointDeformer在inference时候的过程。给一个新的3D shape，KeypointDeformer在它的surface上预测3D keypoints。如果一个用户将chair leg上的keypoint向上移动，整个chair leg都会超相同的方向形变（fig 1下面一行）。我们的模型在这些可操纵的keypoints上提供了可选择的categorical deformation prior，比如说如果一个用户将一个airplane一侧wing上的keypoints向后移动，这一侧的wing会整体向后移动，而另一侧的wing也会随之移动同样的程度（fig 1上面一行）。当用户仅仅希望移动一侧wing的时候，我们的方法同样也允许这种操作。我们的模型可以仅仅对于shape进行editing，也可以对两个shapes做shape alignment，还可以生成新的shapes来扩充datasets。

![overs]({{ '/assets/images/DEFORM-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1 用非监督学习到的3D keypoints来进行shape deformation。我们用非监督学习的方式学习到的3D keypoints可以对object的shape进行intuitive控制。这个figure显示了交互式控制的独立的步骤。红色的箭头标注了keypoints被操作要移动的方向。注意到移动keypoints造成的shape形变是局部的并且object shape是按照intuitive的方式形变的：比如说，将airplane的wing上的keypoint向后拉，则整个wing也会向后倾斜，而保持原本shape的细节不变化。*

尽管3D keypoints对于shape editing来说很有效果，但是获取3D keypoints和deformation models的明确的监督信息不仅很贵，而且是ill-defined的。从而，我们提出了一个unsupervised框架来将寻找3D keypoints和构建deformation model这两个任务同时完成。为了解决我们的问题，我们设计两个在一起作用的模块：1）一个detecting keypoints的方法；2）一个deformation model将keypoints的位移信息传递到shape的其它部分。为了达成上述两个目的，我们设计了一个学习任务来将一个source shape align到一个target shape上，这两个shapes可以是同一个object category里差别很大的两个instances。我们同时提出了一个简单有效的keypoint regularizer，其会促进模型学习到semantically consistent的keypoints，而且这些keypoints分布的很好，靠近object的surface并且隐式的保留着shape symmetries。我们训练所得到的就是一个deformable model，可以基于自动监测到的3D control keypoints来deform一个shape。因为keypoints是低维的，我们还可以在这些keypoints上学习一个category prior，这样就可以进行semantic shape editing了。

总而言之，我们的方法有以下几个关键的优势：

* 其给了用户一个intuitive并且简单的方法来交互式的控制object shapes
* keypoint prediction和deformable的模型都是unsupervised的
* 由我们的方法所找到的3D keypoints对于shape control来说比其他的keypoints都要好，包括人为标注的
* 我们的unsupervised 3D keypoints对于同一类别的object的不同的instances来说是semantically consistent的，从而给了我们sparse correspondences。

我们在标准的benchmarks上测试了我们的unsupervised 3D keypoints的semantic consistency，并在unsupervised方法中获得了sota的结果。我们同时也展现了由我们的方法找到的keypoints对于shape deformation来说非常的合适。最后，我们提供了用户交互时操作shape control的实例，并且在project主页上包含了交互式shape control的视频。


**2. Related Work**

**2.1 Shape deformation**

我们的方法和geometric modeling里的detail-preserving deformations十分相关，包括[Laplacian-based shape editing](http://mesh.brown.edu/dgp/pdfs/sorkine-cgf2006.pdf)，[As-Rigid-As-Possible shape deformation](https://diglib.eg.org/bitstream/handle/10.2312/SGP.SGP07.109-116/109-116.pdf?sequence=1&isAllowed=n)和[cages](https://www.cse.wustl.edu/~taoju/research/meanvalue.pdf)。这些方法通过各种类型的限制（比如说points在一个optimization框架里）来允许进行shape editing，但它们一个最主要的问题就是它们仅仅依赖于geometric properties而并没有考虑到semantic attributes或者category-specific shape priors。这样的priors可以通过利用stiffness性质给object surface涂色获得，或者从一系列已经知道correspondence的meshes上学习得到。然而，这种监督信息十分昂贵，而且对于新的shapes来说就不管用了（training set没有见过的shapes，或者有新的priors的shapes）。[Yumer的文章](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.723.3455&rep=rep1&type=pdf)用一个提供了多个控制shape的sliders的data-driven的框架来解决了这个问题。然而这个方法需要一系列从专家标注的信息中提取的predefined attributes。我们提出了一个unsupervised方法，并且为用户提供了一种利用keypoints直接控制deformation的方法，更加的semantic。而且， 我们的方法可以基于检测到的3D keypoints来进行category-specific deformation，允许通过几个keypoints的editing来实现semantically consistent的editing（比如说，只调整airplane一侧wing的keypoint也会使得另一侧wing deformation）。

另一个相关的问题是[deformation transfer](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.6553&rep=rep1&type=pdf)，也就是利用两个shapes之间已知的correspondences将source mesh上的deformation转移到target mesh上。近期有些工作利用deep learning来隐式的学习shape correspondences来align两个shapes，比如[1](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yifan_Neural_Cages_for_Detail-Preserving_3D_Deformations_CVPR_2020_paper.pdf)，[2](https://arxiv.org/pdf/1804.08497.pdf)和[3](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_3DN_3D_Deformation_Network_CVPR_2019_paper.pdf)。我们也是用一个shape alignment的目标函数来训练我们的网络，但是我们在中间层的control的时候显式的表示使用的是3D keypoints，其也可以被用来仅仅做shape editing。和前面的利用priors的方法比，他们一直都需要一个target shape来表示所需要得到的deformation。

**2.2 User-guided shape editing**

我们的方法和最近的利用deep learning来学习可以提供对shape做interactive editing的generative模型。[Tulsiani的文章](https://openaccess.thecvf.com/content_cvpr_2017/papers/Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper.pdf)用primitives来抽象代表shapes，然后通过surface的primitives的deformation来edit shape。但是，shape editing并不是它们主要的目标，而且也不清楚直接进行primitive transformation能多大程度的保留local shape details。近似的工作进一步改进了这个方法，他们通过学习一个[point-based](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hao_DualSDF_Semantic_Shape_Manipulation_Using_a_Two-Level_Representation_CVPR_2020_paper.pdf)、[shape handles](https://openaccess.thecvf.com/content_CVPR_2020/papers/Gadelha_Learning_Generative_Models_of_Shape_Handles_CVPR_2020_paper.pdf)或者[disconnected shape manifolds](https://openaccess.thecvf.com/content_ICCV_2019/papers/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.pdf)的primitives的generative model来改进原先的基于primitives的model的缺点。这些方法通过找到最佳匹配用户editing的latent primitive representations来做到interactive editing。但是他们的方法所用到的用户interface比较复杂，需要素描或者直接操控primitives。而且最关键的，因为这些editing是基于generative models的，这些方法可能会改变original shape的local details。而相对而言，我们直接对原shape进行deform，会有更好的shape detail的保留。我们将提出的方法和[DualSDF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Hao_DualSDF_Semantic_Shape_Manipulation_Using_a_Two-Level_Representation_CVPR_2020_paper.pdf)的结果进行对比来阐述上述的优势。


**2.3 Unsupervised keypoints**

在2D keypoint discovery领域，unsupervised方法有很多论文都已经有了不错的结果，[4](https://proceedings.neurips.cc/paper/2018/file/1f36c15d6a3d18d52e8d493bc8187cb9-Paper.pdf)，[5](https://openaccess.thecvf.com/content_CVPR_2020/papers/Jakab_Self-Supervised_Learning_of_Interpretable_Keypoints_From_Unlabelled_Videos_CVPR_2020_paper.pdf)，[6](https://arxiv.org/pdf/1808.06882.pdf)，[7](https://openaccess.thecvf.com/content_ICCV_2019/papers/Thewlis_Unsupervised_Learning_of_Landmarks_by_Descriptor_Vector_Exchange_ICCV_2019_paper.pdf)[8](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)[9](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)，但在3D keypoint discovery领域，unsupervised的方法却还没有被研究完全。[Suwajanakorn的文章](https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf)利用3D pose information作为supervision来从两张关于同一个object的不同角度的图片检测3D keypoints。我们这篇文章聚焦于在3D shapes上学习3D keypoints。[Chen的文章](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Unsupervised_Learning_of_Intrinsic_Structural_Representation_Points_CVPR_2020_paper.pdf)输出一个结构化的3D representation来获取sparse或者dense的shape correspondences。和我们这篇文章里进行3D keypoints discovery方法最像的文章是[Fernandez的这篇文章](https://arxiv.org/pdf/2003.07619.pdf)，他们采用了显式的symmetric限制条件。在我们这篇文章里，我们是为了shape control这个目的来利用unsupervised方法来进行keypoints的检测。尽管我们主要关注shape editing，我们的方法可以在semantic consistency 3D keypoints搜索任务上达到sota的效果。这样的unsupervised的3D keypoints检测方法可以在control领域用作一个latent representation，他们目前还需要手动定义3D keypoints来作为一个监督信号。


**3. Method**

我们的目标是学习一个keypoint detector，$$\Phi: x \longrightarrow p$$，来将一个3D object shape $$x$$映射到一个semantically consistent的3D keypoints的集合$$p$$。我们同时也想学习一个输入为keypoints的conditional deformation model，$$\Psi: (x, p, p^{'}) \longrightarrow x^{'}$$，将shape $$x$$利用deformed control keypoints映射到shape $$x^{'}$$，其中$$p$$描述的是initial（source）keypoint locations，$$p^{'}$$描述的是target keypoint locations。为keypoints和deformation model获取显式的监督信息十分expensive而且ill-defined。因此，我们提出了一个unsupervised的learning框架来训练上述提到的两个functions。我们通过设计了一个pair-wise shape alignment的辅助任务来实现，这个辅助任务的核心想法就是将keypoints learning和deformation model联合起来学习，从而可以对两个任意的shapes做alignment。更仔细地说，我们的模型首先利用一个Siamense network在source和target shapes上预测3D keypoint locations。之后我们利用检测到的keypoints的对应关系来deform source shape（检测到的keypoints是默认有序的，从而有着对应关系）。为了保持local shape detail，我们使用了一个基于keypoints的cage-based deformation方法。我们使用了一个新颖的但十分简单高效的keypoint regularization term，使得keypoints是well-distributed的，并且距离object surface很近。Fig 2显示了我们的模型的整体框架。

![framework]({{ '/assets/images/DEFORM-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. Model。我们的模型使用预测到的unsupervised keypoints $$p$$和$$p^{'}$$来将source shape $$x$$ aligns到target shape $$x^{'}$$。unsupervised keypoints描述了object的pose并用作deformation的control points。整个模型使用在deformed source shape $$x^{\*}$$和target shape $$x^{'}$$之间的similarity loss和keypoint regularization loss来进行end-to-end的训练。在interactive shape manipulation的test time，用户可以选择只输入一张source shape $$x$$，keypoint predictor $$\Phi$$就会预测一些unsupervised 3D keypoints $$p$$出来。然后用户可以手动控制keypoints $$p$$使其变成target keypoints$$p^{'}$$，然后再用deformation model $$\Psi$$来生成deformed source shape $$x^{\*}$$，如fig 1，fig 9或者project page上的补充材料里的视频所示。*

**3.1 Shape Deformation with Keypoints**

我们将每个object表示为一个point cloud $$x \in R^{3 \times N}$$，是从object mesh里均匀采样得来的。我们先从source和target里预测keypoints。keypoint predictor $$\Phi$$使用$$x$$作为输入，输出一个ordered set，$$p = (p_1,...,p_K) \in R^{3 \times K}$$，表示的是3D keypoints。这个keypoint predictor的encoder对于source和target是公用的，使用Siamese architecture来实现。而shape deformation function $$\Psi$$的输入是source shape $$x$$，source keypoints $$p$$和target keypoints $$p^{'}$$。在test阶段，输入一张图片，得到了其的source keypoints，用户editing之后得到了target keypoints，之后生成输入图片的deformation，整个interactive shape deformation过程如fig 2所示。

为了在deform object的过程中保持它的local shape details，我们使用最近刚出现的[differentiable cage-based deformation algorithm](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yifan_Neural_Cages_for_Detail-Preserving_3D_Deformations_CVPR_2020_paper.pdf)。cages是个很经典的shape modeling方法，其使用一个粗糙的封闭的mesh将shape包起来。deform cage mesh就会导致里面包裹的shape也发生deformation。cage-based deformation function $$\beta: (x,c,c^{\ast}) \longrightarrow x^{\ast}$$的输入是source control cage $$c$$，deformed control cage $$c^{\ast}$$，以及source shape $$x$$（也就是一开始从mesh里采样得到的point cloud）。我们通过一开始用一个球体包住source shape $$x$$，之后再将每个cage vertex $$c_V$$向object的中心推进直到它和object surface之间只有一个很小的距离这样一种方法来为每个shape都自动的获取包裹其的cage。fig 2显示了所得到的cage的样子。尽管cages对于shape-preserving deformation来说是个有用的方法，但通过deform cages来获得内部的shape的deformation并不是那么的直观，特别是对新手用户来说，因为cage vertex并不直接落在shape的表面上，并没有一个粗糙的structure，而且在不同的shape之间（同一个object或者同一类object的不同姿态的shape）并不semantically consistent。我们提出keypoints用作操纵cage deformation的方式更为合理。

为了用我们检测到的keypoints来控制object deformation，我们需要将这些keypoints和cage vertices联系起来。我们通过使用一个linear skinning function，首先计算source和target keypoints之间的relative distance，$$\delta p = p^{'}-p$$，然后将一个可学习的influence matrix，$$W \in R^{C \times K}$$乘上$$\delta p$$，在加到source cage vertices，$$c_V$$上，就获得了新的target cage vertices，$$c_{V}^{\ast}$$。其中，$$p,p^{'},\delta p$$都是$$K \times 3$$的矩阵，$$c_V, c_V^{\ast}$$是$$C \times 3$$的矩阵，而$$K$$和$$C$$分别表示keypoints和cage vertices的个数。所以deformed cage vertices，$$c_V^{\ast}$$计算方式为：

$$c_V^{\ast} = c_V + W \delta p$$

为了满足对于每个shape来说cage是唯一的这样的事实，我们将上述的influence matrix，$$W$$，设置为输入shape $$x$$的一个函数。详细的说，influence matrix是一个composition，$$W(x) = W_C + W_I(x)$$，其中$$W_c$$是对于每一类object的所有instances都共用的canonical matrix，而$$W_I(x)$$则是每个instance独自的offset，是利用influence predictor $$W_I = \Gamma(x)$$以source shape $$x$$为输入计算而来。我们同时也通过最小化其Frobenius norm来regularize这个instance specific offset，$$W_I$$，为了防止它过拟合influence matrix $$W$$。我们将这个regularizer命名为$$L_{inf}$$。最后，我们限制$$W$$使得每个keypoint最多只能影响$$M$$个最近的cage vertices来实现locality。


**3.2 Losses和Regularizers**

我们的KeypointDeformer是通过最小化source和target shape之间的similarity loss，再加上keypoint regularization loss和instance-specific influence matrix regularization term，利用SGD实现的end-to-end的训练。

**Similarity loss**

理想情况下，我们希望利用已知的meshes之间的correspondence来计算deformed source shape $$x$$和target shape $$x^{'}$$之间的similarity。但是这样的correspondence是不存在的，因为我们希望能在最普遍的object category CAD模型上训练。我们通过将source shape和target shape都表示为point cloud，然后再计算他们之间的Chamfer distance来近似这个similarity loss。这个loss记为$$L_{sim}$$。


**Farthest Point Keypoint regularizer**

我们提出了一个简单有效的keypoint regularizer $$L_{kpt}$$来使得预测的keypoints $$p$$是well-distributed的，也就是再object surface上，并且能保持这个shape category本身的symmetric structure。具体来说，我们设计了一个**Farthest Sampling Algorithm**来从输入的shape mesh里采样一个无序集合$$q = \{q_1,...,q_J\} \in R^{3 \times J}$$作为point cloud。采样的起始点是随机的，所以每次我们计算这个regularization loss的时候我们都使用的不同的point cloud $$q$$。给定这些随机的farthest points，regularizer最小化所预测的keypoints $$p$$和这些采样到的点$$q$$之间的Chamfer distance。也就是说，这个regularizer希望keypoint detector $$\Phi$$能够学习到那些和$$q$$分布类似的keypoints。Fig 3展示了$$q$$的特性。这些采样到的点对于提供了输入的object shape $$x$$的一个均匀的覆盖，其再不同的instances之间比较稳定，而且保持了最初input shape的symmetric结构。

![LOSS]({{ '/assets/images/DEFORM-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 3. Farthest Point Keypoint regularizer. 我们使用一个随机的初始点来做farthest point sampling，来regularize预测到的keypoints。(a) 展示了对于一个给定的点，其被farthest point sampling algorithm所选到的频率。颜色越深表明这个点被选到的概率越大。采样点的期望locations对原shape有一个很好的覆盖而且保留了原shape的symmetry特征。而且，它们的子集在不同的object instances之间保持了semantically stable。使用采样点的期望locations作为keypoint location的prior效果很好，因为keypoint predictor会学会对这些采样点里的噪声比较robust。在airplane的例子里我们可以看到，fuel tank的顶点（红圈标记）并没有被keypoint predictor用作keypoint，(b) 而wing的顶点（绿圈）则被选中为keypoint，因为其在数据集里更加consistent（大多数飞机都有wings，但很多并没有fuel tank）。* 

这个regularization的另一个intuition是我们可以将这些采样的farthest points $$q$$理解为keypoint locations的一个noisy prior。这个prior并不是完美的——在某些shape上可能会遗失某些重要的点，或者有一些不合理的点——但是neural network keypoint predictor会以一种对这些noise robust的方式学到keypoint的locations，而且会偏向于学习那些consistent的keypoints，如fig 3所示。

**Full objective**

总结来说，我们的training objective是：

$$L = L_{sim} + \alpha_{kpt}L_{kpt} + \alpha_{inf}L_{inf}$$

其中$$\alpha_{kpt}$$和$$\alpha_{inf}$$是scalar loss系数。我们的方法很简单而且并不需要对于shape deformation增加额外的shape specific regularization，比如说[这篇文章]()里所用的point-to-surface距离，normal consistency，symmetry losses。这是因为keypoints提供了一个shapes之间的low-dimensional的correspondence，而且cage deformations是这些keypoints的一个linear function，从而阻止了那些会导致local deformation的极端的deformations。


**3.3 Categorical Shape Prior**

因为我们利用一系列semantically consistent的keypoints来代表一个object shape，我们可以通过计算training set里的shape对应的keypoints的PCA来获取categorical shape prior。这个prior可以用来指导keypoint manipulation，也就是上面提到的$$W_C$$。比如说，如果用户想要改变一个airplane一个wing上的一个keypoint，根据寻找到能够最佳重构这个被改变的keypoint的新位置的PCA basis coefficients，其余的keypoints就会被这些basis coefficients”同步协调“。从而这些keypoints就会根据这个prior（也就是这个PCA）落到新的位置。这个prior还可以通过采样一系列新的keypoints来生成新的shapes：调整某些keypoints，然后PCA经过计算basis coefficients来对所有的keypoints位置进行调整，从而得到了新的keypoints位置，然后利用上述的deformation model来生成新的shape，就可以将这个新的shape加入已有的3D shape datasets里。


**4. Experiments**

**5. Conclusion**

我们提出了一个利用自动检测的semantic 3D keypoints和一个利用keypoints来联合学习到的deformation model对3D objects的shape进行控制的方法。KeypointDeformer model为用户提供了一个简单的interface用来进行interactive shape control。这个方法的一个limitation是我们的方法假设shape collections是aligned的。但是，在我们对真实的扫描数据进行实验的时候发现，已有的自动alignment方法就已经够用了。另一个limitation是keypoint representation并不允许对object某一部分的rotation进行建模。在这篇文章里，我们聚焦于shape control和keypoint prediction任务，然而3D keypoint在很多其它应用里都还有很多应用。将我们的unsupervised 3D keypoints detection方法应用在其它的应用领域也是很有意思的。



### [Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point Sets](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700545.pdf)

*Clara Fernandez-Labrador, Ajad Chhatkuli, Danda Pani Paudel, Jose J. Guerrero, Cedric Demonceaux, Luc Van Gool*

*ECCV 2020*

>又是ethz Luc Van Gool大佬的作品。

**Abstract**

从同一类别的objects的集合里自动找到category-specific的3D keypoints是一个非常有挑战性的问题。当objects是由3D point clouds表示的时候问题难度就更大了，因为shape和semantic parts都有变化，而且我们也没有coordinate frames的信息。如果keypoints能够表示objects的shape以及所有的objects的keypoints之间的有序的correspondencs能够很容易的被构建，那么我们就能将这种keypoints叫做category-specific的。这篇文章要用一种unsupervised方法来学习上述这种3D keypoints，数据集是一批同一个未知种类的objects的misaligned 3D point clouds。为了能够实现这个目标，我们利用不知道symmetry plane的symmetric linear basis shapes来对某个种类的objects的keypoints进行建模。使用symmetry prior使得我们能够在misalignment很严重的情况下仍然学到稳定的keypoints。在我们的认知范围内，这是第一篇直接同一未知类别的objects的point clouds上学习3D keypoints的文章。基于四个公开的benchmark datasets，我们定量定性的阐述了我们的方法的效果。我们的实验同时还表明通过这种方法所学到的3D keypoints是geometrically和semantically consistent的。

**1. Introduction**

能够表示任何类别的object的keypoints的方法在geometric reasoning界一直都备受关注，因为其很简单而且易于操作。基于keypoints的方法对于很多vision任务来说都是至关重要的，包括：3D reconstruction，registration，human body pose，recognition和generation。但是很多keypoints是手动被定义的，是通过考虑它们的semantic locations比如说facial landmarks，human body joints等来手动确定这些keypoints的位置的。为了更好的从keypoints上获利，有一些研究尝监督试学习检测keypoints，或者非监督的自动检测keypoints。通过监督数据来学习检测keypoints这个任务获得了很大的成功。然而，从没有标签的3D数据中直接获取3D keypoints——来表示有意义的shapes和semantics——从而使得这些keypoints的作用和手动定义的那些一样，因为很困难并没有受到过多的关注。

因为objects本身就是位于三维空间内，所以3D keypoints对于geometric reasoning任务效果更好是很自然的。对于给定的3D keypoints，它们在2D图片中的counterpart可以很简单的用camera projection来实现。然而，能直接从3D数据（point clouds）上预测keypoints会有优势，因为很多时候multiple camera views或者multiple images是不可获取的。在这篇文章里，我们关注如何从3D point clouds中直接学习keypoints locations。实际上，带有keypoints的3D structures对于很多的应用包括registration，shape completion，shape modeling都是足够使用的了，并不需要它们的2D counterparts。

因为deformation或者对比同一个类别不同的objects，3D objects就会有shape variations，找到consistent的keypoints对于geometric reasoning就很重要了。回忆一下semantic keypoints比如说facial landmarks和body joints。为了达到同样的目的，我们可不可以自动找到这些keypoints，其对于同一个类别的同一个objects的deformation以及同一个类别的不同objects之间的差异都是consistent的？这是我们这篇文章主要想要回答的问题。更进一步，我们想要用unsupervised方法直接从3D point clouds上找到这样的keypoints。我们将这些keypoints称为category-specific，它们被期望于能描述objects的shape信息，并能够在同一类别的所有objects之间保持有序的对应关系。更加严格地说，我们将category-specific keypoints所需要满足的性质定义为：1）对于同一类别的具有不同shapes或者不同alignments的不同的objects具有很好的泛化效果，也就是说对于形状不同或者没有对齐的同一个类别的不同的objects，找到的仍然是这些keypoints；2）能做到一对一的有序的对应并且具有semantic consistency；3）在保持shape symmetry的情况下能够代表这一类别的object的形状特征。这些性质不仅使得这些keypoints变得有意义，同时也加强了其的应用价值。但是在3D point clouds上学习category-specific keypoints是一个非常难的问题。而如果数据还是misaligned并且要使用unsupervised方法，那这个问题就更难了。相关的工作并没有全部考虑这些因素，而是只考虑了一部分：[USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.pdf)使用aligned数据并且并不针对某一个具体的类别的object，[6-DoF Object Pose from Semantic Keypoints](https://arxiv.org/pdf/1703.04670.pdf)在2D图像上使用监督信息，[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf)使用aligned 3D数据并且使用多张知道pose的2D图片，其在没有显式考虑shapes的情况下获得了category specific的3D keypoints。[3D Landmark Model Discovery from a Registered Set of Organic Shapes](https://clementcreusot.com/publications/papers/creusot2012-PCP.pdf)使用预先定义好的local shape descriptors和一个template模型，而且只针对faces。

在这篇文章里，我们说明拥有上述性质的category-specific keypoints可以用unsupervised的方式，通过基于未知的linear basis shapes结合non-rigidity来对它们建模的方式，学习到。在考虑具有对称性的object类别的时候，我们还在deformation模型上加入了未知的reflective symmetry。对于那些没有对称性的object类别，我们使用symmetric linear basis shapes来对所谓的symmetric deformation spaces进行建模，比如说human body deformations。我们所提出的learning方法并不需要假设shapes是aligned好的、或者预先计算好了basis shapes或者已经知道了对称平面，所有的这些值都是通过end-to-end的方式学到的。我们的模型相比较于之前NRSfM([Multiview Aggregation for Learning Category-Specific Shape Reconstruction](https://arxiv.org/pdf/1907.01085.pdf)和[Symmetric Non-Rigid Structure from Motion for Category-Specific Object Structure Estimation](https://arxiv.org/pdf/1609.06988.pdf))里的方法来说十分有效而且灵活。我们通过将一个object类别的shape basis和对称平面都作为neural network的参数来学习的办法来实现。训练的时候每次输入是一个3D point cloud，并没有使用Siamese-like的模型结构。在inference的时候，网络会预测basis coefficients和pose，再用来估计instance-specific keypoints。使用前面提到的四个benchmark datasets里的多个类别的objects，我们定量且可视化定性的衡量了我们所提出方法的有效性。我们的实验证明我们的方法找到的keypoints是geometrically和semantically consistent的，这是通过相同instances的registration以及part-wise的semantic assignment来衡量的。我们进一步显示symmetric basis shapes可以被用于对每类object的symmetric deformation space进行建模，比如说human body。


**2. Related Work**

objects的category specific keypoints在NRSfM方法里用得很多，但是却很少有论文来研究如何找到它们。考虑模型的输出，我们的方法和[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://keypointnet.github.io/keypointnet_neurips.pdf)很像，这篇文章通过解决一个辅助任务来学习category-specific 3D keypoints，这个辅助任务是学习同一个object的不同角度的view之间的rigid transformation，而且这个方法假设图片是aligned好的。尽管这个方法在2D和3D上都显示了比较好的结果，但是其并没有显式的对shape进行建模。结果就是，这个方法需要同一个object的不同角度的照片是aligned好的，从而才能计算keypoint correspondences。在[6-DoF Object Pose from Semantic Keypoints
](https://arxiv.org/pdf/1703.04670.pdf?ref=https://githubhelp.com)里同样对于六自由度estimation考虑了类似的任务，其使用了一个low-rank shape prior来为3D keypoints提供condition。尽管low-rank shape modeling是个很有力的工具，但这篇文章还是需要对heatmap prediction进行监督，而且依赖于aligned shapes和预先计算好的shape basis。[Single Image 3D Interpreter Network](https://arxiv.org/pdf/1604.08685.pdf)同样也使用low-rank shape prior，但是它们的训练完全基于监督的方法。而且，上述所有的方法都是从images上利用heatmaps的方式学习到keypoints，再将其提升到3D空间内的。不同于上述的这些共工作，[3D Landmark Model Discovery from a Registered Set of Organic Shapes
](https://clementcreusot.com/publications/papers/creusot2012-PCP.pdf)使用了deformation model和symmetry来直接从3D数据上预测keypoints，但是其需要一个face template，aligned shapes以及已知的shape basis。某一类别的所有objects的shape modeling再NRSfM工作中早就已经被研究透了。linear low-rank shape basis，low-rank trajectory basis，isometry或者piece-wise rigidity是NRFfM的提出的不同的方法。最近，有一些工作使用low-rank shape basis来设计可被学习的模型。另一个能被用来进行model shape category的方法是reflective symmetry，其也和object pose密切相关。尽管[Symmetric Non-Rigid Structure from Motion for Category-Specific Object Structure Estimation](https://yuan-gao.net/pdf/ECCV2016%20-%20Sym-NRSfM.pdf)说明low-rank shape basis可以使用未知的reflective symmetry进行构造，如何将其改造成可学习的NRSfM方法并不简单。还有最近的工作假设symmetry plane是从几个已知的planes里挑选出来的。更进一步的是，这些方法都没有为non-rigidly deforming objects比如说human body构造symmetry。[Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Unsupervised_Learning_of_Probably_Symmetric_Deformable_3D_Objects_From_Images_CVPR_2020_paper.pdf)在一个warped canonical空间内概率化的考虑模型symmetry，来对不同objects进行3D reconstruction。

shape modeling是我们工作的一个重要的方面，另一个重要的方面是如何从一个unordered point set上学习到ordered keypoints。尽管在deep neural networks领域对于point sets来说有了好几个很好的成果，但这些在images上有用的方法对于point sets来说就不管用了。一个[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf)利用一个Siamese architecture通过一个正确预测rotation的代理任务来用非监督学习的方式预测3D keypoints。[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)利用alignment作为代理任务来预测keypoints。在下面的章节里，我们将会展示如何利用low-rank symmetric shape basis来对shape instances进行建模，以及如何利用shape modeling来预测有序的category-specific keypoints。


**3. Background and Theory**

**Notations**

我们利用花体拉丁字母（比如$$\mathcal{V}$$）或者加粗大写拉丁字母（比如**V**）来表示sets和matrices。小写或者大写的普通字母，比如K来表示scalars。小写的加粗拉丁字母来表示vectors，比如**v**。小写的拉丁字母来表示indices，比如$$\mathcal{i}$$。大写的希腊字母来表示mappings或者functions，比如$$\Pi$$。我们使用$$\mathcal{L}$$来表示loss functions。operator mat将一个向量**v** $$\in R^{3N \times 1}$$转换为一个矩阵**M** $$\in R^{3 \times N}$$。

**3.1 Category-specific Shape and Keypoints**

我们的shape使用point clouds表示的，由一个无序的points集合来表示$$S=(s_1, s_2, \cdots, s_M),s_j \in R^3, 1 \leq j \leq M$$。所有的同一个类别的这样的point clouds定义的shape组成了category shape space $$\mathcal{C}$$。我们将$$\mathcal{C}$$里的第i个category-specific shape instance记为$$S_i$$。category shape space $$\mathcal{C}$$可以是一系列离散的shapes，也可以是由deformation function $$\Psi_C$$生成的一个光滑的category-specific shapes流形。我们这篇文章关注点在于从point cloud $$S_i$$里学到有用的3D keypoints。为了达到这个目标，我们这一节定义category-specific keypoints，并且介绍生成keypoints的模型。

*category-specific keypoints* 我们将一个shape $$S_i$$的category-specific keypoints表示为一系列的points，$$P_i = (p_{i1}, p_{i2}, \cdots, p_{iN}), p_{ij} \in R^3, 1 \leq j \leq N$$。和shape $$S_i$$不同，这个集合$$P_i$$是有序的。我们的目标就是学习一个mapping $$\Pi_C: S_i \longrightarrow P_i$$来为$$\mathcal{C}$$内任意一个shape $$S_i$$学习到category-specific keypoints。在之前我们已经定义了category-specific的keypoints应该是什么样的。如果用数学语言来描述的话就是这样的：

* 1) Generalization: $$\Pi_C(S_i) = P_i, \forall S_i \in C$$
* 2）corresponding points and semantic consistency: 给定$$S_a, S_b \in \mathcal{C}$$，我们希望$$P_{aj} \iff P_{bj}$$。而且$$P_{aj}$$和$$P_{bj}$$需要有相同的semantics。
* 3) representative-ness: $$vol(S_i) = vol(P_i)$$以及$$p_{ij} \in S_i$$，其中$$vol(.)$$是一个对于shape计算volume的算子。如果$$S_i \in \mathcal{C}$$有reflective symmetry，那么$$P_i$$也得有相同的symmetry。


**3.2 Category-specific Shapes as Instances of Non-rigidity**

近期有一些工作将category shapes里的不同的instances利用non-rigid deformations来进行建模（[C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion](https://openaccess.thecvf.com/content_ICCV_2019/papers/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf)，[Single Image 3D Interpreter Network](https://dspace.mit.edu/handle/1721.1/114448)，[Multiview Aggregation for Learning Category-Specific Shape Reconstruction](https://proceedings.neurips.cc/paper/2019/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)，[Deep Non-Rigid Structure from Motion](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf)）。这个想法的基于这些shapes通常都有geometric similarities这样一个事实。结果是，存在一个deformation function $$\Psi_C: S_T \longrightarrow S_i$$，将一个global shape property $$S_T$$（shape template或者basis shapes）映射到一个category shape instance $$S_i$$。然而，对$$\Psi_C$$进行建模是很困难的，而且在很多情况下$$\Psi_C$$可能并不存在一个很简单的表述。这个问题，就是为什么dense Non-rigid Structure from Motion (NRSfM)任务这么困难的原因。从另一个角度来看，我们可以考虑一个deformation function $$\Phi_C: P_T \longrightarrow P_i$$，从global keypoints property $$P_T$$映射到每个instance的category-specific keypoints $$P_i$$。$$\Phi_C$$要满足，$$\Phi_C$$的一对对应点，也是$$\Psi_C$$的一对对应点。并且如果以点对来定义$$\Psi_C$$和$$\Phi_C$$，应该有$$\Phi_C \subset \Psi_C$$。和$$\Psi_C$$不同的是，$$\Phi_C$$的建模可能会很简单。因此，我们选择在keypoints空间内$$P = (P_1, P_2, \cdots, P_L)$$来寻找non-rigidity modeling。而所学到的$$\Phi_C$$就可以是$$\Psi_C$$的一个抽象。non-rigidity可以被用来定义prediction function $$\Pi_C$$：

$$\Pi_C(S_i;\theta) = \Phi_C(r_i;\theta) = P_i \tag{1}$$

其中$$\theta$$是$$\Pi_C$$的函数parameters，$$r_i$$是每个instance特有的vector parameter。在我们的设定下，我们希望能从$$\mathcal{C}$$的shape instances里用非监督的方式来学习到参数$$\theta$$。在NRSfM的设定里，对shape deformation进行建模的两种常见方法是low-rank shape prior和isometric prior。在这篇文章里，我们对instance-wise symmetry和deformation space的symmetry使用low-rank shape prior进行建模。

**3.3 Low-rank Non-rigid Representation of Keypoints**

NRSfM关于low-rank shape basis的方法是rigid orthographic factorization prior的一个自然的拓展，在[Recovering Non-Rigid 3D Shape from Image Streams](http://vision.jhu.edu/reading_group/Bregler2.pdf)和[Shape and motion from image streams under orthography: a factorization method](http://users.eecs.northwestern.edu/~yingwu/teaching/EECS432/Reading/Tomasi_TR92.pdf)。关键的想法是很大一部分object deformations都可以用$$K$$个不同pose的basis shapes的线性组合来表示，而且这个$$K$$是个不大的数。在rigid的情况下，这个$$K$$就是1。在non-rigid的情况下，这个$$K$$大一些，具体的数字取决于deformations的复杂程度。考虑$$\mathcal{C}$$里$$F$$个shape instances，并且在每个shape instance的keypoints instance $$P_i$$里考虑$$N$$个点。下面的式子描述了使用shape basis进行的projection：

$$P_i = \Phi_C(r_i;\theta) = R_i mat(B_C c_i) \tag{2}$$

其中$$B_C = (B_1, \cdots, B_K), B_C \in R^{3N \times K}$$构成了low-rank shape basis。vector $$c_i \in R^K$$表示对于instance $$i$$的不同的shape basis的线性系数。从而每个instance的keypoints就可以完全被basis $$B_C$$和系数$$c_i$$所表示了。之后，projection matrix $$R_i \in SO(3)$$就是instance $$i$$的rotation matrix。$$mat(.)$$表示将得到的$$3N \times 1$$的矩阵转换为$$3 \times N$$的矩阵。

在我们这个问题里，$$P_i, c_i, B_C, R_i$$都是不知道的。我们要通过上述的式子来学习，其中$$\theta$$包括了$$\Phi_C$$的function parameter，basis $$B_C$$，而$$r_i$$则包括了instance-wise pose $$R_i$$和系数$$c_i$$。


**3.4 Modeling Symmetry with Non-Rigidity**

很多object种类的shapes都有固定的reflective symmetry。为了寻求并使用这种symmetry，我们考虑两个不同的priors：instance-wise symmetry和symmetric deformation space。

*Instance-wise symmetry* 关于一个固定平面的instance-wise reflective symmetry在很多rigid object类别上都能被观察到（比如[A Scalable Active Framework for Region Annotation in 3D Shape Collections](https://dl.acm.org/doi/pdf/10.1145/2980179.2980238)和[3D ShapeNets: A Deep Representation for Volumetric Shapes](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wu_3D_ShapeNets_A_2015_CVPR_paper.pdf))。在NRSfM里，这样的symmetry被和shape basis prior结合起来使用过。然而，用来同时学习symmetry和shapes的一个简便的representation还未被研究过。最近的learning-based的工作[Multiview Aggregation for Learning Category-Specific Shape Reconstruction](https://proceedings.neurips.cc/paper/2019/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)和[Normalized Object Coordinate Space for Category-Level
6D Object Pose and Size Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2019_paper.pdf)通过在几个平面上穷举式的寻找来使用symmetry prior，然后再用来预测symmetric dense non-rigid shapes。但是这样的方法在shapes并没有aligned的情况下是不行的。我们将公式2改造一下就可以加入instance-wise symmetry：

$$P_{i\frac{1}{2}} = R_i mat(B_{C\frac{1}{2}}c_i), P_i = \left[P_{i\frac{1}{2}}, A_C P_{i\frac{1}{2}} \right] \tag{3}$$

其中$$P_{i\frac{1}{2}} \in R^{3 \times N/2}$$表示的是一半的category-specific keypoints。$$P_{i\frac{1}{2}}$$被$$A_C \in R^{3 \times 3}$$反射一次，再和$$P_{i\frac{1}{2}}$$连起来，获得最终全部的category-specific keypoints。$$B_{C\frac{1}{2}} \in R^{3N/2 \times K}$$来表示对于一半的keypoints的shape basis，注意$$K$$并没有变化，说明shape basis并没有变少，只是我们只表示一半的keypoints。reflection operator $$A_C$$通过一个过原点的unit normal vector $$n_C \in R^3$$来定义。我们从公式2到公式3的变化，既减小了计算量，又将对称性加入了keypoints之中。

*Symmetric deformation space* 在很多non-rigid objects里，shape instances并不是symmetric的。但是再deformation space里可能会存在symmetry，比如说，human body。假设在$$\mathcal{C}$$里的某一个shape instance $$S_k$$有着关于$$n_C$$的reflective symmetry，这样我们就可以把其分为两部分：$$S_{k\frac{1}{2}}$$和$$S_{k\frac{1}{2}}^{'}$$。而且我们可以认为对于这个类别的所有的shape instances，这个reflective symmetry都是存在的。

**Definition 1(Symmetric deformation space)** 在$$\mathcal{C}$$里的某一个shape instance $$S_k$$有着关于$$n_C$$的reflective symmetry，这样我们就可以把其分为两部分：$$S_{k\frac{1}{2}}$$和$$S_{k\frac{1}{2}}^{'}$$。如果对于任意的一半的shape deformation instance $$S_{i\frac{1}{2}}$$，其都存在一个shape instance $$S_j \in \mathcal{C}$$，使得$$S_{j\frac{1}{2}}^{'}$$和$$S_{i\frac{1}{2}}$$对称，那么称$$\mathcal{C}$$是一个symmetric deformation space。

上述定义对于keypoints shape space $$P$$来说一样成立。instance-wise symmetric space是上述定义的一个特例。但是公式3并不能描述symmetric deformation space里的keypoints instances。我们通过引入可以被非对称的加权的symmetric basis来对这种keypoints进行建模，从而：

$$P_i = R_i \left[mat(B_{C\frac{1}{2}} c_i), mat(B_{C\frac{1}{2}}^{'} c_i^{'}) \right] \tag{4}$$

其中$$B_{C\frac{1}{2}}^{'}$$是将$$B_{C\frac{1}{2}}$$利用$$A_C$$反转而得，而$$c_i^{'}$$构成第二部分basis的权重。尽管公式4增加了计算量，但是其增加了模型能够表示symmetry deformation space的能力。从而我们有了如下的proposition：

**Proposition 1** 如果$$B_{C\frac{1}{2}}^{'}$$和$$B_{C\frac{1}{2}}$$关于某个平面对称，如果系数$$c_i$$和$$c_i^{'}$$满足同一个概率分布，那么上述最新的公式就表示了一个symmetric deformation space。

有了proposition 1的结论，我们就可以利用公式4来表示non-rigid symmetric objects，并且只要我们满足$$c$$和$$c^{'}$$的分布相同这样一个条件，我们仍然可以保持symmetry的性质。


**4. Learning Category-Specific Keypoints**

在这一节里，我们通过对$$\Phi_C$$进行建模来描述使用非监督方法学习category-specific keypoints的过程。更加准确地说，我们希望将函数$$\Pi_C: S_i \longrightarrow P_i$$作为一个参数为$$\theta$$的neural network，使用从$$\Phi_C$$里获得的supervisory signal来学习。关于从point sets上学习keypoints，[USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)训练了一个Siamese network来对rigid objects的keypoints进行预测，但keypoints的顺序是不知道的，这个方法对于rotation是稳定的。我们部分的网络结构是受这篇文章的启发的，但它们的源头都是基于[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation
](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf)。但是我们使用的是单个输入，从而避免了Siamese network很复杂的训练过程。fig 1展示了网络结构，它的输入是一个shape $$S_i$$，且在$$SO_2$$里是misaligned的。

![ModelssStructure]({{ '/assets/images/SYMMETRY-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 1. Network Architecture。pose and coefficient branch和additional learnable parameters生成了category-specific keypoints。node branch预测那些领导训练过程的nodes。第3节是建模，第4节是训练。

>$$SO_2$$表示的是det=1的orthogonal空间，也就是rotation matrix组成的空间，2表示是2维的，也就是所有的2维旋转操作组成的group称为$$SO_2$$，其可以被很简单的表示为$$SO_2 = \left[ \left[cos \theta, -sin \theta \right]. \left[sin \theta, cos \theta \rifht] \right]$$，其中$$\theta \in R$$。

这种设置是很合理的，因为point clouds一半都是沿着竖直方向对齐。我们下面描述一下网络的各个不同的组成部分。

*Node branch* 这个branch预测了一个稀疏的nodes集合，它们是潜在的category-specific keypoints，但是并没有被排序。我们将其表示为$$X_i = {x_{i1}, x_{i2}, \cdots, x_{iN}}$$，其中$$x_ij \in R^3, j \in {1,2, \cdots, N}$$。一开始，作者利用Farthest Point Sampling (FPS)算法来从输入的point clouds里采样$$N$$个node，再使用在[So-net: Self-organizing network for point cloud analysis](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf)和[USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)提到的point-to-node groupoing来对每个node选取一个包含了若干points的local neighborhood，这样就创建了$$N$$个clusters。point cloud $$S_i$$里的每个点都和上述这些nodes里的某一个建立关联。这个branch有着两个像PointNet的networks，然后再接了一个KNN groupoing层，再之后再用MLP来输出nodes。

*Pose and coefficients branch* 我们利用这个branch来预测$$R_i$$和$$c_i$$。对于$$R_i$$，我们可以使用一个旋转角来表示它。这个branch包含一个MLP用来预测这些参数。这个branch的输出size对于公式3和公式4描述的两种情况是不同的，后者的输出size翻倍。


*Additional learnable parameters* 公式3或者公式4里未知的其它的量对于一个object类别$$\mathcal{C}$$来说是常量。这些量不需要对于每个instance都进行预测。我们选择将其作为网络参数来优化。这些量是shape basis $$B_{\mathcal{C}} \in R^{3N \times K}$$和对称平面的法向量$$n_{\mathcal{C}} \in R^{3}$$。我们发现shape basis的$$K$$的选取最好是在5到10之间。实际上所生成的keypoints对于$$K$$值的选取并不敏感，比较大的$$K$$就会导致比较稀疏的shape coefficient $$c_i$$。我们也可以用其它的量来代替对称平面的法向量，比如说Euler角。

在inference的时候，我们使用Non-Maximal Suppression来获得最终的$$N^{'}$$个keypoints。我们对于某个类别的object的不同instances都输出同样数量的keypoints，因为它们具有同一个geometric model。


**Training Losses**

为了符合我们在section1里所定义的category-specific keypoints所需要满足的要求，我们按照如下方式来定义loss functions。

*Chamfer loss with symmetry and non-rigidity* 公式1表明这个网络$$\Pi_{\mathcal{C}}$$可以被最小化node predictions $$X_i$$和deformation function $$P_i = \Phi_C(R_i, c_i; B_{\mathcal{C}},n_{\mathcal{C}})$$之间的$$l_2$$loss来实现。但是正如[USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)所说，因为网络不具备预测keypoints顺序的能力，所以$$l_2$$ loss并不会收敛。但Chamfer loss可以收敛，这个loss会先对于$$X_i$$里的每个$$x_{ij}$$找到其在$$P_i$$里最近的那个点$$p_{ik}$$，然后再最小化这个距离，反之亦然。

$$\mathcal L_{chf} = \Sigma_{k=1}^N \mathop min_{p_{i,j} \in P_i} \left | \left | x_{ik} - p_{i,j} \right | \right | ^2_2 + \Sigma_{j=1}^N \mathop min_{x_{i,k} \in X_i} \left | \left | x_{ik} - p_{i,j} \right | \right | ^2_2 \tag{5}$$

公式5里的Chamfer loss保证所学习到的keypoints满足category-specific property里的generalisability——因为它们是对于这个category定义的共用的shape basis的线性组合。为了对对称性建模，公式3和公式4可以用于计算公式5里的$$P_i$$。


*Coverage and inclusively loss* 公式5表示的Chamfer loss并不能保证keypoints符合object shape。但是我们可以加入如下限定：a) coverage loss：keypoints能够覆盖整个category shape。b) inclusivity loss：keypoints和point cloud离得不远。coverage loss可以通过计算nodes $$X_i$$和point cloud $$S_i$$的volume之间的Huber loss来获得，需要使用到singular values。但我们这里就直接用3D bounding box来算了，因为简单。

$$\mathcal L_{cov} = \left | \left | vol(X_i) - vol(S_i) \right| \right | \tag{6}$$

inclusivity loss用只有一侧的Chamfer loss就可以表示：

$$\mathcal L_{inc} = \Sigma_{k=1}^N \mathop min_{s_{i,j} \in S_i} \left | \left | x_{ik} - s_{ij} \right | \right | ^2_2 \tag{7}$$



**5. Experiments**

**6. Conclusion**

这篇文章探究了如何在misaligned 3D point clouds上自动检测到那些能保证对于不同物体之间的shape variation以及同一个物体的deformation都consistent的keypoints的方法。我们发现这个问题可以用一个非监督学习的方式通过用对称的线性basis shapes来表示keypoints这个方式被解决。而且，这些被学习到的category-specific keypoints在不同的输入之间具有1对1的对应关系，并且是semantic consistent的。基于所学习到的keypoints的应用包括registration，generation，shape completion等。我们的实验表明我们的方法可以获得很高质量的keypoints，并且对于更复杂的deformation我们的方法也有潜力。未来的工作方向可以是通过非线性的方式来对更复杂的deformation进行建模（本文里的shape basis是线性组合的）。



### [USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html)

[Page](https://github.com/lijx10/USIP)

*Jiaxin Li, Gim Hee Lee*

*ICCV 2019*

**Abstract**

在这篇文章里，我们提出了USIP detector：一个非监督的稳定的关键点检测器，其可以不需要任何标注从经过了任何transformation的3D point cloud里学习到高度repeatable和准确定位的keypoints。我们的USIP detector由一个能够从输入的3D point clouds以及它们经过随机transformations之后的point clouds上学到稳定的keypoints的feature proposal network所组成。我们还进行了degeneracy分析并且提出了防止问题出现的办法。我们通过使用一个probabilistic chamfer loss来最小化training point cloud pairs上所检测到的keypoints之间的距离来学习到高度repeatable和定位准确的keypoints。在从Lidar，RGB-D以及CAD模型里获得的人造的或者真实的3D point clounds数据集上的大量实验证明了我们的USIP detector很大程度的超过了现有的hand-crafted以及deep learning-based 3D keypoint detectors。


**1. Introduction**

3D interest point或者keypoint detection是考虑如何在经过任意的$$SE(3)$$ transformation之后的3D point clouds上找到稳定的并且repeatable的点。这些所检测到的keypoints在很多CV和robotics任务里扮演着重要的角色，在这些任务里3D point clouds被广泛用于表示3D空间里的objects和scenes的数据结构。这些任务包括对于3D object modeling的geometric registration，point cloud based SLAM，3D object或者place recognition。在这些任务里，keypoints被用作计算rigid transformations的对应关系，或者被用作能获取有效features的位置。

尽管对于2D images来说已经有了很多成功的hand-crafted detectors，但对于3D point clouds来说成功的hand-crafted detectors就几乎没有。这个差异很大程度上是因为对于2D图片来说，我们有具有丰富信息的RGB channels，而对于3D point clouds来说只有点的位置的信息，这对于设计能提取有效信息的hand-crafted keypoints的算法来说是很难的。而且3D point clouds如果在经过任意的transformations之后（也就是说在不同的coordinate frame里），这个难度就更大了。

基于deep learning的3D keypoint detectors很少（实际上现在只有一篇[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)），但是3D keypoint descriptors的文章却越来越多（[PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors]([https://arxiv.org/pdf/1808.10322.pdf](https://openaccess.thecvf.com/content_ECCV_2018/papers/Tolga_Birdal_PPF-FoldNet_Unsupervised_Learning_ECCV_2018_paper.pdf))，[PPFNet: Global Context Aware Local Features for Robust 3D Point Matching](https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf)，[Learning Compact Geometric Features](https://openaccess.thecvf.com/content_ICCV_2017/papers/Khoury_Learning_Compact_Geometric_ICCV_2017_paper.pdf)，[3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.pdf)）（detector仅仅是为了找到keypoint的coordinate，其不关心也不知道keypoints的features或者它和其他数据之间的关系，而keypoint descriptors侧重于获取keypoints的features）。这是因为对于3D point clouds来说，并没有标注好了的keypoints coordinates数据集。和利用很容易获取到的registered重合的3D point clouds作为监督信号的3D descriptors不同，我们无法为3D point clouds标注ground truth keypoints。

我们提出USIP detector：一个基于deep learning的非监督的稳定的keypoint detector，其可以在不需要任何监督数据情况下对于做了任意transformation的3D point clouds检测到高度repeatable和精确定位的keypoints。为了达到这个目的，我们提出了一个Feature Proposal Network（FPN）用来从一个输入的3D point cloud上输出一个集合的keypoints以及它们每个点的不确定性。我们的FPN使用的是估计position的方法来改进了keypoint localization，因为其它的方法（[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)）使用的是在point cloud里选取keypoints，所以会造成量化误差。在训练过程中，我们使用随机生成的$$SE(3)$$ transformation来处理每个point clouds，从而得到point clouds pairs，用作FPN的输入。进一步的，我们指出并且避免出现我们的USIP可能不会成功的问题。我们利用probabilistic chamfer loss来最小化训练数据point cloud pairs的keypoints之间的差别，从而找到高度repeatable以及精确定位的keypoints。另外，我们还引入了point-to-point loss来迫使keypoints足够靠近point cloud。我们在多个生成的或者真实的3D point cloud数据集（包括Lidar，RGB-D和CAD）上做了大量实验来验证我们的USIP detector。某些定性的结果如fig 1所示。我们的主要贡献有：

![Mfdaf]({{ '/assets/images/USIP-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*fig 1. 我们的USIP detector在四个数据集上找到的keypoints的例子：(a) ModelNet40，object model (b) Oxford RobotCar, outdoor SICK LiDAR (c) KITTI (在Oxford上训练的）,outdoor Velodyne LiDAR。

* 我们的USIP detector完全是非监督的，因此避免了获取那些本来就不可能能有的监督信息（3D point cloud上的keypoints的位置）
* 我们分析了USIP detector可能会不管用的情况，并且提出了避免这些问题出现的解决办法
* 我们的FPN通过估计keypoint的位置而不是从point cloud里选择现有的point改进了keypoint localization
* 我们提出probabilistic chamfer loss和point-to-point loss来找到高度repeatable和准确定位的keypoints
* 在训练时候我们使用了随机生成的transformation作用在point clouds上，这使得我们的网络对于rotation来说有很好的效果


**2. Related Work

和最近很成功的基于deep learning的3D keypoint descriptors不同，大多数现有的3D keypoint detectors仍然还是hand-crafted的。Local Surface Patches（LSP）和Shape Index（SI）基于一个point的最大和最小principal曲率，如果一个point在预先定义的某个领域内是全局极值点，那么就认为这个点是一个keypoint。Intrinsic Shape Signatures（ISS）和KeyPoint Quality（KPG）选取那些沿着每个主轴都有很大的变化的那些点作为keypoints。MeshDoG和Salient Points（SP）利用类似于SIFT的Difference of Gaussian operator构建了一个曲率的scale space，有着局部最大值的那些点被选为keypoints。Laplace-Beltrami Scale-space（LBSS）通过对每个点使用一个Laplace-Beltrami operator来选取keypoints。

更近的工作，LORAX提出将point set投射到一个depth map上，再利用PCA来选择那些具有普遍geometric characteristics的keypoints。所有的hand-crafted方法都是靠着point clouds的局部的几何特征来选择keypoints的。因此，这些detectors的表现会因为扰动，比如说noise，density variations或者transformations，而效果变差。据我们所知，目前仅有的基于deep learning的3D keypoint detector就是weakly supervised 3DFeatNet（[3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf)），其利用GPS/INS标注的point clouds来训练。然而，3DFeat-Net的训练很大程度上聚焦于利用Siamese结构来学习具有区分性的descriptors。其并没有确保keypoint detection具有良好的效果。相比较而言，我们的USIP被设计为可以得到高度repeatable和定位准确的keypoints。更进一步的是，我们的方法是完全非监督的，并不依赖于任何有标注的数据集。

**3. Our USIP Detector**

Fig 2的(a)解释了训练我们的USIP detector的pipeline。我们将一个point cloud记为$$X = \left[ X_0, \cdots, X_N \right] \in R^{3 \times N}$$。一个集合的transformation matrices $$\brace T_1, \cdots, T_L \rbrace$$，其中$$T_l \in SE(3)$$是随机生成的，其应用到point cloud $$X$$上就得到了$$L$$对训练inputs，记为$$\lbrace \lbrace X, \tilde{X_1} \rbrace, \cdots, \lbrace X, \tilde{X_L} \rbrace \rbrace，其中$$\tilde{X_l} = T_l X \in R^{3 \times N}$$。我们丢掉$$l$$下标，将训练input pair和它们对应的transformation matrix构成的三元组记为$$\lbrace X, \tilde{X}, T \rbrace$$。在训练期间，$$X$$和$$\tilde{X}$$分别喂给FPN，都会输出$$M$$个proposal keypoints和它们对应的不确定性，分别记为$$\lbrace **Q** = \left[ Q_1, \cdots, Q_M \right], \Sigma = \left[ \sigma_1, \cdots, \sigma_M \right]^T \rbrace$$，和$$\lbrace \tilde{**Q**} = \left[ \tilde{Q_1}, \cdots, \tilde{Q_M} \right], \tilde{\Sigma} = \left[ \tilde{\sigma_1}, \cdots, \tilde{\sigma_M} \right]^T \rbrace$$。其中$$Q_m \in R^{3}$$，$$\tilde{Q_m} \in R^3$$，$$\sigma_m \in R^{+}$$，$$\tilde{\sigma_m} \in R^{+}$$。为了提升keypoint localization的精度，我们不需要$$Q_m \in **Q**$$是$$X$$里的任何点。对于$$\tilde{Q_m}$$也是一样。

对于$$\tilde{**Q**}$$，我们再计算$$**Q**^{'} = T^{-1}\tilde{**Q**} \in R^{3 \times M}$$，而$$**Q**^{'}$$应该和$$**Q**$$很像。这里我们做一个假设，也就是每个keypoint的不确定性，在经过$$T^{-1}$$操作后不会改变，也就是说$$\Sigma^{'} = \tilde{\Sigma}$$。从经历过任意transformation的3D point clouds里检测具有高度repeatble特性以及定位准确的keypoints的任务就可以通过最小化$$**Q**$$和$$**Q**^{'}$$来实现。为了实现这个目标，我们提出loss function：$$\mathcal{L} = \mathcal{L_c} + \lambda \mathcal{L_p}$$，其中$$\mathcal{L_c}$$是probabilistic chamfer loss，用于最小化$$**Q**$$和$$**Q**^{'}$$里的对应的keypoints的probabilistic距离。$$\mathcal{L_p}$$是point-to-point loss，用来最小化所得到的keypoints和距离它最近的point clouds里的点的距离。$$\lambda$$是个超参数。

**Probabilistic Chamfer Loss**

一个最小化$$**Q**$$和$$**Q**^{'}$$之间距离的方法就是使用Chamfer loss：

$$ \Sigma_{i=1}^M \mathop min_{Q_j^{'} \in **Q**^{'} \left | \left | Q_i - Q_j^{'} \right | \right | + \Sigma_{j=1}^M \mathop min_{Q_i \in **Q**} \left | \left | Q_i - Q_j^' \right | \right |  \tag{1}$$

公式1最小化一个point cloud里的点和其在另一个point cloud里最近的那个点之间的距离。然而，$$M$$个proposals并不是等重要的。如果$$**Q**$$里面的点$$Q_i$$的位置并不好，如果还按照上述的方式来计算，那么其也会导致$$Q_i^{'}$$的结果也不好。

为了解决上述这个问题，我们设计我们的FPN同时也学习每个keypoint proposal的不确定性$$\Sigma$$和$$\Sigma^{'}$$，再计算一个probabilistic chamfer loss $$\mathcal{L_c}$$。对于$$Q_i$$和$$Q_j^{'}$$，$$i=1,\cdots,M$$，其上面定义的概率分布为：

$$p(d_{ij} | \sigma_{ij}) = \frac{1}{\sigma_{ij}} exp(-\frac{d_{ij}}{\sigma_{ij}}) \tag{2}$$

其中$$\sigma_ij = \frac{\sigma_i + \sigma_j^{'}}{2}$$，$$d_ij = \mathop \min_{Q_j^{'} \in **Q**^{'}} || Q_i - Q_j^{'}|| \geq 0$$。

$$p(d_{ij} | \sigma_{ij})$$是一个合规的概率分布。$$d_ij$$越小，那么proposal keypoints $$Q_i$$和$$Q_j^{'}$$是高度repeatable以及定位准确的keypoints的概率就越高。

假设对于所有的$$d_{ij} \in D_{ij}$$，$$**Q**$$和$$**Q**^{"}$$之间的联合分布是：

$$p(D_{ij} | \Sigma_{ij}) = \Pi_{i=1}^M p(d_{ij} | \sigma_{ij}) \tag{3}$$

因为最近邻的选择不同，所以说$$d_{ij} \neq d_{ji}$$，且$$\sigma_{ij} \neq \sigma_{ji}$$。

$$**Q**^{"}$$和$$**Q**$$之间的联合分布是：

$$p(D_{ji} | \Sigma_{ji}) = \Pi_{j=1}^M p(d_{ji} | \sigma_{ji}) \tag{4}$$

其中$$\sigma_ji = \frac{\sigma_i + \sigma_j^{'}}{2}$$，$$d_ji = \mathop \min_{Q_i \in **Q**} || Q_i - Q_j^{'}|| \geq 0$$。

最后我们的概率chamfer loss就定义为：

$$\mathcal{L_c} = \Sigma_{i=1}^M -lnp(d_{ij} | \sigma_{ij}) + \Sigma_{j=1}^M -lnp(d_{ji} | \sigma_{ji})$$

$$ = \Sigma_{i=1}^M (ln \sigma_{ij} + \frac{d_ij}{\sigma_{ij}}) + \Sigma_{j=1}^M (ln\sigma_{ji} + \frac{d_{ji}}{\sigma_{ji}}) \tag{5}$$

我们通过计算公式2关于$$\sigma_{ij}$$的导数来分析其的物理含义：

$$\frac{\partial p(d_{ij} | \sigma_{ij})}{\partial \sigma_{ij}} = 0$$

可得$$\sigma_{ij} = d_{ij}$$。

这说明，给定一个$$d_{ij} >0$$，$$\sigma_{ij} = d_{ij}$$的时候，上述概率取到最大值。假设我们有三个proposal keypoints，$$(Q_i,Q_j^{'},Q_k^{'})$$，其中$$d_{ij}$$和$$d_{ki}$$是两对keypoints pairs的最近邻距离。当$$d_{ij} \longrightarrow 0$$而且$$d_{kj}$$很大的时候，我们需要$$\sigma_k^{'}$$的值很大。这也就是说，$$\lbrace Q_i, Q_j^{'} \rbrace$$是高度repeatable且精确定位的keypoints，而$$Q_k^{'}$$不是。因此，$$\sigma_k^{'}^$$比较大，说明我们的概率chamfer loss是定义正确的。


**Point-to-point loss**

为了减小keypoints localization的错误，我们不需要keypoints是point cloud里的任何一个点。但是这可能会让keypoints离point cloud太远。我们通过引入一个loss function $$\mathcal{L_p}$$来解决这个问题。

$$\mathcal{L_{point}} = \Sigma_{i=1}^M \mathop min_{X_j \in X} \left | \left | Q_i - X_j \right | \right | + \Sigma_{i=1}^{M} \mathop min_{\tilde{X_j} \in \tilde{X}} 
\left | \left | \tilde{Q_i } - \tilde{X_j} \right | \right | $$

其中$$X_j \in X$$是$$Q_i$$在point clouds里最近的点。


**Feature Proposal Network**

FPN的结构如fig 2 (b)所示。我们先从给定的输入point cloud $$X \in R^{3 \time N}$$使用Farthest Point Sampling采样$$M$$个nodes，记为$$S = \left[ S_1, \cdots, S_M \right] \in R^{3 \times M}$$。之后才采用[So-net: Self-organizing network for point cloud analysis](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf)里的point-to-node grouping的方法（也就是每个points，将其归属于距离它最近的那个nodes）来为每个$$S_m \in S$$构造一个points构成的neighborhood，从而我们获得了$$\lbrace \lbrace X_1^1 |S_1, \cdots, X_1^{K_1} |S_1 \rbrace, \cdots, \lbrace X_M^1 |S_M, \cdots, X_M^{K_M} |S_M \rbrace \rbrace$$，其中$$K_1, \cdots, K_M$$表示每个nodes的neighborhood points的数量。

>Farthest Point Sampling的解释可以看[这篇博客](https://jskhu.github.io/fps/3d/object/detection/2020/09/20/farthest-point-sampling.html)

point-to-node方法比node-to-point KNN或者radius-based ball-search要好在以下两个方面：(1) $$X$$里的每个点都唯一的归于了某个node的neighborhood，而另外两个方法可能会有些点没有归属；(2) point-to-node grouping方法对于不同的scale以及point density都可以适应，而KNN search受到density变化以及ball-search受到scale变化的影响很大。为了使得FPN是translation equivariant的，我们将每个node构成的neighborhood都进行归一化，记归一化之后的结果为$$\lbrace \hat X_m^1 | S_m, \cdots, \hat X_m^{K_m} | S_m \rbrace$$。其中$$\hat X_m^{k}  = X_m^{k} - S_m$$，$$1 leq k \leq K_m$$。将经过上述操作之后的point cloud $$X \in R^{3 \times N}$$（也就是将$$X$$按照选择出来的$$M$$个nodes分为$$M$$个clusters，然后再将每个node对应的cluster除了node以外的点都减去node的坐标实现归一化）被喂给如fig 2 (b)里所示的一个类似PointNet（[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf)）的网络结构，这样$$X$$里的每个点都有一个feature，再将每个cluster里的points的features综合起来获取一个和$$S_m$$相关的局部feature向量$$G_m$$。再之后，对于每一个$$(G_m |S_m)$$，我们使用KNN来找到$$G_m$$的K个最近的向量（是在上述nodes的feature空间里找，而不是在所有的$$X$$里的points经过了之前的PointNet之后所得到的feature空间里找，也就是在$$\lbrace G_1 | S_1, \cdots, G_M |S_M \rbrace$$里利用KNN来搜索）。这也就是，利用KNN grouping layer应用在在局部feature向量集合里$$\lbrace G_1 | S_1, \cdots, G_M |S_M \rbrace$$来获取层次化的信息综合。对于每个$$(G_m|S_m)$$来说，将其找到的K个最近的node feature向量记为$$\lbrace





### [Unsupervised Learning of Visual 3D Keypoints for Control](http://proceedings.mlr.press/v139/chen21b/chen21b.pdf)
[Code](https://github.com/buoyancy99/unsup-3d-keypoints) [Post](https://buoyancy99.github.io/unsup-3d-keypoints/)

*Boyuan Chen, Pieter Abbeel, Deepak Pathak*

*ICML 2021*

This paper proposed a method to directly learn 3D keypoint coordinates with only 2D images of the same scene from different view points as input in an unsupervised learning manner. The camera parameters are known. 

The whole process is as below:

Important hyperparameter: Keypoint number $$K$$.

**Step1** For each input 2D image $$I_n$$, use an encoder $$\Phi_n$$ to learn $$k$$ heatmaps $$C_n^k \in R^{S\times S}$$ and depth maps $$D_n^k \in R^{S\times S}$$. For each $$C_n^k$$, they used softmax to turn it into a probabilistic heatmap $$H_n^k$$ to represent the probability that keypoint $$k$$ will be at each position. Each depth map $$D_n^k$$ is a dense prediction of distance from camera plane for 3D keypoint k being at each position, i.e., each element in $$D_n^k$$ is the distance of keypoint $$k$$ to the camera plane.
After getting $$H_n^k$$ and $$D_n^k$$, they used the expectation across the whole heatmap to calculate the coordinate of keypoint $$k$$ in camera $$n$$ plane, $$\left[u_n^k, v_n^k, d_n^k\right]$$ as:

$$u_n^k = \frac{1}{S}\Sigma_{u,v}u H_n^k(u,v)$$, $$v_n^k = \frac{1}{S}\Sigma_{u,v}v H_n^k(u,v)$$, $$d_n^k = \Sigma_{u=1}^{S}\Sigma_{v=1}^{S}D_n^k(u,v) H_n^k(u,v)$$

Note that for each camera, we know the projection function to project the world coordinates into camera coordinates, denote it as $$\Omega_n$$, and the opposite projection as $$\Omega_n^{-1}$$. 

**Step2** Each camera will give a prediction of keypoint $$k$$ coordinates, the authors used some weighted techniques to get the final keypoint coodinates from these $$n$$ predictions, i.e., $$x^i = \frac{1}{n}\Sigma_{i=1}^n A_n^i x_n^i$$, $$y^i = \frac{1}{n}\Sigma_{i=1}^n A_n^i y_n^i$$, $$z^i = \frac{1}{n}\Sigma_{i=1}^n A_n^i z_n^i$$, where $$\left[x^i, y^i, z^i\right]$$ is the final coodinate of keypoint $$i$$ in the world coordination, and $$\left[x_n^i, y_n^i, z_n^i\right]$$ is calculated from $$\Omega_n^{-1}(\left[u_n^i, v_n^i, d_n^i\right])$$.
The calculation of weights $$A_n^k$$ is as:

$$A_n^k = \frac{exp{(\frac{1}{S^2}\Sigma_{p=1}\Sigma_{q=1}C_n^k(p,q))}}{\Sigma_{i=1}^K exp{(\frac{1}{S^2}\Sigma_{p=1}\Sigma_{q=1}C_n^i(p,q))}}$$

**Step3** This step is a reconstruction step. The authors believed that if the decoder can reconstruct the input image from the keypoints projected from the learned 3D keypoints $$\left[x^i, y^i, z^i\right]$$, the model learns the geometry of the scene. The input of the decoder is the scaled stacked Gaussian distribution matrix with $$u_n^i, v_n^i$$ as the center and $$I_2/d_n^i$$ as the coviriance matrix. Note that each input image $$I_n$$ has a decoder $$\phi_n$$.

![Model Structure]({{ '/assets/images/Unsupervised_3D.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. Overview of our Keypoint3D algorithm. (a) For each camera view, a fully convolutional neural network encodes the input image into K heat maps and depth maps. (b) We then treat these heat maps as probabilities to compute expectation of spatial $$u,v$$ coordinates in camera plane. These expected values and the saptial variances are used to resample final $$u,v$$ keypoint coordinates which adds noise that prevents the decoder from cheating to hide the input information in the relative locations $$u,v$$ keypoints. We also take expectation of depth coordinate, $$d$$, using the same probability distribution. These $$\left[u; v; d\right]$$ coordinates are then unprojected into the world coordinate. (c) We take attention-weighted average of keypoint estimations from different camera views to get a single prediction in the world coordinate. (d) For decoding, we project predicted keypoints in world coordinate to $$\left[u; v; d\right]$$ in each camera plane. (e) Each keypoint coordinate is mapped to a gaussian map, where a 2D gussian is created with mean at $$\left[u, v\right]$$ and std inversely proportional to $$d$$. For each camera, gaussian maps are stacked together and passed into decoder to reconstruct observed pixels from the camera. (f) Together with reconstruction, we also jointly train a task MLP policy on top of predicted world coordinates via reinforcement learning.*


### [Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Weakly-Supervised_Discovery_of_Geometry-Aware_Representation_for_3D_Human_Pose_Estimation_CVPR_2019_paper.pdf)
*Xipeng Chen, Kwan-Yee Lin, Wentao Liu, Chen Qian, Liang Lin*

*CVPR 2019*

This work proposed a method to learn 3D coordinates of human body joints in order to do human pose estimation. This model is based on skeleton extracted from the raw RGB images, not an End-to-end framework.

Hyperparameter: Number of Keypoints $$K$$.

**Step1** Inputs are source image $$I_s$$ and target image $$I_t$$, and the rotation matrix are known due to the parameters of cameras. First, they use existing skeleton algorithm to extract skeleton maps of $$I_s$$ and $$I_t$$.

**Step2** Instead of a traditional encoder-decoder framework, they use a novel view synthesis method, i.e., source image $$I_s$$ are encoded and combined with rotation matrix $$R_{s \rightarrow t}$$, target image $$I_t$$ are reconstructed from the decoder. The 3D keypoint coordinates are the output of the encoder, as a geometry-aware representation, as explained in the paper. They also design the bidirectional encoder-decoder framework, which hinges on two encoder-decoder networks with same architecture to perform view synthesis in the two directions simultaneously, i.e., from $$I_s$$ to $$I_t$$ and from $$I_t$$ to $$I_s$$. These two reconstructions will involve two losses.

**Step3** They believe that the 3D keypoints of these two images should be the same. There are two encoders and the outputs should be the same, thus involve a new loss.

![Model Structure1]({{ '/assets/images/weak-supervised.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1 The framework of learning a geometry representation for 3D human pose in a weakly-supervised manner. There are three main components. (a)Image-skeleton mapping module is used to obtain 2D skeleton maps from raw images. (b)View synthesis module is in a position to learn the geometry representation in latent space by generating skeleton map under viewpoint $$j$$ from skeleton map under viewpoint $$i$$. (c) Since there is no explicit constrain to facilitate the representation to be semantic, a representation consistency constrain mechanism is proposed to further refine the representation.*



### [Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://keypointnet.github.io/keypointnet_neurips.pdf)
*Supasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, Mohammad Norouzi*

*NeurIPS 2018 Oral*

**Abstract**

这篇文章提出了KeypointNet，一个end-to-end的geometric reasoning框架，来学习一批最优的category-specific 3D keypoints。给定一张图片，KeypointNet为下游任务提取最优的3D keypoints。我们在3D pose estimation任务上应用我们的框架，我们提出一个能找到一个object的两个view之间的相对姿态的differentiable的目标函数。我们的模型能在一类有着不同视角和不同instances的object图片种学习到geometrically和semantically都consistent的keypoints。我们发现，我们所提出的这个框架并没有用到任何标注（监督信息），却在pose estimation任务上比使用同样框架的监督方法效果要好。


**1. Introduction**

CNN已经说明同时优化feature extraction和classification可以很大程度的改进object recognition任务的效果。而现有的解决geometric vision problems（比如说3D reconstruction，shape alignment等）的方法，包含了两个stage，也就是一个单独的keypoint detection模块，和后来的geometric reasoning操作。在这篇文章里，我们探索是否可以对于geometric vision problem提出一种end-to-end的框架，在其中keypoints作为下游任务的latent variables被优化出来。

考虑确定一张图片里一辆汽车的3D pose这样一个问题。一个标准的做法是先检测一个稀疏的category-specific keypoints的集合，之后再利用这些keypoints使用某种geometric reasoning框架（比如说，PnP algorithm）来重建3D pose或者相机角度。我们可以利用很强的监督信息，对于每个object category，利用很多张标有keypoints位置的该类object的图片，来学习一个针对该类object的keypoint detector。或者使用model-based fitting方法来学习这样的keypoint detector。研究者已经建立了标有keypoints的face，hands，human body的datasets。但是，对于某个object category的图片里的keypoints进行选取和标注的过程是十分复杂而且ill-defined的。为了设计合理的keypoint set，需要将这个keypoint set要用在什么下游任务这个信息考虑进来。直接利用下游任务来优化keypoints的选择会自然的鼓励网络学到对于这个下游任务最有用的那些keypoints。

这篇文章提出了KeypointNet，一个end-to-end的geometric reasoning框架，来对于一个特别的下游任务，学习一个最优的category-specific 3D keypoints。我们的方法相对于先前工作的创新点在于我们利用一个任意的下游任务来将keypoints作为latent variable学习出来。我们的框架对于任何目标函数对于keypoints positions是可微分的下游任务都是可以使用的。我们这篇文章使用的是3D pose estimation作为下游任务，我们的最关键的技术贡献在于：1）一个新的对于keypoint positions可微分的pose estimation目标函数；2）一个multi-view consistent的loss function。这个pose estimation任务的目标函数旨在为恢复同一个object的两个views之间的relative pose任务找到最优的keypoints集合。multi-view consistency loss估计在一个object的3D transformations之间找到consistent的keypoints。值得注意的是，我们是从一些2D图片里检测3D keypoints（2D points加上深度），而且我们的pose estimation和multi-view consistency都是针对3D keypoints detection设计的。

我们的KeypointNet对于一个给定的object类别可以在含有不同角度和不同instances的这个类别的object的一系列图片种学习到geometrically和semantically都consistent的keypoints。某些学习到的keypoints对应到很有意思而且semantically有意义的地方，比如说car的wheels，并且这些keypoints还可以不通过object geometry直接获取3D的信息。我们在ShapeNet dataset上对于不同的object类别做了三个实验。对于3D pose estimation，我们还和有标注的监督方法进行了对比。令人惊讶的是，我们的end-to-end方法在没有keypoints标注的情况下取得了更好的结果。


**2. Related Work**

2D和3D的keypoint detection任务都是CV领域的被研究了很久的问题，而keypoint inference在object localization任务上一直被用作一个早期的stage，比如说，使用CNN在monocular RGB图片上检测2D的human joint position的任务。因为在HCI，motion capture和security应用领域都有广泛的需求，有很多工作都在研究keypoint detection的任务。

和我们这篇论文更加有关系的工作，就是有一批利用CNN从monocular RGB图片种检测3D human keypoint的论文，它们使用了各种各样的CNN结构、监督信息、以及3D structural prior来学习一批已经定义好的3D joint locations。还有一些工作利用所学习到的2D keypoint detector结合3D priors来做2D-to-3D-lifting或者从depth图片里找到data-to-model的对应关系。[Honari等人的文章](https://openaccess.thecvf.com/content_cvpr_2018/papers/Honari_Improving_Landmark_Localization_CVPR_2018_paper.pdf)通过引入semi-supervised任务比如说attribute prediction或者equivalent landmark prediction来改进landmark prediction。相反的是，我们的keypoints set并不是被定义为了prior，而是作为下游的geometric estimation任务的latent variable被优化出来。

在CNN feature representation中设计latent structure在很多领域都被研究过了。比如说，capsule网络以及它的变种将隐藏层的输出的activation结果的大小和方向结合起来用作构建更高层的features。我们的KeypointNet的输出可以看作一个latent 3D feature，其因为multi-view consistency loss和后续下游任务的relative pose目标函数的原因而倾向于表示的是图片里object的3D keypoint locations。

最近的很多工作研究了对于同一个类别的object，不同的图片含有这个类别的object的形状和外表差异很大的不同的instances，他们研究了如何建立它们之间的2D correspondence matching。比如说，[Choy的文章](https://proceedings.neurips.cc/paper/2016/file/b495ce63ede0f4efc9eec62cb947c162-Paper.pdf)使用了一个基于appearance的contrastive loss来学习geometry和semantic similarity。[Han的文章](https://openaccess.thecvf.com/content_ICCV_2017/papers/Han_SCNet_Learning_Semantic_ICCV_2017_paper.pdf)提出了一个新的SCNet结构用来对于2D semantic correspondence学习一个geometrically plausible model。[wang的文章](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.pdf)通过解决一个feature selection和labeling的问题利用deep features在一系列图片上做一个multi-image matching的任务。[Thewlis的文章](https://arxiv.org/pdf/1706.02932.pdf)使用ground truth transforms（图片对之间的optical flow）和point-wise matching来学习一个dense object-centric坐标模型。

还有一些工作致力于通过不同的监督信息来学习latent 2D或者3D features。[Arie-Nachimson的文章](http://www.weizmann.ac.il/math/ronen/sites/math.ronen/files/uploads/arie-nachimson_basri_-_constructing_implicit_3d_shape_models_for_pose_estimation.pdf)为rigid objects建立了3D models之后利用这个model来从一张2D图片和一些3D的latent features里估计3D pose。受到cycle-consistency对于学习correspondence的作用的启发，[Zhou的文章](https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Learning_Dense_Correspondence_CVPR_2016_paper.pdf)训练了一个CNN利用CAD模型来对同一个类别的object的不同instance的图片之间预测correspondence。独立于我们的文章，[zhang的文章](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)利用一个reconstruction的目标函数来为已知类别的object的图片检测稀疏的2D landmarks。[Rhodin的文章](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Helge_Rhodin_Unsupervised_Geometry-Aware_Representation_ECCV_2018_paper.pdf)使用一个multi-view consistency loss，和我们的相似，来为human pose estimation任务学习3D latent variables。和这一段中提到的这些论文里的方法不同的是，我们的latent keypoints是为一个下游任务来优化的，这种方法会产生对于下游任务更有用的keypoints。通过在真实的物理3D结构中表示我们所学到的keypoints，我们发现我们的方法甚至可以发现图片中被遮住部分的correspondence，而且其还具有很大的姿态差异。

寻找3D correspondence的工作也在持续的开展。[Salti的论文](https://openaccess.thecvf.com/content_iccv_2015/papers/Salti_Learning_a_Descriptor-Specific_ICCV_2015_paper.pdf)将3D keypoint detection任务转换为一个对于points的binary classification任务，而这些points的ground truth similarity label是由一个预训练好的3D descriptor提供的。[Zhou的工作](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Xingyi_Zhou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.pdf)使用view-consistency作为监督指标来预测3D keypoints，但仅仅是在depth images上使用。相似的，[Su的文章](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Render_for_CNN_ICCV_2015_paper.pdf)使用生成的模型通过使用CNN viewpoint embedding来匹配object和真实的图片来估计object的viewpoint。


**3. End-to-end Optimization of 3D Keypoints**

给定一张含有一个已经知道类别的object的图片，我们的模型可以预测一个3D keypoint的有序的list，定义为pixel coordinates和相对应的depth values。这样的keypoints要求对于这个类别的object的不同instances以及不同角度的图片具有geometrically和semantically consistency，在fig 1能看到。

![RESULT]({{ '/assets/images/DOWNSTREAM-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. KeypointNet在ShapeNet测试集上对于cars，planes和chairs的结果。我们的网络能够泛化到没见过的appearance和shape变化上去，并且还能consistently的预测被遮住的部分比如说wheels和chair legs。*

我们的KeypointNet有N个head从而可以提取N个keypoints，而且它们是有顺序的，同一个head用来提取同一个semantic的keypoint。这些keypoints可以作为基于稀疏点的feature representations的构建模块，对于geometric reasoning和pose-aware，pose-invariant object recognition任务会有用。

和利用监督方法学习一个从图片到list的keypoint positions的映射的方法不同，我们并不将keypoint positions用作一个priori。取而代之的是，我们利用一个下游的任务来联合选取keypoints。我们在训练的时候使用的是relative pose estimation任务，这个任务是给定同一个object的两个views，而且直到这两个views之间的rigid transformation $$T$$，我们要在两个views里预测最优的两个3D keypoints lists，$$P_1$$和$$P_2$$，从而使得一个view能最佳的match另一个view。过程如fig 2所示。

![PIPELINE]({{ '/assets/images/DOWNSTREAM-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. 在训练过程中，同一个object的两个views都会给KeypointNet作为输入。两个views之间的rigid transformation $$(R,t)$$也作为监督信号被提供。我们学习一个3D keypoints的ordered list，在两个views里是consistent的而且能够让我们复原transformation。在inference的时候，KeypointNet从一张单独的输入图片提取3D keypoint positions。*

我们设计了一个目标函数$$O(P_1，P_2)$$，基于此我们可以优化一个从一张图片到一个3D keypoint的ordered list的parametric mapping（也就是我们的KeypointNet）。我们的目标函数有两个主要的组成部分：

* 一个multi-view consistency loss，在知道transformation的情况下用来衡量两个3D keypoint ordered lists之间的差异；
* 一个relative pose estimation loss，使得用orthogonal procrustes从$$P_1$$和$$P_2$$还原出的rotation $$R^{'}$$和真实的rotation $$R$$相同。

我们认为这两个terms使得模型可以检测重要的keypoints，其中某些是人类会为不同object类别选取的semantically meaningful的locations。注意到我们并没有直接去寻找那些semantically meaningful的keypoins，因为它们可能对于下游的任务来说并不是最好的，而且会很难去挑选。下面我们将会先解释我们的目标函数，之后介绍KeypointNet的结构。

**notation**

每个training tuple有一对图片$$(I,I^{'})$$，其是同一个object的两个不同角度的图片，并且有它们的relative rigid transformation $$T \in SE(3)$$，将潜在的3D shapes从$$I$$变换到了$$I^{'}$$。$$T$$有着以下的matrix form：

![MATRIX]({{ '/assets/images/DOWNSTREAM-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}

其中$$R$$和$$t$$分别代表一个3D rotation和一个3D translation。我们将通过优化一个目标函数$$O(f_{\theta}(I), f_{\theta}(I^{'}))$$，来学习一个函数$$f_{\theta}(I)$$，参数是$$\theta$$，将一个2D的图片$$I$$映射到一个3D keypoint locations的list，$$P=(p_1,p_2,...,p_N)$$，其中$$p_i=(u_i,v_i,z_i)$$。


**3.1 Multi-view consistency**

我们的multi-view consistency loss的目的就是确保keypoints能在不同的views里跟踪consistent的部分。两个views里的3D keypoints应该分别在3D空间里对应到object的同一个3D像素点。我们假设已知相机的global focal length $$f$$。下面我们使用$$\left[x,y,z$$\right]代表3D坐标，使用$$\left[u,v\right]$$代表pixel坐标。照片$$I$$的keypoint $$\left[u,v,z\right]$$映射到照片$$I^{'}$$的计算由下面的projection operators给出（从$$I^{'}$$映射到$$I$$也给出）：

$$\left[\hat{u}, \hat{v}, \hat{z}, 1\right]^T  \sim \piT\pi^{-1}(\left[u,v,z,1\right]^T)$$

$$\left[\hat{u}^{'}, \hat{v}^{'}, \hat{z}^{'}, 1\right]^T  \sim \piT^{-1}\pi^{-1}(\left[u^{'},v^{'},z^{'},1\rifht]^T)$$

其中，$$\hat{u}$$表示$$u$$映射到第二张图片里的坐标，$$\hat{u}^{'}$$表示$$u^{'}$$映射到第一张图片里的坐标。$$\pi:R^4 \rightarrow R^4$$表示将一个homogeneous的在camera coordinates下的3D coordinate $$\left[x,y,z,1\right]^T$$映射到一个pixel position再加上depth的perspective projection操作：

$$\pi(\left[x,y,z,1\right]^T) = \left[\frac{fx}{z}, \frac{fy}{z}, z, 1\right] = \left[u,v,z,1\right]^T$$

我们定义一个symmetric multi-view consistency loss如下：

$$L_{con} = \frac{1}{2N}\Sigma_{i=1}^N ||\left[u_i, v_i, u_i^', v_i^'\right]^T - \left[\hat{u_i}^{'}}, \hat{v_i^{'}}, \hat{u_i}, \hat{v_i}\right]^T||^2$$

我们仅仅在可见到的图片空间（也就是2D图片）上衡量error，所以上述$$L_{con}$$不包含深度信息$$z$$，因为深度信息从来就不能直接被观察到，而且和$$u$$，$$v$$具有不同的取值范围。但是我们要注意，在我们的计算中，只有有了$$z$$我们才能够计算两个views之间的projection $$\pi$$。

multi-view consistency对于在不同的views间获取一个consistent的2D keypoint positions集合已经是足够了。但是只有这个loss会导致一个崩坏的结果：所有的keypoints都会集中到同一个位置，这是没有作用的。我们可以定义某种diversity来分开这些keypoints，但是对于multi-view consistency，仍然存在无数种解满足这个条件。所以说我们采取了利用某个下游的任务来选取keypoints的方式。我们使用pose estimation作为我们的任务。

**3.2 Relative pose estimation**

keypoint detection的一个重要的应用就是还原一对图片里的object之间的relative transformation。从而，我们定义一个可微分的目标函数用来衡量预测的relative rotation $$\hat{R}$$（用两个keypoints集合利用Procrustes'alignment计算得来）和ground truth rotation $$R$$之间的差异。因为我们的KeypointNet具有translation equivariance性质，并且我们已经有了上述的multi-view consistency loss，我们在这个loss里就不衡量translation loss了。从而这个pose estimation目标函数定义为：

$$ L_{pose} = 2arcsin(\frac{1}{2\sqrt{2}}||\hat{R}-R||)$$

这衡量了从两个keypoints集合里计算出的least-squares estimate $$\hat{R}$$和ground truth relative rotation matrix $$R$$之间的角度差异。这个目标函数可以用可微分的运算来实现。

为了估计$$\hat{R}$$，$$X$$和$$X^{'} \in R^{3 \times N}$$表示两个views的camera coordinates的3D keypoint locations.也就是说，假设$$X = \[X_1,X_2,...,X_N\]$$以及$$X_i = (\pi^{-1}pi)\[:3\]$$。$$X^{'}$$也有类似的定义。$$\tilde{X}$$和$$\tilde{X}^{'}$$表示$$X$$和$$X^{'}$$分别将自身的mean减掉之后的矩阵。从而optimal least-squares rotation $$\hat{R}$$就用以下的方式计算而得：

$$\hat{R} = Vdiag(1,1,...,det(VU^T))U^T$$

其中$$U,\Sigma, V^T = SVD(\tilde{X}\tilde{X}^{'}^T)$$。这个计算rotation $$\hat{R}$$的方式叫做orthogonal Procrustes problem。为了确保$$\tilde{X}\tilde{X}^{'}^T$$是invertible的，并且为了增强keypoints的鲁棒性，我们在$$X$$和$$X^{'}$$内加入Gaussian噪音。

从实验结果来看，pose estimation目标函数很大程度的帮助网络产生合理的keypoints，导致网络可以自动的找到一些有意思的地方比如说wheels，飞机的wings，chair的legs等。我们相信这是因为这些部分在一个object类型里是geometrically consistent的（比如说所有的car都有圆形的wheels）。这些keypoints还在空间中分散开。这些对于下游任务都是有好处的。


**4. KeypointNet Architecture**

从图片到keypoints的映射的一个重要的性质就是在pixel层面上的translation equivariance。也就是，如果我们将整个input图片，比如说，向左移动一个像素，这个网络输出的所有的keypoint locations也都会向左移动一个像素。训练一个没有这个性质的标准的CNN需要有一个很大的training set，其中包含在各个地方的objects，而且在inference的时候还不能确保equivariance（因为inference时候的object可能会出现在training图片从没出现过的位置上）。

我们使用下面很简单的方式来获得这个equivariance。我们并不是让网络直接输出keypoints的locations，而是让网络输出一个probability distribution map，$$g_i(u,v)$$，表示的是keypoint i在pixel位置$$(u,v)$$地方可能出现的概率，而$$\Sigma_{u,v}g_i(u,v)=1$$对于所有的$$i$$都成立。我们使用一个spatial softmax来对整个image pixels生成这样的distribution。我们之后再计算这个spatial distribution的期望来获得一个pixel coordinate：

$$\left[u_i, v_i\right]^T = \Sigma_{u,v} \left[ug_i(u,v), vg_i(u,v)\right]^T$$

对于$$z$$ coordinates来说，我们同样再每个pixel的位置预测一个深度值，叫做$$d_i(u,v)$$，然后：

$$z_i = \Sigma_{u,v} d_i(u,v) g_i(u,v)$$

为了我们输出的feature map和输入的图片尺寸相同（这样我们才能用上述的计算方式来表示原始输入图片里的keypoint的位置），我们使用的是stride=1的fully convolutional architecture（从[这篇文章](https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)用来的），同样也用于semantic segmentation。为了增加网络的感受野，我们将几层dilated convolutions堆叠起来，和[这篇文章](https://d1wqtxts1xzle7.cloudfront.net/56029055/wavenet-with-cover-page-v2.pdf?Expires=1653489733&Signature=NZ8YPKKd4QYPyTFT8LDppJaaJyMLmGidAbnGDeuBHMw7LnvM2cWmiw0YEjxvua5Fjb3hEKU36z0WjnfGZBF3tGJwG1juJvWNULa0y7d2p96v3ab4vrgWhAiBYU3xVT-uiXvoNwVTWmiXaPOARFGc2ulOjfKBS4wpMvgYSHdhZT9IJ4dLmRIQFZpcmOC9~65DuTsKslExG9l37v9-Chzd18dKCYk8yPmza3ke~tNxXcvI2X4COKBSeYxYTdfViS1nivA8GflrCmuPZidn3jAPtdv4O3S1U9EKbtzicuQYvoAQ6mGgCIBwE0sqVtbvI7yKAJi-LBdijyjVgfTtkPaiXg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)类似。

我们上述这种对于equivariant neural network的设计不仅帮助极大的减小了所需要达到好的泛化效果的训练图片的数量，也减少了camera coordinates到image coordinates的计算量，从而网络可以更专心地做infering depth的任务。

**Architecture details**

所有层的kernel都是$$3 \times 3$$大小的，我们将13个dilated convolutions层堆叠在一起，dilation rates分别是1，1，2，4，8，16，1，2，4，8，16，1，1，每层的输出通道都是64，除了最后一层的输出通道是$$2N$$，一半是$$g_i$$，一半是$$d_i$$。我们对于每层都使用leakyRelu和batch normalization，除了最后一层。最后一层，也就是输出层，是不经过activation function的，前$$N$$个通道作为$$N$$个keypoint的$$d_i$$直接输出，而后$$N$$个通道作为keypoint的$$g_i$$还需要经过一个spatial softmax的计算。最后利用之前的公式计算出每个keypoints的u,v,z。

**Breaking symmetry**

很多object类别在至少一个对称轴上是对称的，比如说一辆轿车的左边和右边很类似。这对于网络来说是个难点，因为不同的part会显示出同样的视觉特征，这只能通过了解全局信息来解决。比如说，分清楚左轮和右轮需要了解更多的相对位置信息。

为了帮助打破这种对称性，我们可以让keypoint prediction依赖于某种对于pose粗糙的量化。这样一种coarse-to-fine的keypoint detection的方法在[这篇文章](https://openaccess.thecvf.com/content_cvpr_2015/papers/Tulsiani_Viewpoints_and_Keypoints_2015_CVPR_paper.pdf)里使用过。


**5. Additional Keypoint Characteristics**

除了上面已经说过了的主要的目标函数，对于很多下游的任务来说，还有很多常见的keypoints的性质可以起到帮助的作用，比如说：1）没有两个keypoints能对应到同一个3D location；2）keypoints需要在一个object的轮廓内。

Separation loss对于两个靠的太近的keypoints进行惩罚：

$$L_{sep} = \frac{1}{N^2}\Sigma_{i=1}^{N}\Sigma_{j \neq i}^N max(0, \delta^2-||X_i- X_j||^2)$$

和consistency loss不一样，这个loss是3D loss，也就是说即使两个pixel占据了同一个像素点，但只要它们的深度z不同，那么它们就可以这样分布。

理想情况下的keypoints不应该有这种separation loss。但是因为我们的keypoint并没有它们具体positions的监督数据，如果我们不加上这个loss，会有keypoints到同一个点的问题。

Silhouette consistency鼓励keypoints落在object的轮廓内。正如我们上面所说，我们网络对于第$$i$$个keypoint的position的预测是通过计算$$g_i$$的期望得来的。一个确保silhouette consistency的方法就是只在silhouette内让概率非零，设计一个单峰的low variance的spatial distribution。

在训练过程中，我们对于object还有其mask的信息，也就是$$b(u,v) \in \{0,1\}$$，其中1表示前景object，0表示背景。从而silhouette consistency loss可以被定义为：

$$L_{obj} = \frac{1}{N}\Sigma_{i=1}^N -log\Sigma_{u,v}b(u,v)g_i(u,v)$$

同时还有个loss用来使得variance比较小：

$$L_{var} = \frac{1}{N}\Sigma_{i=1}^N \Sigma_{u,v} g_i(u,v)||\left[u,v\right] - \left[u_i, v_i\right]||^2$$

这一项使得distribution峰比较尖，从而帮助控制keypoint落在object的边界内。


**6. Experiments**

我们的training data是从ShapeNet里生成的，其是一个很大的dataset，包含大约51000个3D模型，有超过270个种类的object。我们为多个object种类都建立了不同的training datasets，包括car，chair，plane等。

**6.1 Comparison with a supervised approach**

**6.2 Generalization across views and instances**

**7. Conclusion & Future work**

我们探索了基于一个end-to-end的geometric reasoning framework，而不是keypoint locations的监督数据，来学习一个3D keypoints的稀疏集合的可能性。我们显示，通过使用两个新的目标函数：relative pose estimation loss和multi-view consistency loss，我们可以在含有不同object instances以及不同的视角的同一个object类别的图片中找到consistent的3D keypoints。我们的keypoints要比直接利用keypoints locations的监督信息学习到的keypoints对于rigid 3D pose estimation任务效果更好。

我们通过训练具有随机背景的ShapeNet图片说明我们的网络对于真实世界的图片也是有用的。最近的在domain adaptation领域的工作可以用来改进我们的结果。同时，也可以直接用带有relative pose labels的真实世界的图片来训练KeypointNet。而这种relative pose labels也可以通过Structure-from-motion来自动估计出来。另一个有意思的方向是利用coarse pose initialization或者联合求解relative transformation，这样就可以不需要pose annotations了。

我们的方法还可以被拓展到处理任意数量的keypoints。比如说，我们可以为每个keypoint都输出一个confidence value，然后设定一个阈值来判断是否需要是真的keypoint，同时在没有顺序的keypoints集合上加一个loss。




### [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://openaccess.thecvf.com/content_cvpr_2017/papers/Simon_Hand_Keypoint_Detection_CVPR_2017_paper.pdf)
*Tomas Simon, Hanbyul Joo, Iain Matthews, Yaser Sheikh*

*CVPR 2017*

This paper proposed a framework to learn 3D keypoints of hand. 

The input is a keypoint detector $$d_0$$ trained on a small labelled dataset $$T_0$$, a sequence of images $$\left\{I_v^f, v=1,2,...,V, f=1,2,..,F\right\}$$, with $$v$$ denote the camera view and $$f$$ denote the time frame.

Hyperparameter: Number of Keypoints $$K$$.

**Step1** First use $$d_0$$ to calculate the image coordinates (no depth) and confidence of each keypoint $$k$$ of $$I_v^f$$, denoted as $$x_v^{f,k}$$ and $$C_v^{f,k}$$. Then use the random sample consensus to pick inliers out of each set $$\left\{(x_v^{k}, C_v^{k})\right\}$$ for each time frame $$f$$. Then the 3D wolrd coodinates are computed as:

$$X_v^{k} = argmin_X \Sigma_{v \in I_v^{k}} ||P_v(X)-x_v^k||^2_F$$

where $$I_v^k$$ is the inlier set, with $$X_f^k \in R^3$$ the 3D triangulated keypoint $$k$$ in frame $$f$$, and $$P_v(X) \in R^2$$ denotes projection of 3D point $$X$$ into camera view $$v$$. They use calibrated cameras, thus $$P_v$$ are known.

**Step2** Then they use a window through the time frame, and pick the frame with highest score. The score is defined as the sum of $$C_v^k$$, thus the frame that has the biggest confidence of all keypoints detection from all camera views.

**Step3** After picking this frame, they add the labelled images into the orignal training dataset and train a new keypoint detector, $$d_1$$. And so on.


### [Learning deep network for detecting 3D object keypoints and 6D poses](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Learning_Deep_Network_for_Detecting_3D_Object_Keypoints_and_6D_CVPR_2020_paper.pdf)
*Wanqing Zhao, Shaobo Zhang, Ziyu Guan, Wei Zhao*

*CVPR 2020*


## 2. 2D Keypoint Learning

### 精读 [OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)
*Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh*

*CVPR2017 and TPAMI 2019*

我们所说的是TPAMI的那个版本，比CVPR的那一版多了一些内容。

**1. Title**
realtime表明这个算法本身运行速度快，multi-person 2D pose estimation清晰的说明了这个算法是用来干什么的，而part affinity field则是说明所用的主要方法。而openpose是给这个算法起个好听的名字，可以发现并不是常见的标题名字的简写组成的名字，openpose后来成为了一个成熟的软件。这种起标题的方式简明，清晰，直接说明白了用什么办法干了什么事情，是一种很好的起标题的方式。


**2. Abstract**
两句话介绍了这篇论文所要解决的问题的背景，也就是multiple 2D person pose estimation的背景，并说明了这篇论文利用的是PAF来做到实时的效果。然后介绍了一下之前利用PAF的论文效果如何，为什么这篇论文利用PAF效果好。然后介绍了实验部分，最后说明这篇文章还发布了Openpose这样一个开源软件。

**3. Introduction**

3.1. 这篇文章的目的是要做multi-person pose estimation，而对于单人的pose estimation有很多论文已经做得很好了，但对于多人来说，有以下几个困难的地方：1）首先，我们并不知道图里到底有几个人，而且每个人所在的位置、大小都不清楚；2）其次，人与人之间可能存在干涉，比如说遮挡、关节的旋转等等，很难分清楚到底哪部分属于哪一个人；3）以往的论文里的算法的复杂度都会随着图里人数量的增加而增加，从而很难实现实时。

3.2. 对于multi-person 2D pose estimation，top-down的方式很常见，也就是先检测图片里有几个人，然后对每个人实行pose estimation，因为单人的pose estimation已经做得很好了，这个算法并不复杂。但它存在着两个很大的问题：首先如果一开始检测人的时候就检测错了或者遗漏了，那之后是没有补救办法的；其次，这样的方法需要对检测出来的每个人都做单人pose estimation，这会使得算法的复杂度和人的数量成正比。所以说，bottom-up的方法也被提了出来，这种方法有能够解决上述两个问题的潜力。但之前的bottom-up方法仍然效率不高，因为它们在最后还是需要利用全局信息来辅助判断，从而要花不少时间。

3.3. 我们在这篇文章里利用Part Affinity Field实现了实时的multi-person 2D pose estimation，而且在几个数据集上效果都是很具有竞争力的（这样表达说明它们的效果不一定比其他论文好，它们的卖点主要是快）。Part Affinity Field是一个2D向量的集合，表示的是四肢的位置和方向信息。我们之后会说明，利用bottom-up的方式，将detection和association结合起来（模型有两个主要部分，一个是PAF refinement，另一个是body part prediction refinement，而PAF就是实现association的方式）逐步推进，可以在利用很小的计算资源的情况下达到很好的效果。

3.4. TPAMI的这一版比CVPR的那一版多的内容有：1）说明了其实PAF refinement才是效果好的主要原因，不可或缺，而body part prediction refinement没那么重要。文章里也做了实验，加深了网络深度，而去掉了body part prediction refinement的部分，效果变好了，速度也变快了；2）我们又提出了一个有标注的foot数据集；3）为了显示我们这个模型很强的generality，我们在vehicle keypoint detection的任务上也做了实验；4）我们这篇论文是我们的开源软件OpenPose的说明文档，OpenPose是第一个实时的body, foot, hand和facial的keypoint识别软件，我们还和Mask R-CNN, Alpha-Pose等著名keypoint识别算法在运行时间上作了比较。


**4. Related Work**

**Single Person Pose Estimation**。单人的pose estimation，因为人是关节性的，所以说传统的方法都是通过将对于身体部位的布局的检测和它们之间的空间关系联合起来看，然后使用某种inference的方式来学习。而关节型的pose estimation的各部位之间的空间关系的表示方法，要么就是1）tree-structured graphical models，在相邻的部位之间做一些encoding；或者是2）non-tree models，在第一种tree-structured model的基础上加一些别的连线，从而实现对遮挡、对称、以及长途关系的表示（因为tree-structured model只对邻居部分有表示）。而对于身体各个部位的检测，CNN就起到了主要的作用，其在body part estimation里比传统的方法要好很多。

> 也就是说，其实single person pose estimation主要就分为两大部分，先想办法检测出身体的各个部分，然后再想办法利用一种好的表示方式来将这些部分连起来，从而实现pose estimation，也就是keypoint detection。

后来也有文章直接构建一个深度的graphical model将两部分放在一起解决；还有文章通过使用感受野比较大的CNN来学习身体部分之间的空间关系。也有文章是multi-stage的方式，每次利用全局信息来进一步优化每个身体部分所在位置的信息。但所有的上面的这些方法都是针对单人的，人的位置和scale在数据集里都是差不多的（不存在非常边缘或者大小差别过大的情况）。

**Multi-person Pose Estimation**。对于多人的pose estimation来说，绝大多数方法使用的是top-down的方式，即首先检测人，然后再单独的在每个检测到人的区域实行单人pose estimation算法。尽管这种方法使得成熟的单人pose estimation算法可以得到利用，但它不仅会因为早早的就预设了每个人的问题而存在问题，也会因为人和人之间也会存在空间依赖关系而出现问题（比如说遮挡等）。有一些文章已经开始考虑引入人与人之间的依赖关系了。有篇bottom-up的文章就没有使用person detection，而是将每个部分联系到每个人，但这个算法需要解决一个integer linear programming的问题，这个问题的计算复杂度很高，所以处理一张照片的时间要个把小时。后续这篇文章的跟进工作改进了身体部分的检测算子，并且改进了优化算法，从而使得检测一张照片的时间变成了几分钟，但最多只能处理150个身体部分。

在这篇文章的早期版本里，也就是CVPR那个版本里，我们介绍了part affinity field，它是一个representation，是由一系列的flow field组成的，而这些flow fields包含了每两个所检测到的身体部分之间的信息（可能是属于不同人的，也可能是一个人的）。和我们上面提到的两个引入人与人之间的依赖关系的论文不同，我们不需要训练就可以直接从PAF里获取pairwise的信息。而这些pairwise的信息对于multi-person pose estimation就已经是足够的了。

在我们的那篇CVPR文章提出之后，又有了一些新的工作。有篇文章进一步简化了各个身体部位的关系graph从而能更快的inference，其还将关节化的人的tracking形式化的表示为身体部分的spatio-temporal的grouping。还有篇文章提出了利用associative embedding来为每个keypoint打上标签，从而可以将标签相似（也就是说embedding之间的距离小）的keypoint归属于同一个人。还有一篇文章提出检测每个keypoint和它的相对位移，再计算每个keypoint属于哪个人。还有一篇文章提出了Pose Residual Network，其输入是keypoint和所检测到的人，而输出则是将keypoint归属到每个人。

我们这篇文章，对于之前的那篇CVPR文章做了一些扩充。我们证明PAF refinement是必要的且是最重要的，还做了实验去除了body part detection refinement的部分，速度增加了，效果变好了。我们同时还提出了第一个body和foot keypoint联合检测的detector，并且提出了一个foot keypoint数据集。我们证明将它们两联合检测不仅会减少inference的时间，还会保持准确率。最后，我们提出了OpenPose，是第一个实时进行body, foot, hand和facial keypoint detection的实时开源软件。


**5. Method**

![overview]({{ '/assets/images/OPENPOSE-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. Overall pipeline. (a) Our method takes the entire image as the input for a CNN to jointly predict (b) confidence maps for body part detection and (c) PAFs for part association. (d) The parsing step performs a set of bipartite matchings to associate body part candidates. (e) We finally assemble them into full body poses for all people in the image.*

Fig 1显示了我们的算法的一个整体的流程。整个系统的输入是一张大小为$$w \times h$$的RGB图片，而输出为图中每个人的生理结构上的2D keypoints的位置。首先，一个feedforward网络输出一个集合$$S$$，用来表示各个身体部位位置的2D confidence maps，和一个part affinity fields（也就是2D的向量）的集合$$L$$用来表示各个身体部位之间的从属程度。集合$$S = \(S_1, S_2, ..., S_J\)$$有J个confidence maps，每个对应一个身体部位，其中$$S_j \in R^{w \times h}$$，$$j = \{1,2,...,J\}$$。而集合$$L = \(L_1, L_2, ..., L_C\)$$有$$C$$个向量，每个对应一个肢体，其中$$L_c \in R^{w \times h \times 2\}$$，而$$c \in \{1,...,C\}$$。为了简洁，我们将身体部位的pairs描述为肢体（因为这里的身体部位就是一个keypoint，而keypoint pair就是将两个keypoint连起来，就表示了一部分肢体），但是某些身体部位的pair并不是肢体，比如说face，我们就笼统的这么说了。我们可以从fig 2看到，$$L_c$$里的每个点都是一个2D的vector。最终，confidence maps和PAFs（也就是集合S和集合L）通过greedy inference联系了起来用于输出图中所有人的2D keypoints位置。

![algorithm]({{ '/assets/images/OPENPOSE-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 2. 上方：多人pose estimation。同一个人的身体部分被连了起来，也包括了脚的keypoints（大脚趾，小脚趾和脚后跟）。下左：关于连接右手肘和手腕的肢体的PAFs。颜色表明了方向。下右：在关于连接右手肘和手腕的肢体的PAFs的每个像素点处的2D向量包含了肢体的位置和方向信息。*

**5.1. Network Architecture**

![architecture]({{ '/assets/images/OPENPOSE-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 3. multi-stage CNN的结构。前一部分的stage用来预测PAFs$$L^t$$, 而后一部分的stage用来预测confidence maps $$S^t$$。每个stage的输出和图片feature连接起来，作为下一个stage的输入。.*

我们的模型结构，如fig 3所示，迭代的预测包含身体部位之间关系的PAFs，如fig 3里蓝色的部分，之后再来检测confidence maps，如fig 3里的米色。这样一种迭代的方式使得预测更加的准确（注意中间的迭代stage也都是有supervision的）。

在我们CVPR的那版里，我们用了$$7 \times 7$$的卷积核，而我们这篇论文里，用3个$$3 \times 3$$的卷积核来替代从而减小计算量。如同DenseNet里那样，这3个卷积层的输出也连接起来了，从而能既有高层feature又有低层的。

**5.2 Simultaneous Detection and Association**

输入的图像通过一个CNN（用的是VGG-19的前10层）来生成一系列的feature maps $$F$$，之后再输出到第一个stage当中。在这个stage里，网络输出一个集合的PAFs $$L^1 = \phi ^ 1 (F)$$，其中$$\phi ^ 1$$表示stage1里用来inference的CNN。在之后的stages里，前一个stage的输出和原始的图像feature maps $$F$$连接起来作为输入，用来输出更加精确的结果

$$L^t = \phi ^ t (F, L^{t-1}), 2 \leq t \leq T_p$$

其中$$\phi ^ t$$表示stage t里用来inference的CNN，$$T_p$是PAF stage的总数。在$$T_p$$个PAF stage之后，再来计算confidence maps，利用的是最新的PAF结果：

$$S^{T_p} = \rho^t (F, L^{T_p}), t = T_p$$

$$S^t = \rho^t (F, L^{T_p}, S^{t-1}), T^p < t \leq T_p + T_C$$

其中$$\rho^t$$指的是stage t用来inference的CNN，而T_C是confidence maps所需要循环的次数。

这个方法和我们的CVPR那个版本里的方法有很大的不同，在那个版本里，每个stage都要进行PAF和confidence maps的计算。因此我们这个版本的计算量减少了一半。我们通过实验发现，增加PAF循环的次数，可以使得效果变好，而增加confidence maps的循环的次数并不会。直觉上来看，通过PAF的结果，我们可以猜到每个身体部位（也就是每个keypoint）的位置，但通过confidence maps的结果，我们无法知道每个身体部位属于哪个人。

![PAF]({{ '/assets/images/OPENPOSE-4.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 4. 右前小臂肢体的PAF。尽管在一开始的stage里还有一些不清晰，但在之后的stage里可以看到PAF很清晰。.*

Fig 4显示了随着stage的增加，PAF计算结果逐渐被精确的过程。confidence maps的计算基于最后一个PAF stage输出的结果，而随着confidence maps stage的推进，结果区别并不大。为了让模型能够预测PAF和body part的confidence maps，我们在每个stage的结尾都使用一个loss function。我们使用的是输出的结果和实际上的body part的confidence maps和PAF之间的$$L_2$$ loss。我们这里给loss function加了权重，因为有些数据集并没有标注完整。对于PAF的stage $$t_i$$和confidence map的stage $$t_k$$的loss function是：

$$ f_L^{t_i} = \Sigma_{c=1}^C \Sigma_p W(p) ||L_c^{t_i}(p) - L_c^{\*}(p)||^2_2 $$

$$ f_S^{t_k} = \Sigma_{j=1}^J \Sigma_p W(p) ||S_j^{t_k}(p) - S_j^{\*}(p)||^2_2 $$

其中$$L_c^{\*}$$是PAF的ground truth，$$S_j^{\*}$$是身体部分confidence map的ground truth，$$W$$是一个非0即1的二分掩码，如果某个位置没有标注就是0，有标注就是1。这个$$W$$是用来避免因为没有标注而导致的错误训练。我们在每个stage都使用loss function，用来解决梯度消失的问题，因为每个stage结尾都有loss function，对梯度的值进行了补充。从而整体的的loss function就是：

$$ f = \Sigma_{t=1}^{T_p} f_L^t + \Sigma_{t=T_p + 1}^{T_p + T_C} f_S^t $$


**5.3. Confidence Maps for Part Detection**

为了能够在训练过程中计算上述的$$f_S$$，我们从有标注的2D keypoints上生成confidence maps $$S^{\*}$$的ground truth。一个confidence map是一个2D的矩阵，用来表示一个keypoint出现在一个像素点的概率值。理想状态下，如果图里只有一个人，那么每个confidence map应该只有1个峰（该keypoint没有被遮挡住的情况下）;如果有多个人，那么每个confidence map对于没有被遮挡住的这一类keypoint都应该有峰（比如说某个confidence map专门表示人的nose的keypoint）。

我们先来对于每个人$$k$$生成confidence map，$$S_{j,k}^{\*}$$。$$x_{j,k} \in R^2$$表示人$$k$$的身体部分$$j$$在图中位置的ground truth。从而$$S_{j,k}^{\*}的位置$$p \in R^2$$处的值就是：

$$ S_{j.k}^{\*}(p) = exp(-||p-x_{j,k}||^2_2 / \sigma^2) $$

其中$$\sigma$$控制峰的大小。从而整个图片（可能包含多个人）的ground truth就是：

$$ S_j^{\*}(p) = max_k S_{j,k}^{\*}(p) $$

**5.4 Part Affinity Fields for Part Association**

![cal]({{ '/assets/images/OPENPOSE-5.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 5. 身体部位association策略。（a) 几个人的两种身体部分（也就是keypoint）分别用红蓝点表示，而所有的点之间都连上了线。（b) 利用中间点进行连线。黑线是正确的，绿线是错误的，但他们都满足连接了一个中间点。(c) 利用PAF来连接，黄色的箭头就是PAF的结果。利用肢体来表示keypoint的位置和keypoint之间的方向信息，PAF减少了错误association的可能性。.*

给定一些已经检测到了的body parts（Fig 5里的红色和蓝色的点），那我们该如何将它们组合起来从而构建未知数量的人的肢体呢？我们需要对每一对body part keypoints都有一个confidence measure，也就是说，measure它们是否属于同一个人。一种可能的方式就是检测这一对keypoints的中间是否还有附加的midpoint。但是当人聚集在一起的时候，很容易出错。这种方式之所以不好是因为1）它仅仅有位置信息，并没有一对keypoint之间的方向信息；2）它仅仅用了midpoint，而不是这两个keypoints之间的所有部分当成一个肢体来使用。

Part Affinity Fieds (PAFs)解决了这些问题。它对于每一对keypoints构成的肢体提供了位置和方向信息。每一个PAF都是一个2D的vector field，在Fig 2里有显示。对于每个肢体的PAF的每个位置的值，其都是一个2D的向量，包含了位置和这个肢体一个keypoint指向另一个keypoint的方向。每个类别的肢体都有一个对应的PAF（由对应的body part keypoints对组成）。

![fig]({{ '/assets/images/OPENPOSE-6.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 6.*

考虑fig 6所示的一个简单的肢体。$$x_{j_1, k}$$和$$x_{j_2, k}$$是人$$k$$的身体部位$$j_1$$和$$j_2$$的ground truth，而这两个部位组成了肢体$$c$$。对于肢体$$c$$上的一点$$p$$，$$L_{c,k}^{\*}(p)$$是一个单位向量，从$$j_1$$指向$$j_2$$；对于其它的点，$$L_{c,k}^{\*}(p)$$的值都是0。

为了能在训练过程中计算$$f_L$$的值，我们需要定义PAF的ground truth，也就是对于人$$k$$，$$L_{c,k}^{\*}在$$p$$点的值为：

$$ L_{c,k}^{\*}(p) = v if p on limb c, k$$ and $$0$$ otherwise。

这里，$$v = (x_{j_2, k} - x_{j_1, l}) / ||x_{j_2, k} - x_{j_1, l}||$$是肢体的有方向的单位向量。一个肢体上的点不仅仅只有两个keypoints连线上的，而是有一个距离阈值，比如说：

$$ 0 \leq v (p - x_{j_1,k}) \leq l_{c,k}$$ 和 $$|v_{verticle} (p - x_{j_1,k})| \leq \sigma_l$$

其中肢体宽度$$\sigma_l$$自定义的，肢体长度由两个keypoints决定，也就是$$l_{c,k} = ||x_{j_2, k} - x_{j_1, k}||，$$v_{verticle}$$是垂直于$$v$$的。


而整个图片的PAF的ground truth是对于所有人取了均值：

$$ L_c^{\*}(p) = 1/n_c(p) \Sigma_k L_{c,k}^{\*}(p) $$

在测试过程中，我们通过计算连接两个keypoints的线段间的PAF的积分来衡量这两个keypoints是否构成了一个肢体。对于两个身体部分$$d_{j_1}$$和$$d_{j_2}$$，我们计算：

$$ E = \int_{u=0}^{u=1} L_c(p(u)) (d_{j_2} - d_{j_1})/||d_{j_2} - d_{j_1}|| du $$

其中$$p(u) = (1-u) d_{j_1} + u d_{j_2}$$。在实践中，我们通过等距离采样来近似这个积分值。


**5.5. Multi-Person Parsing using PAFs**

对于每个身体部位keypoint的location，我们都有好几个备选的值，这是因为图中有多个人或者因为计算错误。而这些keypoints组成的肢体就会有很多种可能了。我们用5.4里定义的积分来计算每个肢体的积分值。从而问题变成了，如何在众多的有着不同积分值（也就是score）的肢体集合中，选择合适的肢体并将其正确连接起来，而这是个NP-hard的问题，如fig 7所示。

![fig]({{ '/assets/images/OPENPOSE-7.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 7. Graph matching。(a) 原始的图片，已经有了身体部位keypoint标注了。(b) K-partite graph。(c) 树状结构。(d) 二分图。*

在这篇文章里，我们使用一种greedy relaxation的方法，持续性的产生高质量的匹配。我们猜测这种方法有效的原因是上述计算的积分值（也就是每个肢体的score）潜在的含有global信息，因为PAF的框架具有较大的感受野。

具体来说，首先，我们获得整张图片身体部分keypoint的集合，$$D_J$$，其中$$D_J = \{d_j_m: j \in \{1, ..., J\}, m \in \{1, ..., N_j\}\}$$，$$N_j$$是身体部位$$j$$的候选数量，而$$d_j^m \in R^2$$是身体部位$$j$$的第$$m$$个候选位置。我们需要将每个身体部位keypoint连接到属于同一个人的同一个肢体的其它身体部位keypoint上，也就是说，我们还需要找到正确的肢体。我们定义$$z_{j_1, j_2}^{m,n} \in \{0, 1\}$$来表示两个身体部位keypoint的候选，$$d_{j_1}^m$$和$$d_{j_2}^n$$是否连在一起，我们的目标是为$$Z = \{z_{j_1, j_2}^{m,n}: j_1, j_2 \in \{1, ..., J\}, m \in \{1, ..., N_{j_1}\}, n \in \{1, ..., N_{j_2}\}$$找到最优的值。

如果我们考虑一个特定的keypoint的pair $$j_1$$和$$j_2$$（比如说neck和right-hip），叫做c-肢体，而我们的目标是：

$$ \max\limits_{Z_c} E_c = \max\limits_{Z_c} \Sigma_{m \in D_{j_1}} \Sigma_{n \in D_{j_2}} E_{m,n} z_{j_1, j_2}^{m,n}$$

$$s.t., \forall m \in D_{j_1}, \Sigma_{n \in D_{j_2}} z_{j_1, j_2}^{m, n} \leq 1$$

$$ \forall n \in D_{j_2}, \Sigma_{m \in D_{j_1}} z_{j_1, j_2}^{m,n} \leq 1$$

其中，$$E_c$$是所有的c-肢体的积分值的和（可能有多个c-肢体，因为可能有多个人），$$Z_c$$是$$Z$$的只关于c-肢体的子集，$$E_{m,n}$$是keypoint $$d_{j_1}^m$$和$$d_{j_2}^n$$之间的定义的积分值，上述要优化的目标的条件，使得我们所学习到的结果里不会有两个肢体公用同一个keypoint。我们可以用Hungarian算法来获取上述优化的结果。

现在我们考虑所有的肢体，那么上述优化的式子即是需要考虑整个$$Z$$并且需要计算所有肢体的所有可能结果，计算$$Z$$是一个K-维的匹配问题（K是肢体的数量）。这个问题是个NP-hard的问题，有很多relaxations的算法存在。在我们这篇论文中，我们添加了两个relaxation。首先，完整的图会对于每两个不同类别的keypoint都有edge，而我们将这个图简化为其能表示人的pose的spanning tree就可以，而多余的edge就不要了。其次，我们将上述K-维的匹配问题解构为一系列二分匹配的子问题并且独立的解决这些问题，所利用的就是每个spanning tree的相邻的两个node所对应的值以及它们之间的连线，所以说是独立的。第二个relaxation之所以可行，直觉上来说，spanning tree里相邻的两个node之间的关系是由PAF网络学习到的，而非相邻的两个node之间的关系是由CNN网络学习到的。

有了上述两个relaxations，我们的问题被简化为：

$$ \max\limits_{Z} E = \Sigma_{c=1}^C \max\limits_{Z_c} E_c $$

从而我们将这个优化问题分解为独立的每个pair的优化问题，而这个在之前所述，可以用Hungarian算法解决。我们再将有共同keypoint的肢体联合起来，这样其就表示出了一个完整的人的pose，或者说骨架。我们的第一个relaxation，将完整的图简化为spanning tree使得整个算法获得了很大程度的加快。

我们目前的模型仍然有多余的PAF连接（比如说耳朵和肩膀的连接，手腕和肩膀的连接等）。这样冗余的连接使得我们的算法对于人群很密集的时候准确度较高。对于冗余的PAF连接，也就是有冗余的肢体，我们在5.5里的parsing算法进行一些简单的修改就行。


**6. OpenPose**

现在有一系列cv和ml的应用需要2D人的pose estimation作为系统的输入。为了帮助研究者们加速它们的工作，我们公布了OpenPose，是第一个实时的只通过一张输入图片联合检测多人的body，foot，hand和face keypoints（一共135个）的系统。

**6.1. System**

目前的2D人的pose estimation库，比如Mask R-CNN或者Alpha-Pose，需要用户自己运行模型，而且也需要自己运行数据处理。而且，目前的face和body的keypoint检测子并未被联合起来，需要用不同的库来实现。而OpenPose解决了所有的这些问题。它可以运行在不同的平台上，包括Ubuntu，Windows，Mac OSX，以及嵌入式系统（比如Nvidia Tegra TX2)。OpenPose还为不同的硬件提供了支持，比如CUDA GPUs，OpenCL GPUs，以及仅有CPU的设备。用户还可以使用image，video，webcam，以及IP camera流作为输入。我们还可以选择是直接展示结果还是将结果存在硬盘里，允许或者不允许body, foot, face和hand的keypoint检测，控制所使用的GPU的数量，跳过某些frame等操作。

OpenPose包括三个不同的组件：(a) body+foot检测；(b) hand检测；(c)face检测。核心组件是body+hand检测。除了使用我们这论文里的模型，你还可以选择使用我们那篇CVPR论文里的模型，而且是在COCO和MPII数据集上训练过的。当我们有了body的keypoint后，face的bounding box就可以通过body的keypoint来确定了，特别是ears，eyes, neck, nose等这些点。而同样的，hand的bounding box也可以通过arm keypoint来确定。这种思想传承于我们在introduction里提到的top-down的keypoint检测算法。OpenPose还提供3D的人的pose estimation，是通过使用non-linear Levenverg-Marquardt refinement来实现3D triangulation实现的，而输入是多个同步的相机。

**6.2. Extended Foot Keypoint Detection**

现存的人的pose数据集包含有限的body part类型。MPII数据集标注了ankles, knees, hips, shoulders, elbows, wrists, necks, torsos以及head tops。而COCO数据集还包括了一些face keypoints。对于这两个数据集，都不含foot keypoints,仅仅有ankles keypoints。但是很多图形学的应用要求foot keypoints，至少要有big toe和heel。如果没有foot keypoints，有很多图形学的应用就会出现floor penetration，foot skate，candy wrapper effect等问题。而我们重新标注了很多foot keypoint的数据。

使用我们的数据集，我们训练了一个foot keypoint检测算法。一个naive的foot检测子可以通过先训练一个body keypoint检测子从而获取脚部的bounding box，之后再在bounding box上训练foot keypoint检测子。但这个方法仍然有我们在introduction里就说过的问题。在我们的论文里，我们使用检测body keypoint的模型来检测body+foot keypoint。我们的body+foot检测模型还包含了两个hip keypoint之间的那个点，为了在upper torso看不到的情况下仍然有好的效果。我们在实验中发现，加入了foot keypoint也有助于body keypoint的学习，特别是ankle。fig 8显示了有些情况下如果没有foot keypoint，无法预测ankle keypoint。

![foot]({{ '/assets/images/OPENPOSE-8.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 8. Foot keypoint analysis。(a) foot keypoint标注，包括big toes，small toes和heels。(b) 仅仅检测body keypoint的模型并未成功检测到右脚ankle这个keypoint。(c) body+foot keypoint联合检测的模型成果检测到了右脚ankle keypoint，因为foot keypoint信息帮助了其的检测。*



**7. Datasets and Evaluations**

我们在multi-person pose estimation的三个benchmarks上验证我们的方法：(1) MPII human multi-person dataset，其包括了3844个training和1758个testing图片，每张图片都是多人的图片，标注了14个body keypoints；(2) COCO keypoint challenge dataset，每张图片也包括了多人，每个人标注了17个keypoints（5个面部的，12个身体的）；(3) 文中提出的foot dataset，是由COCO dataset加上我们自己的标注生成的，15K张图片。这三个数据集包括了各种各样场景下的图片，而且还包括了人群，人的scale不同，遮挡，以及接触等众多情况。我们的方法在COCO 2016 keypoints challenge上获了第一名，在MPII数据集上远超其它方法。我们还与Mask-RCNN和Alpha-Pose进行了对比，量化了我们这个系统的效率，并分析了失败的案例。


**在MPII，COCO和自己生成的数据集上的实验省略不说了，效果都很好。论文也和Mask-RCNN等方法进行了对比，在效果差别很小的情况下，速度要快了很多。我们重点来看看在vehicle pose estimation上应用论文中的模型的内容。**


**7.1. Vehicle Pose Estimation**

我们这篇论文里的方法不仅限于human body或者foot的keypoint检测，还可以拓展到任意的keypoint检测任务。为了说明这个，我们用同样的网络结构在vehicle keypoint detection任务上也进行了运行。还是用在object keypoint similarity (OKS)上定义的mean average precision (mAP)来衡量效果。效果很不错，fig 9显示了在数据集上的效果。这个实验用的是Intersection dataset，从这篇文章来的：https://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf

![car]({{ '/assets/images/OPENPOSE-9.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 9. 验证集里的vehicle keypoint检测的结果，vehicle的keypoint在很具有挑战性的情况下仍然被检测出来，这些情况包括遮挡、vehicles之间的overlap，不同的scale等。*


**8. Conclusion**

实时的multi-person 2D pose estimation是使得机器能够理解和推断人和人之间的互动的关键点。(1) 在这篇文章里，我们展示了一个keypoint association的explicit nonparametric representation，其包含了人的肢体的position和orientation的信息。(2) 我们设计了一个联合学习keypoint和keypoint association的模型。(3) 我们使用了一个greedy parsing的算法来产生高质量的人的body pose的结果，而且对于多人的情况仍然效率很高。(4) 我们证明了PAF refinement要比keypoint detection refinement重要得多，从而让我们的模型相对于之前的CVPR版本要快很多。(5) 将body和foot keypoint检测联合起来会在效果和效率上都有提升。我们构造了一个包含了15K张图片的foot keypoint数据集。(6) 我们将这篇论文的结果开源作为OpenPose，是第一个实施的body, foot, hand和face keypoint检测系统。OpenPose在很多地方都有应用，且已经被收入了OpenCV库里。


## 3. Keypoint matching

### [Multi-Image Semantic Matching by Mining Consistent Features](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.pdf)

*Qianqian Wang, Xiaowei Zhou, Kostas Daniilidis*

*CVPR 2018*

**Abstract**

这篇论文提出了一种多张图片匹配的方法来估计多张图片之间的语义对应关系。和之前需要优化所有的成对的对应关系的方法不一样，我们提出的方法只需要识别并匹配图片集合里一个稀疏的features集合就可以。利用这种方法，我们提出的方法可以去除那些不可重复的features，而且对于上千张图片也是可以操作的。我们还提出了一个low-rank的约束来确保在整个图片集合里的features对应关系具有geometric consistency。我们的方法除了在multi-graph matching和semantic flow任务上表现很好之外，我们还可以用这个方法来重构object-class模型，并且从没有任何标注的图片中找到object-class landmarks。


**1. Introduction**

计算图片之间的feature对应关系在CV领域是个很基础的问题。low-level的geometric features（比如说SIFT）对于相同场景下的图片匹配做得很好。最近，对于semantic matching的兴趣越来越高涨，比如说在不同的object instances或者scenes之间建立semantic对应关系。semantic matching这个领域大多数的研究工作集中在成对的情况下，也就是说每次只考虑一个图片对（两张）。而在多张图片中找到consistent correspondences在很多情况下很重要，比如说object-class model reconstruction，automatic landmark annotation等。这篇文章的关注点就是多张图片的semantic matching任务。

尽管semantic matching和multi-image matching问题已经有了很多的进展（related work），但是下述的问题仍然存在。首先，semantic matching的可重复的feature point的寻找仍然是个未解决的问题。之前的工作通过使用所有的pixels（dense flow）或者随机挑选points来避开这个问题，但是导致的结果就是大量的不可重复的features，它们导致和其它的图片之间建立不了真正的对应关系。其次，之前的multi-image matching方法主要优化的是cycle consistency，而并没有同时考虑geometric consistency。现在已经有了很多方法在pairwise的设定下加入了geometric约束（比如说RANSAC和graph matching），但对于multi-image的设定并没有解决方案。最后，现存的multi-image matching方法计算量很大，对于几百张图片就算不了了。所以分析更大的数据集需要更scalable的算法。

在大多数情况下，我们只需要一些具有cycle consistency和geometric consistency的可以高度重复的features能够对应就可以了，而它们只会占据features集合的一小部分。dense的对应关系可以通过插值的方法来实现。因此，和之前那些优化所有的pairwise对应关系的multi-image matching方法不同的是，我们将问题描述为一个feature选择和labeling的问题：从繁复的pairwise对应关系出发，我们在每张图片的初始candidates集合里选取一些feature points，然后通过给它们labels来构建它们在多张图片之间的对应关系。上述挑选和加上标签的过程是通过共同优化cycle consistency和geometric consistency来实现的。将问题描述成这样的形式可以让我们：1）显式的在初始的feature points集合里解决掉那些不可重复的feature points；2）大量的减少变量的数量，从而设计出一个scable的模型，可以解决上千张图片。受到factorization-based structure from motion的一些经典结果的启发，我们提出一个low-rank的约束来使得multi-image matching具有geometric consistency，而且其有很高效的优化算法。fig 1给了个例子来解释问题描述以及我们所提出的方法。

![eg]({{ '/assets/images/MATCHING-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. 给定多张图片的初始feature candidates和每两张图片之间的带有噪音的features之间的对应关系，我们提出的方法从这些feature points中识别出一部分可靠的，并且为它们在所有的图片间构建cycle consistent和geometric consistent的对应关系。上述figure给了一个通过我们提出的方法从1000张图片中挑选出的可靠的feature points的例子（彩色的十字表示feature points）。十字的颜色代表着其之间的对应关系。上述figure的最后一列显示了一张图片的初始的feature points集合（上面）以及手动标注的landmarks（下面）。有意思的是，用我们的方法（unsupervised的方法）来找到的可靠的feature points和手动标注的landmarks高度相似。*

我们这篇文章的主要贡献如下：

* 我们提出了一个新的方法来解决multi-image semantic matching的问题，我们将其转换为一个feature selection和labeling的问题。我们所提出的算法可以在图片集合中找到consistent的feature points，并且是scable的，可以处理上千张图片。
* 我们为multi-image matching介绍了一个新的low-rank的约束，从而我们的算法可以同时优化cycle-consistency和geometric consistency。
* 我们在标准的benchmarks上阐述了我们的方法的很好的效果。我们同时也展示了两个应用：1）从一系列同类但不同instance的图片出发重建3D object-class models，而且并不需要任何手动标注；2）匹配了1000张猫头的图片并且发现算法自动找到的feature points十分具有代表性，它们和表示eyes，ears，mouth等图片的关键点很重合，所以说我们这个方法在automatic landmark detection这个任务上也是很有潜力的。

**2. Related work**

**Image Matching**

在经典的image matching任务里，图片之间的稀疏的feature对应关系是通过low-level的geometric feature detectors（比如说corners和covariant区域）和descriptors（比如说SIFT，SURF，HOG等）来估计的。geometric consistency是通过使用RANSAC作为一个后续的处理步骤或者通过解决一个最小化图片之间的geometric distortion的graph matching问题来实现的。很多最近的工作尝试在不同的场景下找到semantic对应关系。hierarchical matching和region-based方法已经被用来找到图片里的high-level的semantics。

**Learning detectors and descriptors**

最近的结果表明利用CNN获取的deep features在matching领域很有效果，并且远远超过了手动设计的features，即便CNN不是针对matching任务来训练的也是一样。监督学习被用来显式的学习descriptors。监督学习的标签来自于手动标注的对应关系、图片transformations和一些额外的信息，比如说CAD models等。同时，也有一些研究尝试学习可重复并且对于transformations鲁棒的feature detectors。这些方法可以被看成一种非监督的学习方式，它们从图片集合中学到了可靠并且consistent的对应关系。


**Multi-image matching**

multi-image matching在技术上和joint maching方法相关。大多数现存的方法都是利用cycle-consistency来改进pairwise对应关系。很多方法被提了出来，比如说unclosed cycle elimination，constrained local optimization，spectral relaxation以及convex relaxation。我们所提出的方法和上述都不同，因为我们希望能找到在所有图片上最consistent的features而不是在所有的pairwise对应关系之间进行优化，从而使得我们的方法更加的scable。而且，我们介绍了一个low-rank的约束来对于所选择的feature points做geometric consistency的优化。在[这篇文章](https://openaccess.thecvf.com/content_iccv_2015/papers/Yan_A_Matrix_Decomposition_ICCV_2015_paper.pdf)里为了multi-graph matching提出的matrix decomposition的方法在graph edges上采用了同样的low-rank约束。在这篇文章里，我们的low-rank约束直接加在feature points locations上，算法更加高效。最近也有的方法提出一种高效的方法来为matching任务寻找具有辨别型的feature points clusters，但是他们并没有考虑geometric consistency的约束。



**3. Preliminaries and notation**

**3. 1 Pairwise matching**

给定n张需要去匹配的images，每张image有$$p_i$$个feature points，对于每个image pair $$(i,j)$$的pairwise feature对应关系可以用一个partial permutation矩阵$$P_{i,j} \in \{0, 1\}^{p_i, p_j}$$来表示，其还需要满足一个限制条件：doubly stochastic constraints，

$$0 \leq P_{i,j} 1 \leq 1$$

$$0 \leq P_{i,j}^T 1 \leq 1$$

$$P_{i,j}$$可以通过在满足上述条件下，最大化其自身和feature similarities之间的inner product来实现。这是一个linear assignment问题，这个问题已经被研究透彻了，并且可以用HUngarian algorithm来解决。寻找$$P_{i,j}$$还可以被构造成一个graph matching问题，之后转换为一个quadratic assignment问题（QAP）。而这个问题通过最大化一个既含有local compatibilities（feature similarity）信息又含有structural compatibilities（spatial rigidity）信息的目标函数来找到assignment。尽管QAP是NP-hard的，但很多高效的算法已经被提出来近似地解决这个问题。我们使用通过linear matching或者graph matching得到的结果，$$W_{i,j} \in R^{p_i \times p_j}$$，作为我们的输入。

**3.2 Cycle consistency**

近期的工作[1](https://arxiv.org/pdf/1402.1473.pdf), [2](https://pages.cs.wisc.edu/~pachauri/perm-sync/assignmentsync.pdf)和[3](https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_27.pdf)提出使用cycle consistency作为一个constraint来进行多张images的匹配。如果下面的条件对于任意的图片$$i,j,k$$都满足，那么所有的图片对之间的对应关系就是cyclically consistent的：

$$P_{i,j} = P_{i,z}P_{z,j}$$

cycle consistency可以通过介绍一个虚拟的universe来更加清晰的描述，这个universe定义为这些图片构成的集合的unique features [2](https://pages.cs.wisc.edu/~pachauri/perm-sync/assignmentsync.pdf)。universe里的每个feature point都必须至少被一张图片所记录到，并且还要建立与这张图片上某个feature point的对应关系。假设图片i和universe之间的对应关系可以用一个partial permutation矩阵$$X_i \in \{0,1\}^{p_i \times u}$$来表示，其中$$u$$是universe里feature points的个数，而且$$u \geq p_i$$对于所有的$$i$$都成立。因为对于universe来说，我们假设cycle-consistent是成立的，所以说pairwise对应关系就可以被表示为

$$P_{i,j} = X_i X_j^T$$

我们将所有的partial matrix按照下面的方式连起来：

![lql]({{ '/assets/images/MATCHING-2.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}

有结果表明，集合$$\{P_{ij}| \forall i,j\}$$是cyclically consistent的当且仅当$$P$$可以被分解为$$XX^T$$。


**4. Proposed methods**

**4.1 Matching by labeling**

回忆一下，$$X \in \{0,1\}^{m \times u}$$是从image feature points到universe feature points的一个map，其中$$m$$和$$u$$分别表示image的local features的个数和universe的features的个数。另一种对$$X$$的解释是$$X$$的每一行是image里的feature的一个label。对于不同图片的features，如果它们的label相同，那么就是相互对应的。为了能够处理所有的图片里的features，之前的工作经常会用一个很大的$$u$$。

但是，并不是图片里的每一个feature都适合拿来做匹配。尤其是在semantic matching的任务里，很多的随机或者平均采样的features在多张图片之间是nonrepeatable的，它们应该在matching的时候被剔除。通过这个事实的启发，我们从大量的feature points里只挑选出$$k$$个，而$$k$$是我们预先设定好的超参数，也就是说每张图片只有$$k$$个feature points被选出来做matching。

>这个$$k$$需要针对图片里所包含的object的不同的种类做不同的调整，而不能通过某种方式学习到，这可以是将来的一个拓展的方向。

假设图片$$i$$的feature points和universe之间的对应关系的partially permutation矩阵是$$X_i \in \{0.1\}^{p_i \times k}$$。每个$$X_i$$是一个partial permutation矩阵，而且是一个瘦长型的矩阵，因为列数为$$k$$，是个较小的值。这个矩阵需要满足：

$$ 0 \leq X_i 1 \leq 1$$

$$ X_i^ T 1 = 1$$

也就是说每个图片里的feature points可能在universe里没有对应的点，有也最多只有一个。但universe里的每个点在每张图片里都有对应的feature points。

>这和我们考虑的情况就不一样了，因为我们考虑的是3D的object，每张图片可能不含有全部的关键点的信息，所以说每列的和也不是1。

>而且这种方法也不能有occlution，也就是说所有的图片里的object的所有的关键点都要能看得到。

集合$$\{X_i|1 \leq i \leq n\}$$就是我们这篇工作里需要去估计的东西。$$X_iX_j^T$$就会给出图片$$i$$和图片$$j$$之间的feature points对应关系。根据3.2里说的，只要$$P$$能分解为$$X_iX_j^T$$，那么其就是cycle-consistent的，而此处就直接通过$$X_iX_j^T$$来表示$$P_{i,j}$$了，所以当然是cycle-consistent的。因为我们想要在initial pairwise matching的基础上找到具有cycle-consistent的并且具有辨识度的feature points，我们通过最小化initial pairwise matching的结果和利用$$X_iX_j^T$$算出来的图片$$i$$和图片$$j$$之间的cycle-consistent的匹配结果，来估计$$X$$的值：

$$ \min_{X} \frac{1}{4}||W - XX^T||^F_2$$

$$s.t. X_i \in P^{p_i \times k}, 1 \leq i \leq n$$

其中$$P$$代表所有的partial permutation矩阵的集合，$$W \in R^{m \times m}$$是$$W_{i,j}$$的集合，其中$$m = \Sigma^t_{i=1} p_i$$是所有图片的feature points的数量的总和。通过解决上述的优化问题，我们就能够找到这个图片集合里最具有代表性的$$k$$个互相匹配的feature points，而且它们满足cycle-consistent的性质。


**4.2 Geometric constraint**

假设我们对同一个场景的$$n$$张图片找到了$$k$$个匹配的feature points。我们使用$$M_i \in R^{2 \times k}$$来表示在图片$$i$$里的这$$k$$个feature points的二维坐标，而且是按照顺序排列的。将所有的这样$$n$$个矩阵$$M_i$$按照行连接起来，就得到了一个$$M \in R^{2n \times k}$$的矩阵，其中每列就是每个feature point。$$M$$在structure from motion里叫做measurement matrix [4](https://link.springer.com/article/10.1007/BF00129684)。有结果表明，在orthographic projection处理之后，$$M$$的秩是4。

在我们的问题里，我们将图片$$i$$里所有的feature points的二维坐标表示为$$C_i \in R^{2 \times p_i}$$。之后，通过$$X_i$$我们可以选出$$k$$个feature points，而这$$k$$个feature points在图片$$i$$里的二维坐标就是$$M_i^{'} = C_iX_i$$，其中$$M_i^{'}$$储存了所选中的$$k$$个feature points的在图片$$i$$里的二维坐标信息，而且是按顺序排列的。

同样的，我们也可以将$$M_i^{'}$$按行来排列，从而得到一个矩阵$$M^{'} \in R^{2n \times k}$$：

![ma]({{ '/assets/images/MATCHING-3.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}

如果feature points都是正确的被挑选出来并且正确的被贴上label，那么矩阵$$M^{'}$$在经过orthographic projection之后就应该是秩为4的。即使object是non-rigid的，$$M^{'}$$仍然可以被一个low-rank矩阵近似 [5](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.230.5079&rep=rep1&type=pdf)。上述的结论可以很好的被利用起来，从而更好的来估计$$X$$的值。假设$$M^{'}$$的真实的秩不比$$r$$大。最小化下面的项可以让我们对于所选择的$$k$$个feature points加上geometric consistent的约束：

##f_{geo} = \frac{1}{2}||M^{'} - Z||^F_2 = \frac{1}{2}\Sigma^n_{i=1} ||C_iX_i - Z_i||^F_2$$

其中$$Z \in R^{2n \times k}$$是一个辅助变量，其秩不大于$$r$$，并且$$Z_i \in R^{2 \times k}$$代表着矩阵$$Z$$的第$$(2i-1)$$行和第$$2i$$行。


>这里的操作很奇怪，为什么要引入一个新的$$Z$$，为什么不直接SVD分解矩阵$$M^{'}$$，之后挑选出最大的r个eigenvalue重新组合成一个矩阵，再来最小化$$M^{'}$$和这个新矩阵之间的Frobius norm。


**4.3 Formulation**

将cycle-consistency和geometric consistency联合起来，我们就获得了最后我们需要优化的目标函数：

$$ \min_{X,Z} \frac{1}{4}||W - XX^T||^F_2 + \frac{\lambda}{2}\Sigma^n_{i=1} ||C_iX_i - Z_i||^F_2$$

$$s.t. X_i \in P^{p_i \times k}, 1 \leq i \leq n, rank(Z) \leq r$$

其中$$\lambda$$控制geometric constraint所占的比重。


**4.4 Optimization**

对于上述的optimization问题，我们通过block coordinate descent方法来优化，也就是说交替的固定其它的变量优化其中一部分变量。

我们曾经尝试将上述optimization问题里对$$X$$矩阵要求点是0或者1的离散条件，改成$$X$$里的点是0到1之间的连续的值，这样的改动在quadratic assignment problems里很常见。然而，我们观察到如果我们做了这样的改动，那么$$C_iX_i = Z_i$$这个约束对于任意的$$Z_i$$来说就是个ill-posed问题，从而geometric constraint就不管用了。因此，我们要保留$$X$$的integer constraint。

>这里值得再深究

因此，为了让上述的optimization问题能够操作，我们将optimization式子里的两项解耦，也就是让它们没有关系，我们引入一个辅助变量$$Y \in R^{m \times k}$$，在第一项中用$$Y$$来替代$$X$$，并且再加入一项来使得$$X$$和$$Y$$尽量靠近，注意到$$Y$$里每个元素的值不是离散的0或者1，而是实数域。从而我们的optimization问题就变成了下面这样：

$$ \min_{X,Z} \frac{1}{4}||W - YY^T||^F_2 + \frac{\lambda}{2}\Sigma^n_{i=1} ||C_iX_i - Z_i||^F_2 + \frac{\rho}{2}||X - Y||^F_2$$

$$s.t. X_i \in P^{p_i \times k}, 1 \leq i \leq n, Y \in C, rank(Z) \leq r$$

其中$$C$$表示满足下列约束关系的矩阵集合：

$$0 \leq Y \leq 1, 0 \leq Y_i1 \leq 1, Y_i^T1 = 1, 1 \leq i \leq n$$

其中$$\lambda$$和$$\rho$$分别用来控制X和Y之间的差距以及geometric constraint所占的比重。当$$\rho$$趋近于无穷大的时候，上述optimization问题则等价于原始的那个。

将之前的optimization问题改造成上述的形式，是因为改造完之后，每个subproblem都变得很简单了。我们用以下的方式交替更新Y，X和Z。

Y是通过projected gradient descent方法来更新的：

$$ Y \leftarrow \Pi_C(Y-\eta(YY^TY - WY + \rho(Y-X))) $$

其中$$\PI_C$$表示到$$C$$上的projection，$$\eta > 0$$是step size。我们先一直优化$$Y$$直到它收敛，再优化X和Z。

每个$$X_i$$是通过Hungarian algorithm来更新的，它的cost matrix构造如下：

$$H_i = \lambda D(C_i, Z_i) - 2\rhoY_i$$$

其中$$D(C_i, Z_i) \in R^{p_i \times k}$$表示$$C_i$$和$$Z_i$$里每个二维点之间的Eucldean距离。

$$Z$$是通过SVD分解来更新的：

$$Z = U\SigmaV^T$$

其中U和V的columns都是来自于$$M^{'}$$的singular vectors，而$$\Sigma$$则是选取了$$M^{'}$$矩阵最大的$$r$$个singular values构造的对角矩阵。（也就是说构造一个rank为r的矩阵Z，其是用SVD分解$$M^{'}$$来实现的）

为了更好的收敛，我们利用一种逐步增大$$\rho$$的方式。对于每个$$\rho$$，我们按照上述方式先更新Y，再交替更新X和Z，直到目标函数不再减小。因为每次更新不会增加目标函数的值，所以说我们最终可以达到局部收敛。在我们的实验里，我们设置$$\rho$$为1，10，199，设置$$\lambda=1$$以及$$r=4$$。

因为这个优化问题是nonconvex的，而且还涉及连续和离散两种变量，所以说需要很好的初始化的技巧。我们首先按照解下述优化问题来初始化Y的值（忽略geometrical consistency）：

$$ \min_{Y} \frac{1}{4}||W - YY^T||^F_2 $$

$$s.t. Y \in C$$

这个优化问题的解是：

$$ Y \leftarrow \Pi_C(Y-\eta(YY^TY - WY)) $$

而X的初始化可以通过将Y离散化而得来，也就是将Y的值变成0或1。


#### 5. Experiments

**5.1 Multi-graph matching**

我们先再multi-graph matching任务上验证我们方法的有效性，在这个任务里，feature point的位置是有标注的，但是它们的对应关系需要被正确预计。匹配的准确率是用recall来衡量的，定义为算法所找到的的真实的对应关系除以所有的真实的对应关系的总和。

我们使用CMU dataset和WILLOW Object Class dataset来做实验。CMU dataset包含hotel sequence和house sequence。每一帧图片都标注了30个feature points的位置并给了它们的SIFT特征。WILLOW Object Class dataset提供了五个object类型的图片（car，duck，motorbike，face和winebottle）以及为每张图片提供10个标注位置的特征点。因为物体的外观在每个类别的众多图片中差别很大，所以几何描述子SIFT很难作用。所以我们采用了利用预训练的CNN网络来提取deep features的方法来获取特征。详细的说，就是每张图片都喂给了一个AlexNet（在ImageNet上预训练），然后将Conv4和Conv5的关键点对应位置的输出连起来作为特征。对于这两个数据集，初始的pairwise对应关系都是通过linear matching solver Hungarian algorithm所得到的，之后再用我们本文中提出的方法来操作。三个有公开代码的方法被用作baselines：[spectral method](https://pages.cs.wisc.edu/~pachauri/perm-sync/assignmentsync.pdf)，[MatchLift](https://arxiv.org/pdf/1402.1473.pdf)和[MatchALS](https://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.pdf)。对于所有的方法，universe的feature points的个数都被设置为每张图片里标注的points的数量。

recall的结果由Table 1显示。Table 1表明我们的方法要比其它的方法要好很多。Table 1还显示了两个结果：1）如果利用[graph matching solver RRWM](https://www.researchgate.net/profile/Minsu-Cho-5/publication/221304918_Reweighted_Random_Walks_for_Graph_Matching/links/54c50ae80cf256ed5a98633c/Reweighted-Random-Walks-for-Graph-Matching.pdf)来取代之前的初始的pairwise对应关系，匹配的准确率在除了duck的情况下都能到100%。2）如果忽略geometrical consistency，匹配的准确率会急剧降低。

![Ta]({{ '/assets/images/MATCHING-4.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Table 1. 在CMU dataset和WILLO Object Class dataset上的recall的结果。我们提出的方法与spectral method，MatchLift以及MatchALS方法进行了对比。$$Ours^-$$指的是我们的方法没有geometric consistency的情况。$$Input^+$$和$$Ours^+$$分别指的是使用RRWM graph matching方法来进行初始的pairwise对应关系的计算，在没用我们的方法和用了我们的方法之后的结果。

fig 2显示了利用geometrical consistency我们可以纠正geometrically distorted matches。

![exam]({{ '/assets/images/MATCHING-5.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 2. 上面一行是没有使用geometrical consistency的情况，下面一行是使用了的情况。蓝色的十字表示正确的对应关系，而红色的十字表示错误的对应关系。*

我们所提出的方法还能够自动选择可靠的feature points用来做matching。fig 3给了个例子，我们给CMU dataset加了随机的feature points。而我们的方法可以去掉这些随机加入的feature points。

![add]({{ '/assets/images/MATCHING-6.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 3. 正确的匹配是绿色的，错误的匹配是红色的。上面一行是只用了pairwise方法来找到的对应关系，而下面一行是再经过我们提出的方法找到的对应关系。我们还在原数据集的每一帧中加入了30个随机挑选位置的feature points作为outliers。我们设置$k=30$。*


**5.2 Dense semantic matching**

在这一节里，我们通过将我们提出的方法和region based semantic flow methods（比如说，[proposal flows](https://arxiv.org/pdf/1703.07144.pdf)）结合起来的方式来在dense matching这个任务上测试我们提出的方法的效果。在proposal flow method里，图片之间的region proposals的对应关系先被估计出来，然后再转换为一个dense flow field。对于一系列图片，我们将所提出的方法应用在proposal flow上，从而改进proposals的pairwise对应关系，因此改进dense flow。

我们通过在PF-WILLOW dataset上来衡量效果，这个数据集将WILLOW Object Class dataset分成了10个子类。它们分别是car(S), (G), (M), duck(S), motorbike(S), (G), (M), winebottle(w/oC), (w/C)和(M)，其中(S)和(G)表示side和general角度，而(C)表示背景很杂乱，(M)表示混合角度。每个子类都包含10张不同object的图片。

percentage of correct keypoints (PCK)被用来作为衡量标准。它衡量了当利用estimated flow将一张图片的标注的keypoints转移到另一张图片上的时候位置正确的keypoints的比例。一个预测的feature point被认为是在正确的位置上如果它到groundtruth point的距离在$$\alpha max(h,w)$$以内，其中$$\alpha$$在0到1之间，$$h$$和$$w$$分别是object bounding box的高和宽。对于proposal flow，selective search（SS）被用来作为proposal generator，HOG是feature descriptor，local offset matching（LOM）用作geometric matching strategy。每张图片都会被提取500个proposals，并用来做matching以及生成dense flow。在我们的算法里，每个proposal都被当做一个feature point，每个proposal的中心点就作为我们geometric consistency里所用的coordinate。对于每个类别我们都设定选取10个feature points，也就是$$k=10$$。

Table 2显示我们的方法改进了原论文里的效果。fig 4给了可视化的效果。

![add]({{ '/assets/images/MATCHING-7.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Table 2*

![add]({{ '/assets/images/MATCHING-8.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 4. 我们将source images扭曲成target image，使用了proposal flow估计出来的dense correspondence，也使用了我们提出的方法进行了优化。*


**5.3 Object-class model reconstruction**

将不同的object instances匹配起来在从一系列图片来重建object-class model这个任务里一直都是一个主要的挑战。一些前期的工作依赖于图片里标注的keypoints [6](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Carreira_Virtual_View_Networks_2015_CVPR_paper.pdf), [7](https://openaccess.thecvf.com/content_cvpr_2015/papers/Kar_Category-Specific_Object_Reconstruction_2015_CVPR_paper.pdf), [8](https://openaccess.thecvf.com/content_cvpr_2014/papers/Vicente_Reconstructing_PASCAL_VOC_2014_CVPR_paper.pdf)。
最近也有工作不需要keypoints标注，但需要object mask来去除背景，[9](https://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.pdf)。我们的方法可以为reconstruction提供consistent的对应关系，而且不需要额外的人工标注。我们利用FG3DCar dataset来验证，匹配一共37张左眼的sedan图片，之后再来重建一个3D模型。我们还额外收集了含有30张不同的motorbikes的dataset。

类似于[9](https://openaccess.thecvf.com/content_iccv_2015/papers/Zhou_Multi-Image_Matching_via_ICCV_2015_paper.pdf)的做法，我们在利用structured forests检测出的图片边缘上均匀采样。而不同于[9]的是，因为我们的方法能够去掉那些nonrepeatable的feature points，所以background里的feature points就自动没了，我们不需要object mask。大约每张图片获取了550个feature points candidates。在5.1里所说的deep features被用作每个feature points的feature，并且之前提到的graph matching solver RRWM被用来做初始pairwise matching。

我们使用precision作为衡量标准，定义为算法所找到的对应关系里真实的对应关系，除以算法找到的所有的对应关系。真实的对应关系的定义和前面在PCK里所用的方法差不多。我们使用不同的$$k$$来测试我们的方法的效果，fig 5显示了结果。

![add]({{ '/assets/images/MATCHING-9.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 5. 早FG3DCar dataset上，对于不同的$$k$$值，precision的结果。*

fig 5的结果表明，使用我们的方法改进的效果要比使用pairwise matching要好得多，pairwise matching在背景很杂乱的时候效果很差。

>但是问题是，用precision来衡量，而且从fig 5也能看出来，在$$k$$很小的时候效果比较好，也就是说在feature points对应关系
越少，效果越好。但这并不能说明什么，因为对于对应关系多的情况，即使它的precision比较低，但也可能是因为多出来的那部分都检测的不对，但这并不能说明它的效果就弱于对应关系少但是效果好的情况。但文章里说的是，所用的对应关系越少，效果越好，说明它们的方法去除掉了那些nonrepeatable的feature points。但这是得不出这个结果的，除非可视化来看到底去除掉了哪些feature points。

fig 6说明了文中提出的方法确实能去除掉很多nonrepeatable的背景中的对应关系（看看就好，没有什么说服力）

![back]({{ '/assets/images/MATCHING-9.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 6. 两个sedan图片之间的匹配结果。正确的和错误的匹配分别用蓝色和红色来表示。上面一行表示的是使用pairwise对应关系得到的结果，下面一行是使用文中提出的方法得到的结果。可以看到很多初始的背景里的feature points都被文中的方法给去掉了。*

对于reconstruction来说，我们简单的利用factorization method来运行了affine reconstruction。最后被选择的feature points，对应关系，以及reconstruction结果在fig 7中被可视化。可以看到，绝大多数的被选择的feature points落在object上并且都能正确匹配，尽管object的外观和视角都很不一样。尽管有一些噪音和没有追踪到的points，我们可以在reconstruction的结果中看到sedan和motorbikes的形状。如果用更成熟的reconstruction方法一定会得到更好的结果。

之前并没有工作在不同的instances之间利用unsupervised的方法进行relative pose estimation。

![rec]({{ '/assets/images/MATCHING-10.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*figure 7. *


**5.4 Automatic landmark annotation**

我们将所提出的方法应用在cat head dataset的前1000张图片上。和之前的实验一样，feature points candidates从图片边缘中采样得到，每张图片得到大约43个candidates。我们将$$k$$设置为10，也就是selected feature points的数量是10。结果在fig 1中显示。如同fig 1所示的那样，初始的candidates在整张图片上均匀分布（包括背景），而selected feature points都在object上，而且对应关系也能在具有不同外观和姿态的object上建立。更加有意思的是，这种自动检测的feature points和人工标注的landmarks重合度很高，都表示了cat的具有特征性的feature points，比如说ears，eyes，mouth等。这个结果说明了我们的方法对于automatic landmark detection这个方向也存在潜力，整个过程模拟了人标注的过程：我们比较了一系列的图片，并且找到了不随着外观、几何形状以及姿态而改变的那部分feature points。


##### 5.5 Computational complexity

#### 6. Conclusion

我们展示了一个新的方法，其将在多张图片上寻找semantic matching的问题转换为了feature points selection和labeling的问题来解决。我们所提出的方法可以在一系列图片之间建立可靠的feature points对应关系，而且这种对应关系还是cycle consistent和geometry consistent的。实验表明我们的方法要比之前的multi-image matching方法要好，并且对于上千张图片也是scable的。我们同时还阐述了几个可能的应用：改进dense flow estimation，不使用人工标注进行reconstruction object-class models，以及自动进行image landmark标注。




































---

*If you notice mistakes and errors in this post, don't hesitate to contact me at* **wkwang0916 at outlook dot com** *and I would be super happy to correct them right away!*

