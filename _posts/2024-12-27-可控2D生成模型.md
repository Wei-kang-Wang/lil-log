---
layout: post
comments: True
title: "可控2D生成模型"
date: 2024-12-27 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

CVPR2023的[3D-aware Conditional Image Synthesis](https://www.cs.cmu.edu/~pix2pix3D/), ICLR2024的[IC-Light: Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport](https://openreview.net/forum?id=u1cQYxRI1H), ICCV2023 Best Paper [ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models](https://github.com/lllyasviel/ControlNet), [DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models](https://mc-e.github.io/project/DragonDiffusion/), [Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis](https://hyper-sd.github.io/)

## \[**ICCV 2017**\] [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://junyanz.github.io/CycleGAN/)

## \[**Arxiv 2018**\] [StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks](https://github.com/NVlabs/stylegan)

## \[**CVPR 2020**\] [StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN](https://github.com/NVlabs/stylegan2)

## \[**ICCV 2021 Oral**\] [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://github.com/orpatashnik/StyleCLIP)

## \[**ICCV 2023**\] [Adding Conditional Control to Text-to-Image Diffusion Models](https://github.com/lllyasviel/ControlNet)

[CODE](https://github.com/lllyasviel/ControlNet)


## [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://dreambooth.github.io/), [StyleShot: A SnapShot on Any Style](https://styleshot.github.io/), 


## \[**ICLR 2023**\] [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://textual-inversion.github.io/)

## \[**CVPR 2024**\] [ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models](https://lukashoel.github.io/ViewDiff/)

## [FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection](https://samsunglabs.github.io/FineControlNet-project-page/)

## \[**ACM SIGGRAPH 2023**\] [Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)

## \[**CVPR 2022 Oral**\] [GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation](https://yudeng.github.io/GRAM/)

## \[**ECCV 2024**\] [OMG: Occlusion-friendly Personalized Multi-concept Generation In Diffusion Models](https://kongzhecn.github.io/omg-project/)

## [DragVideo: Interactive Drag-style Video Editing](https://dragvideo.github.io/)

## \[**ECCV 2024**\] [Viewpoint Textual Inversion: Discovering Scene Representations and 3D View Control in 2D Diffusion Models](https://jmhb0.github.io/view_neti/)
