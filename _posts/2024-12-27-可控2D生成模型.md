---
layout: post
comments: True
title: "可控2D生成模型"
date: 2024-12-27 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---

CVPR2023的[3D-aware Conditional Image Synthesis](https://www.cs.cmu.edu/~pix2pix3D/), ICLR2024的[IC-Light: Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport](https://openreview.net/forum?id=u1cQYxRI1H), ICCV2023 Best Paper [ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models](https://github.com/lllyasviel/ControlNet), [DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models](https://mc-e.github.io/project/DragonDiffusion/)

## \[**ICCV 2023**\] [Adding Conditional Control to Text-to-Image Diffusion Models](https://github.com/lllyasviel/ControlNet)

[CODE](https://github.com/lllyasviel/ControlNet)


[DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://dreambooth.github.io/), [StyleShot: A SnapShot on Any Style](https://styleshot.github.io/), 


## \[**ICLR 2023**\] [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://textual-inversion.github.io/)
