---
layout: post
comments: True
title: "关键点检测"
date: 2024-07-30 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 2D Keypoint Detection from Images

### Supervised 2D keypoint Detection

#### \[**ECCV 2016**\] [Stacked hourglass networks for human pose estimation](https://arxiv.org/pdf/1603.06937.pdf)

*Alejandro Newell, Kaiyu Yang, Jia Deng*

[CODE](http://www-personal.umich.edu/~alnewell/pose)

![hourglass0]({{ '/assets/images/HOURGLASS-0.PNG' | relative_url }}){: width=200px style="float:center"} 

这篇文章是2D keypoints detection的经典之一（因为它的Hourglass网络结构）。

Hourglass网络的设计受启发于获取各个尺度的信息的需求。尽管局部信息对于识别比如faces或者hands等局部物体来说很重要，但最终的pose estimation需要对整个human body有一个全面的理解。人的orientations，四肢的arrangements，以及相邻关节的关系等等，这些信息都需要同一张图片不同尺度的信息才能良好的获得。Hourglass网络能够将各个尺度的信息综合起来从而输出pixel-wise的预测。
这样的网络必须要有某些机制来有效的处理和合并不同尺度的特征。之前有些工作使用独立的不同的网络来处理不同分辨率的图片输入，再将这些输入综合起来处理。但Hourglass network使用的是一个单一的pipeline，使用skip connections来在各个分辨率下保存空间信息。

Hourglass网络的结构如下：卷积层和max pooling层将输入图片的分辨率降低到一个很低的值。在每个max pooling层，在做max pooling之前，输入分叉为两部分，不进入max pooling的那部分会做更多的卷积操作，留着之后使用。最后在达到了最低分辨率之后，网络开始进行upsampling以及结合之前不同分辨率下的features的操作。为了将两个相邻分辨率的特征结合起来，作者对较低分辨率的feature map使用了最近邻upsampling方法，再加上较高分辨率的那个feature map，从而得到输出。hourglass网络的结构是对称的，之前每用max pooling降低分辨率一次，之后就使用upsampling提升分辨率一次，所以最终的分辨率会和输入图片分辨率一样。在之后，再加上两个卷积大小为$$1 \times 1$$的卷积层，从而得到网络最终的输出。网络的最终输出是一系列的heatmaps，每个heatmap表示的是每个keypoint在图片中每个像素点出现的概率值的大小。

初始输入的图片分辨率为$$256 \times 256$$，为了节约计算成本，作者在将图片输入hourglass模块之前，先通过了一个带有残差链接的$$stride=2$$，$$padding=3$$，卷积核为7的卷积层，和一个max pooling层，从而分辨率变为$$64 \times 64$$，之后在进入上述所说的hourglass模块里。对于hourglass里所有层输出的feature maps，其通道数都是256。

本文提出的最终的网络结果是多个hourglass模块堆叠而成，前一个hourglass模块的输出是后一个hourglass模块的输入。这样的结构使得网络可以重新衡量初始的估计值，改进效果。而且使用这种结构，网络中间某个hourglass模块的输入也可以拿来计算loss，具体做法是，对于中间某个hourglass模块的输出，其经过$$1 \times 1$$卷积得到heatmap输出，从而可以拿来被计算loss，而这个heatmaps再经过$$1 \times 1$$的卷积层回到操作之前的通道数，再和这个hourglass的输出加起来，作为下一个hourglass模块的输入。每个hourglass模块的权重并不共享，而且每个hourglass拿来计算的loss使用的是同一种loss以及同一个ground truth。

本文的evaluation是根据标准的Percentage of Correct Keypoints（PCK）metric来计算的，给定一个normalized的distance，落在ground truth这个distance以内的都算判断正确，而PCK metric计算的是判断正确的keypoints占的比例。本文使用了FLIC和MPII两个数据集来测试效果，对于FLIC数据集，这个distance是利用躯干的大小进行了normalize，而对于MPII数据集，这个distance是根据头的大小进行了normalize。

> 这篇文章并不能解决多人的问题，对于多人的情况，只会考虑最靠近图片中心的那个。如果要看多人的算法，可以去看OpenPose那篇论文。

> 本文的方法不能检测到被遮挡住的keypoints，这仍然是一个需要被解决的问题。


#### \[**CVPR 2017** $$\&$$  **TPAMI 2019**\] [OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)
*Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh*

[post](https://github.com/CMU-Perceptual-Computing-Lab/openpose)

OpenPose是一个成熟的可以进行multi-person的2D关键点检测的算法（已商业化）

对于单人的pose estimation有很多论文已经做得很好了，但对于多人来说，有以下几个困难的地方：1）首先，我们并不知道图里到底有几个人，而且每个人所在的位置、大小都不清楚；2）其次，人与人之间可能存在干涉，比如说遮挡、关节的旋转等等，很难分清楚到底哪部分属于哪一个人；3）以往的论文里的算法的复杂度都会随着图里人数量的增加而增加，从而很难实现实时。

对于multi-person 2D pose estimation，top-down的方式很常见，也就是先检测图片里有几个人，然后对每个人实行pose estimation，因为单人的pose estimation已经做得很好了，这个算法并不复杂。但它存在着两个很大的问题：首先如果一开始检测人的时候就检测错了或者遗漏了，那之后是没有补救办法的；其次，这样的方法需要对检测出来的每个人都做单人pose estimation，这会使得算法的复杂度和人的数量成正比。所以说，bottom-up的方法也被提了出来，这种方法有能够解决上述两个问题的潜力。但之前的bottom-up方法仍然效率不高，因为它们在最后还是需要利用全局信息来辅助判断，从而要花不少时间。

这篇文章里利用Part Affinity Field实现了实时的multi-person 2D pose estimation。Part Affinity Field是一个2D向量的集合，表示的是四肢的位置和方向信息。利用bottom-up的方式，将detection和association结合起来（模型有两个主要部分，一个是PAF refinement，另一个是body part prediction refinement，而PAF就是association）逐步推进，可以在利用很小的计算资源的情况下达到很好的效果。

![overview]({{ '/assets/images/OPENPOSE-1.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 1. 算法流程。输入是一张大小为$$w \times h$$的RGB图片，而输出为图中每个人的生理结构上的2D keypoints的位置。首先，一个feedforward网络输出一个集合$$S = (S_1, S_2, ..., S_J)$$有J个confidence maps，每个对应一个身体部位，其中$$S_j \in R^{w \times h}$$，$$j = \lbrace 1,2,...,J \rbrace$$，用来表示各个身体部位位置的2D confidence maps，和一个part affinity fields（也就是2D的向量）的集合$$L= (L_1, L_2, ..., L_C)$$，每个$$L_c \in R^{w \times h \times 2}$$对应一个肢体，$$c \in \lbrace 1,...,C \rbrace$$，用来表示各个身体部位之间的从属程度。最终，confidence maps和PAFs（也就是集合S和集合L）通过greedy inference联系了起来用于输出图中所有人的2D keypoints位置。*

![architecture]({{ '/assets/images/OPENPOSE-3.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 3. OpenPose里的stages。上图左边部分用来预测PAFs $$L^t$$, 而右侧部分用来预测confidence maps $$S^t$$。每个stage的输出和图片feature连接起来，再作为下一个stage的输入。*

> 注意每个stage的CNN并不共享参数

> OpenPose有两个版本：TPAMI版本和CVPR版本，在CVPR版本里，每个stage都会同时输出PAF和confidence maps，然后结合图片features作为下个stage的输入。而在TPAMI版本里，前$$T_P$$个stages仅仅输出PAF，而后$$T_C$$个stages仅输出confidence map，这样可以大大减少计算量。这样的改进是因为作者通过实验注意到，减少每个stage的输出，并不会太多影响实验结果，而且需要将PAF的stages放在前面。这也符合intuition，因为如果给定一张图片的PAF，很容易猜到每个keypoint在哪里，但如果只是给一系列的keypoints，不仅无法知道单个人的keypoints怎么连接，更不知道keypoints都分别是属于哪个人的。

对于PAF的stage $$t_i$$和confidence map的stage $$t_k$$的loss function分别是：

$$ \mathcal{L}_{P}^{t_i} = \Sigma_{c=1}^C \Sigma_p W(p) \lvert L_c^{t_i}(p) - L_c^{\ast}(p) \rvert^2_2, \quad \mathcal{L}_{C}^{t_k} = \Sigma_{j=1}^J \Sigma_p W(p) \lvert S_j^{t_k}(p) - S_j^{\ast}(p) \rvert^2_2 $$

其中$$L_c^{\ast}$$是第$$c$$个肢体的PAF的ground truth，$$S_j^{\ast}$$是第$$j$$个keypoint的confidence map的ground truth，上述两个求和都是对所有像素进行的，$$p$$指的就是像素点。$$W(p)$$是图片的mask。

> 在每个stage都使用loss function，用来解决梯度消失的问题，因为每个stage结尾都有loss function，对梯度的值进行了补充。

网络整体的loss function就是$$ \mathcal{L} = \Sigma_{t=1}^{T_P} \mathcal{L}_{P}^{t} + \Sigma_{t=T_P + 1}^{T_P + T_C} \mathcal{L}_{C}^t $$

Confidence maps GT，$$S_j^{\ast}$$的计算方式是，先对于图片里的每个人$$k$$生成confidence map，$$S_{j,k}^{\ast}$$。$$x_{j,k} \in \mathbb{R}^2$$表示人$$k$$的keypoint$$j$$的ground truth。从而$$ S_{j,k}^{\ast}(p) = exp(-\lvert p-x_{j,k} \rvert^2_2 / \sigma^2) $$，其中$$\sigma$$控制峰的大小。从而整个图片（可能包含多个人）的ground truth就是：$$ S_j^{\ast}(p) = \max_k S_{j,k}^{\ast}(p) $$

Part Affinity Fieds (PAFs) GT的计算方式较为复杂一点。每一个PAF都是一个2D的vector field。对于每个肢体的PAF的每个像素位置的值，其都是一个2D的向量，包含了位置和这个肢体一个keypoint指向另一个keypoint的方向。预先定义好的body part keypoints对决定了哪些有哪些keypoints对构成肢体。

对于某张图片，其可能含有多个人，假设$$x_{j_1, k}$$和$$x_{j_2, k}$$是人$$k$$的两个keypoints，这keypoints对（有序）组成了肢体$$c$$。对于肢体$$c$$上的一点$$p$$，$$L_{c,k}^{\ast}(p)$$是一个单位向量，从$$j_1$$指向$$j_2$$，对于其它的点，$$L_{c,k}^{\ast}(p)$$的值都是0。但肢体不仅是一条直线，而是一个具有一定宽度的长方形。具体来说：$$v = (x_{j_2, k} - x_{j_1, l}) / \lvert x_{j_2, k} - x_{j_1, l} \rvert$$是从$$x_{j_1, k}$$指向$$x_{j_2, k}$$的单位向量，$$v_{\bot}$$是垂直于$$v$$的一个单位向量，而对于任意的像素点$$p$$，如果其满足$$ 0 \leq v (p - x_{j_1,k}) \leq \lVert x_{j_2, k} - x_{j_1, k} \rVert$$ 和 $$\lvert v_{\perp} (p - x_{j_1,k}) \rvert \leq \sigma_l$$，那么就认为$$p$$在肢体$$c$$上，从而$$L_{c,k}^{\ast}(p) = v$$，别的像素点位置的值就是0（长度为2的0向量）。

而整个图片的PAF的ground truth是对于所有人取了均值：$$ L_c^{\ast}(p) = 1/n_c(p) \Sigma_k L_{c,k}^{\ast}(p) $$，其中$$n_c(p)$$统计的是在点$$p$$处非零向量的PAF的个数，也就是对应着有相交的肢体的情况。

有了上述confidence map和PAF的GT的定义，就可以开始网络的训练了。

而在测试过程中，对于输入的图片，对于每个keypoint，能得到一个预测的confidence map，使用non-maximum suppresion（非极大抑制）可以得到该keypoints可能出现的多个位置，这些多个位置可能对应着图片里多个人的该keypoint，但也可能是假阳性。为了进一步筛选，就需要使用预测的PAF。对于每张图片，我们得到了$$C$$个PAF，对于每个PAF，我们计算连接该PAF对应的两个keypoints之间的线段上PAF的积分来作为score，用来衡量这两个keypoints是否构成了一个肢体。

> 如果这两个keypoints按照之前的non-maximum suppression的计算，都有好几个预测值，那么对于每对都需要计算该积分

这个积分具体是这样算的，对于两个检测到的keypoints，$$d_i$$和$$d_j$$：

$$ E_{d_i, d_j} = \int_{u=0}^{u=1} L_c(p(u)) (d_j - d_i)/ \lVert d_j - d_i \rVert du, \quad p(u) = (1-u) d_i + u d_j$$。

> 在具体操作的时候，上述积分使用离散点的和来近似

从而我们将从一系列所有keypoints的candidates里挑选最终的keypoint set的问题描述为了一个最大化所有的PAF对应的score的和的优化问题，这个优化问题的约束条件是，每个PAF必须包含且仅包含该PAF对应的两个keypoints的分别各一个candidate。然而这个优化问题是个NP-hard的问题。这篇文章使用一种greedy relaxation的方法，持续性的产生高质量的匹配。

> 文章猜测这种方法有效的原因是上述计算的积分值（也就是每个肢体的score）潜在的含有global信息，因为PAF的框架具有较大的感受野

具体来说，首先，我们获得整张图片所有keypoints的所有candidates的集合，$$D_J = \lbrace d_{j}^{m}: j = 1, ..., J, m = 1, ..., N_j \rbrace$$，$$N_j$$是keypoint $$j$$的candidates的数量，而$$d_j^m \in R^2$$是keypoint $$j$$的第$$m$$个candidate的position。定义$$z_{i,j}^{m,n} \in \{0, 1\}$$来表示两个keypoint $$i,j$$的两个候选$$d_{i}^m$$和$$d_{j}^n$$是否构成肢体。肢体的候选集合为$$Z = \{z_{i, j}^{m,n}: i, j \in \{1, ..., J\}, m \in \{1, ..., N_i \}, n \in \{1, ..., N_j \}$$。

严格来描述这个优化问题，对于一对keypoint对$$i$$和$$j$$（比如说neck和right-hip），以及它们组成的肢体$$c$$，目标是：

$$ \max\limits_{Z_c} E_c = \max\limits_{Z_c} \Sigma_{m \in D_i} \Sigma_{n \in D_j} E_{m,n} z_{i, j}^{m,n}, \quad s.t., \forall m \in D_i, \Sigma_{n \in D_j} z_{i, j}^{m, n} \leq 1, \quad \text{and} \quad \forall n \in D_j, \Sigma_{m \in D_i} z_{i, j}^{m,n} \leq 1$$

> 上述定义的constraint，使得我们所优化到的结果里不会有两个$$c$$肢体公用同一个keypoint。而对于该优化目标$$E_c$$，我们可以用Hungarian算法来获取上述优化的结果。

但问题的目标$$Z$$是所有的肢体的目标$$Z_c$$的和，计算$$Z$$的最优值是一个$$K-Matching$$问题（$$K$$是肢体的数量），这个问题是个NP-hard的。对于该问题，有很多relaxations的算法存在。在这篇论文中，因为按照对人体的肢体的定义，所有的肢体$$c$$以及所有的keypoints构成的集合是一个spanning tree，作者将上述$$K-Matching$$问题分解为一系列二分匹配的子问题并且独立的解决这些问题，也就是将$$Z$$的优化分解成了独立对每个$$Z_c$$进行优化，即：

$$ \max\limits_{Z} E = \Sigma_{c=1}^C \max\limits_{Z_c} E_c, \quad s.t., \forall c, \quad \forall m \in D_i, \Sigma_{n \in D_j} z_{i, j}^{m, n} \leq 1, \quad \text{and} \quad \forall n \in D_j, \Sigma_{m \in D_i} z_{i, j}^{m,n} \leq 1 $$

> 第二个relaxation之所以可行，直觉上来说，spanning tree里相邻的两个node之间的关系是由预测PAF的网络学习到的，而非相邻的两个node之间的关系是由预测Keypoint的网络学习到的。

从而我们将这个优化问题分解为独立的每个pair的优化问题，而这个在之前所述，可以用Hungarian算法解决。我们再将有共同keypoint的肢体联合起来，这样其就表示出了一个完整的人的pose，或者说骨架。


### Unsupervised 2D Keypoint Detection

#### \[**ICCV 2017**\] [Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)

[CODE](https://github.com/alldbi/Factorized-Spatial-Embeddings)

这篇文章的方法基于factorizing image deformations，这种deformation可能是由viewpoint变化引起的，或者是因为物体本身的形变引起的。作者提出的方法基于的假设是deformation前后的图片的keypoints是consistent的。

> 实际上，通过对图片加上人为构造的deformation，将原始图片上所检测到的keypoint加上deformation，以及加上deformation之后的图片所检测到的keypoint，的差异作为约束条件，是现如今非常常见的一种约束keypoint detection的辅助loss，在FewShot3DKP，StableKeypoint里都有这样的loss

$$S \subset \mathbb R^3$$表示一个物体的3维surface，比如说一只鸟，$$\pmb x: \Lambda \rightarrow \{0,1,\cdots,255 \}^3$$表示的是这个物体的一张RGB图片，其中$$\Lambda$$是image grid。$$S$$是3维物体的形状，其和$$\pmb x$$是无关的。作者考虑的是学习一个函数$$q = \Phi_S(p; \pmb x)$$将$$S$$上的点$$p \in S$$映射到对应的图片$$\pmb x$$上的某个点$$q \in \Lambda$$上去。作者提出了一个利用viewpoint factorization来自动学习$$\Phi_S$$的方法。为了实现这个目标，我们考虑从另一个角度来看这个物体从而获得的另一张图片$$\pmb{x^{'}}$$。利用$$g$$来表示由于视角变化导致的图片歪曲，也就是$$\pmb{x^{'}} \approx \pmb x \circ g$$。利用$$\Phi_S$$，我们可以将$$g$$$分解为：

$$g = \Phi_S(\dot; \pmb{x^{'}}) \circ \Phi_S(\dot; \pmb x)^{-1}$$

等价于：$$\forall q \in S: \Phi_S(p; \pmb x \circ g) = g(\Phi_S(p; \pmb x))$$，这个约束条件表明$$S$$上所检测到的点$$p$$需要随着视角的变化仍然保持一致。

> 上述的$$g$$可以用来描述由于视角、物体本身的deformation以及category-specific image collection里不同个体的shape variance

为了学习这个函数$$\Phi_S$$，作者将这个函数利用一个深度神经网络来表示，并采用一种Siamese的结构，输入是$$(\pmb x, \pmb{x^{'}}, g)$$

> 如果我们任意给定一个物体的两个视角的图片，那么$$g$$一般是不知道的，所以说与其再利用某种方法来估计$$g$$，不如直接随机的构造出$$g$$，再利用$$g$$从$$\pmb x$$上构造出$$\pmb{x^{'}}$$。

$$\Phi_S$$是由神经网络所表示的，可以使用现有的任何一种用于keypoint detection的框架来实现它（比如HRnet，Hourglass等）。网络的输入是图片$$x$$，输出是检测到的keypoints，主要的loss如下：

$$\mathcal L_{align} = \frac{1}{K} \sum_{r=1}^K \lVert \Phi(\pmb x_r \circ g) - g(\Phi(\pmb x_r)) \rVert^2$$

上述的loss会使得网络学习到的keypoints都是consistent的，但并没有阻止网络所学习到的都是同一个点，也就是这些点全都聚集到了一起。为了避免这个现象，作者加了一个diversity loss来使得这些probability maps对应到图片的不同区域。最直接的方式就是直接对两个keypoints的heatmap的相互重合的区域增加惩罚：

$$\mathcal L_{div}(\pmb x) = \frac{1}{K^2} \sum_{r=1} \sum_{r^{'}=1} \sum_u p(u \vert \pmb x, r) p(u \vert \pmb x, r^{'})$$

上述公式的一个缺点就是计算量是quadratic的。一个简便的方法是：

$$\mathcal L_{div}^{'} (\pmb x) = \sum_u (\sum_{r=1}^K p(u \vert \pmb x, r) - max_{r=1, \cdots, K} p(u \vert \pmb x, r))$$

具体实现的时候，对于一张图片$$\pmb x$$和用thin-plate splines（薄板样条插值）TPS实现的transformation $$g_1, g_2$$，网络的输入图片对为：$$g_1 \circ \pmb x$$和$$g_2 \circ (g_1 \circ \pmb x)$$。

> 该文章最大的问题在于，所描述的keypoints在经过deformation之后仍要保持一致，这只能对于很小的deformation来说，因为大的deformation会直接导致keypoints可能不再可见。而该文章最大的贡献在于，如同前面所说，其提出的keypoints在图片经过小的deformation之后仍需要保持consistency这个性质，仍然被很多unsupervised的keypoint甚至structure检测，或者part segmentation论文当作辅助loss来使用。


#### \[**CVPR 2018**\] [Unsupervised Discovery of Object Landmarks as Structural Representataions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)

*Yuting Zhang, Yijie Guo, Yinxin Jin, Yijun Luo, Zhiyuan He, Honglak Lee*

[POST](https://www.ytzhang.net/projects/lmdis-rep/)

本文提出了一个autoencoder的框架来将检测到的landmarks显式的表示为structural representations。encoder输出的是landmarks，其加了限制从而能够输出有效的landmarks信息；而decoder模块以这些landmarks作为输入重构输入图片，从而形成一种end-to-end的结构来学习这些landmarks。

[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)提出了一个无监督学习的方式在突破可能有transformations的情况下来检测稳定的landmarks表示物体的局部语义信息。然而，这个方法并没有显式的鼓励这些landmarks出现在那些可以用作image modeling的位置上。

![usr1]({{ '/assets/images/USR-1.PNG' | relative_url }}){: width=400px style="float:center"}

作者使用一个深度神经网络来将输入图片$$\pmb I$$转换为输出是$$K+1$$个通道的detection confidence map $$\pmb D \in \left[0,1\right]^{W \times H \times (K+1)}$$。这个confidence map会检测$$K$$个landmarks，而第$$K+1$$个通道表示的是背景。$$\pmb D$$的分辨率$$W \times H$$可以和输入图片的分辨率一样，或者更小，但是它们需要是同比例的。

作者提出一个轻量化的hourglass网络，来从输入图片中获取原始的detection score map $$\pmb R$$：

$$\pmb R = hourglass_l (\pmb I; \theta_l) \in \mathbb R^{W \times H \times (K+1)}$$

其中$$\theta_l$$表示的是网络参数。hourglass网络可以让detectors兼顾检测局部特征和利用全局信息。在获得了$$\pmb R$$之后，再在$$\pmb R$$的每个位置都做一次softmax，也就是每个位置沿着所有的通道数做一次softmax（包括表示背景的score map），从而所得到的$$\pmb D$$的每个位置，沿着所有通道的值加起来就是1，$$\pmb D$$里的每个值都在0到1之间：

$$\pmb D_k(u,v) = \frac{exp(\pmb R_k(u,v))}{\Sigma_{k^{'}=1}^{K+1} exp(\pmb R_{k^{'}}(u,v))}$$

其中$$\pmb D_k$$是$$\pmb D$$的第$$k$$个通道。

将$$\pmb D_k$$看作一个加权的map，作者使用加权的均值坐标作为第$$k$$个landmark的坐标，也就是：

$$(x_k, y_k) = \frac{1}{\zeta_k} \sum_{v=1}^H \sum_{u=1}^W (u,v) \cdot \pmb D_k(u,v)$$

其中$$\zeta_k = \sum_{v=1}^H \sum_{u=1}^W \cdot \pmb D_k(u,v)$$是空间归一化因子。将landmark和landmark detector简记为：

$$\pmb l = \left[x_1, y_1, \cdots, x_K, y_K \right]^T = landmark(\pmb I; \theta_l)$$

流程图上半部分（蓝色）的左半部分表示的就是landmark detector。

$$\pmb l$$里的元素应该是所检测到的landmark coordinates，但是到目前为止，并没有限制条件使得它们是landmarks，到现在为止它们只是任意的latent representations。因此，作者提出了下列soft constraints作为正则项来迫使这些所检测到的representations具有landmarks的特性。

* Concentration constraint

作为一个单一location的detection confidence map，$$\pmb D_k$$的值需要集中于一个局部的区域内。$$\pmb D_k / \zeta_k$$可以被认为是一个2维的概率分布，从而可以计算出其沿着$$x$$轴和$$y$$轴的方差，$$\sigma_{det, u}^2, \sigma_{det, v}^2$$。作者定义了如下的constraint loss来鼓励这两个方差都很小：

$$L_{conc} = 2\pi e (\sigma_{det, u}^2 + \sigma_{det, v}^2)^2$$

上述的表示的是以$$(x_k,y_k)$$为中心，以$$(\sigma_{det,u}^2 + \sigma_{det, v}^2)/2 \cdot \mathbb I$$为协方差矩阵的二维高斯分布的熵的exponential。这个高斯分布是$$\pmb D_k / \zeta_k$$的一个近似，$$L_{conc}$$越小，分布就越集中于中心点的位置，越符合要求。

该高斯分布记为：$$\overline{ \pmb{ D_k}}(u,v) = (1 / WH) \mathcal N((u,v); (x_k, y_k), \sigma_{det} \mathbb I)$$

* Separation constraint

理想情况下，autoencoder的目标函数可以自动使得$$K$$个landmarks分布在不同的区域内从而很好地完成decoder里的reconstruction任务。然而，由于随机的初始化，landmarks可能互相靠得很近。这会导致优化过程陷入局部最优点。为了解决这个问题，作者提出了一个显式的loss来空间分隔这些landmarks：

$$L_{sep} = \sum_{k \neq k^{'}}^{1, \cdots, K} exp(\frac{-\lVert (x_{k^{'}}, y_{k^{'}}) - (x_k, y_k) \rVert_2^2}{2 \sigma_{sep}^2})$$

* Equivariance constraint

图片的某一个特定的landmark应该位于一个具有明显局部特征的地方（这个局部特征也应该有明确的语义信息）。这需要landmarks对于image transformations来说具有equivariance的特性。更具体来说，如果相对应的视觉信息仍然存在于transformed之后的图片中的话，一个landmark应该要跟着transformation变化而变化（比如说camera或者object的移动）。$$g$$用来表示这种transformation，从而原image $$\pmb I$$和transformed之后的image $$\pmb I^{'}$$就有着如下的对应关系：$$\pmb I^{'}(u,v) = \pmb I(g(u,v))$$，也就是说，$$g(x_k^{'}, y_k^{'}) = (x_k ,y_k)$$，从而引入如下的限制：

$$L_{eqv} = \sum_{k=1}^K \lvert g(x_k^{'}, y_k^{'}) - (x_k, y_k) \rvert_2^2 $$

如果$$g$$是已知的话，那么这个loss就是定义好了的。和[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)一样，作者使用一个thin plate spline（TPS）来使用随机参数模拟$$g$$。作者使用随机的translation，rotation以及scaling来为TPS确定global affine component；之后再perturb一系列control points来确定TPS的local component。除此之外，如果训练集合是以视频的形式出现的，可以将dense motion flow用作$$g$$，而下一帧就是$$\pmb I^{'}$$。

除了keypoint detector之外，这篇文章的另一个贡献是对于每个keypoint，还align了一个local feature descriptor。

对于简单的图片，比如说MINIST，landmarks就足够描述物体的形状了。但对于大多数实际的图片来说，landmarks并不足以表示所有的视觉内容，所以就需要额外的latent representations来encode补充信息。而一方面，我们不能引入过多的全局信息，因为这样会导致模型不容易学习到landmark的局部信息（因为这样的全局信息就足够decoder完成reconstruction任务了）；而另一方面，我们也需要一定的全局信息来帮助landmarks的定位。为了解决这个trade-off，作者给每个landmark都计算了一个low-dimensional的local descriptor。

作者使用另一个hourglass网络来获取一个feature map $$\pmb F$$，其和detection confidence map $$\pmb D$$的尺寸是一样的：

$$\pmb F = hourglass_f(\pmb I; \theta_f) \in \mathbb R^{W \times H \times S}$$

对于每个landmark来说，作者使用一个average pooling来获取这个landmark的local feature descriptor，其中这个average pooling的权重是由一个中心在这个landmark点的soft mask构成的。具体来说，我们将之前所说的中心点位于$$\[ x_k, y_k \]$$的二维高斯分布$$\overline{ \pmb{ D_k}}$$作为这个soft mask。一个可学习的linear operator（实际上就是几层MLP）被加在每个feature上从而为每个landmark学习到一个独立的feature descriptor（注意到每个landmark都有一个自己的linear operator，不是共享的）。

> 作者认为之前所学习到的feature $$\pmb F$$使得所有keypoint的features都公用一个空间，而此处对于每个landmark使用不同的linear projection将其映射到自己独立的空间里。每个landmark独有的这个linear operator使得每个landmark descriptor可以编码独有的信息

具体来说，第$$k$$个landmark的latent descriptor就是：

$$\pmb f_k = \pmb W_k \sum_{v=1}^H \sum_{u=1}^W (\overline{ \pmb{ D_k}}(u,v) \cdot \pmb F(u,v)) \in \mathbb R^{C}, \quad \text{with} C < S$$

我们还需要获取一个背景的descriptor。但使用上述方法利用一个高斯分布来近似背景的confidence map是不合理的，所以直接令$$\overline{ \pmb{ D_{K+1}}} = \pmb D_{K+1} / \zeta_{K+1}$$。

将所有的landmarks的descriptors和背景的descriptor放在一起，我们就有了$$\pmb f = \left[ \pmb f_1, \pmb f_2, \cdots, \pmb f_{K+1} \right] \in \mathbb R^{C \times (K+1)}$$。流程图的下半部分的左边部分表示了获取这个landmark descriptors $$\pmb f$$的过程。


**Landmark-based decoder**

作者先从所检测到的landmark coordinates恢复detection confidence map $$\tilde{ \pmb{ D}} \in \mathbb R^{W \times H \times (K+1)}$$。具体来说，作者使用一个二维高斯分布来表示每个landmark的confidence map，其中心点是每个landmark的coordinates，其协方差矩阵是$$\sigma_{det}^2 \mathbb I$$，其中$$\sigma_{dec}$$不是之前计算出来的，是一个超参数。

$$\tilde{ \pmb{R_k}}(u,v) = \mathcal N((u,v); (x_k, y_k), \sigma_{dec}^2 \mathbb I), k=1,2,\cdots,K, \tilde{ \pmb{ R_{K+1}}} = \pmb 1 \tag{11}$$

然后再从$$\tilde{ \pmb{ R}}$$中，对于每个位置，沿着所有的通道做一次归一化，从而恢复detection score map $$\tilde{ \pmb{ D}}$$：

$$\tilde{ \pmb{ D_k}}(u,v) = \tilde{ \pmb{ R_k}}(u,v) / \sum_{k=1}^{K+1} \tilde{ \pmb{ R_k}}(u,v) \tag{12}$$

fig1的上半部分的右半部分描述了这个过程。

对于每个landmark的descriptor $$f_k$$（也包括背景的），作者对于每个landmark来说都利用一个landmark-specific linear operator $$\tilde W_k$$将这个descriptor再次进行线性变化，之后加上一个activation function（文中使用的是LeakyReLU），试图将其再次转换到所有feature公用的空间里（也就是之前$$\pmb F$$所代表的空间）。使用$$\tilde D$$作为global unpooling的权重，feature map的恢复过程如下：

$$\tilde {\pmb {F}}(u,v) = \sum_{k=1}^{K+1} \tilde{ \pmb{ D_k}}(u,v) \cdot \tau(\tilde{ \pmb{ W_k}} \pmb f_k) \in \mathbb R^{W \times H \times S} \tag{13}$$

其中$$\tau$$就是activation function。这个过程由fig1下半部分的右半部分所描述。

本文里的方法使得反向传播算法可以通过landmark coordinates进行传播。每个landmark对应的高斯分布的方差$$\sigma_{dec}^2$$决定其周围的邻居可以给这个landmark贡献多少信息。在训练一开始的时候，需要比较大的$$\sigma_{dec}^2$$来使得训练可行，也就是说每个landmark要依靠的邻居点还很多，而随着训练的进行，需要更准确的定位，也就是需要比较小的方差。对于这样两个相互矛盾的需求，作者同时使用了拥有不同$$\sigma_{dec}$$值来获取多个版本的$$\tilde{ \pmb{ D}}$$和$$\tilde{ \pmb{ F}}$$：$$(\tilde{ \pmb{ D_1}}, \tilde{ \pmb{ F_1}}), (\tilde{ \pmb{ D_2}}, \tilde{ \pmb{ F_2}}), \cdots, (\tilde{ \pmb{ D_M}}, \tilde{ \pmb{ D_M}})$$。

最后，将上述这些$$(\tilde{ \pmb{ D_1}}, \tilde{ \pmb{ F_1}}), (\tilde{ \pmb{ D_2}}, \tilde{ \pmb{ F_2}}), \cdots, (\tilde{ \pmb{ D_M}}, \tilde{ \pmb{ D_M}})$$沿着通道这个维度连起来，再输入一个hourglass网络里，最终获取重建的图片：

$$\tilde{ \pmb{ I}} = hourglass_d (\left[\tilde{ \pmb{ D_1}}, \tilde{ \pmb{ F_1}}), (\tilde{ \pmb{ D_2}}, \tilde{ \pmb{ F_2}}), \cdots, (\tilde{ \pmb{ D_M}}, \tilde{ \pmb{ D_M}} \right]; \theta_d) \tag{14}$$

fig1里的最右边的灰色线表示了这个过程。


**Overall training objective**

图片reconstruction loss驱动了整个training的进行，定义这个loss为$$L_{recon} = \lVert \pmb I - \tilde{ \pmb{ I}} \rVert_F^2$$，而且输入的图片$$\pmb I$$是被归一化到0到1之间的。从而，整个网络的loss，$$L_{AE}$$定义为：

$$L_{AE} = \lambda_{recon} L_{recon} + \lambda_{conc} L_{conc} + \lambda_{sep} L_{sep} + \lambda_{eqv} L_{eqv} \tag{15}$$


#### \[**NeurIPS 2022 Spotlight**\] [AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints](https://arxiv.org/pdf/2205.10636.pdf)

[CODE](https://github.com/xingzhehe/AutoLink-Self-supervised-Learning-of-Human-Skeletons-and-Object-Outlines-by-Linking-Keypoints)

这篇文章要解决的问题是无监督的从一类物体的图片里学习到2D keypoints。模型的输入是一类物体（比如人脸）的RGB图片，输出是2D keypoints，数量和顺序是固定的。也就是说，模型在输入图片后，输出$$K$$个heatmap，然后从$$K$$个heatmap里获取$$K$$个2D keypoints。所以不同图片的2D keypoints之间的correspondence也是直接就有的。

很多无监督2D keypoints的方法都是一个auto encoder的结构，但这篇文章的创新点有如下几个：

* 首先，网络除了输出$$K$$个heatmap之外，还会输出一个$$K \times K$$的graph，用来表示没两对keypoints之间的权重，而且这个graph是global的，也就说对于每个输入图片，其是共同的。
* 其次，对于每对输出的keypoints，构建一个edge heatmap，大小和输入图片一样，是以这两个keypoint的连线为中心的Gaussian分布，再利用上面的graph，得到每个像素点位置的global edge heatmap的值
* 在得到这个global edge heatmap之后，还是一样需要一个decoder来reconstruct原输入图片，显然只有这个骨架是没办法还原的，所以文章的做法是对于输入图片，mask掉其绝大部分区域，和这个edge heatmap沿着channel连起来，作为decoder的输入。

这篇文章的方法很简单，却很有效果。

有个要注意的技术细节是，reconstruction loss不仅仅是reconstructed的图片和原图片之间的mse loss，而是perception loss，也就是将这两个图片都输入某个预训练好的网络，比如在ImageNet上预训练好的VGG19，然后对比很多层的输出之间的差异的和。这样做要比仅仅在像素层面比较区别更加鲁棒。


#### \[**CVPR 2024 Highlight**\] [Unsupervised Keypoints from Pretrained Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Hedlin_Unsupervised_Keypoints_from_Pretrained_Diffusion_Models_CVPR_2024_paper.pdf)

*Eric Hedlin, Gopal Sharma, Shweta Mahajan, Xingzhe He, Hossam Isack, Abhishek Kar, Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi*

[CODE](https://github.com/ubc-vision/StableKeypoints)

这篇文章和[Unsupervised Semantic Correspondence Using Stable Diffusion](https://ubc-vision.github.io/LDM_correspondences/)一样，基于的想法是，对于训练好的text-based StableDiffusion模型，即使是random的text embedding，和multi-head encoder得到的image embedding计算相似度再可视化，都能看到图中object大致的形状，并且相似度高的区域具有一定的semantics。

所以本文的想法是，对于预训练好的StableDiffusion，optimize若干个text embedding，使得每个text embedding和image embedding计算完相似度之后（这个相似度matrix就可以理解成一个heatmap），该heatmap是单峰的，这也就意味着该text embedding对应着所有图片的某一固定区域（比如眼睛），如下图所示：

![unsuper1]({{ '/assets/images/diffusion2dkp1.png' | relative_url }}){: width=800px style="float:center"} 

具体来说，假设transformer有$$L$$层，每一层有$$C$$个head，那么每一层就可以得到$$C$$个pixel-level的image embedding：$$F_{I}^c \in \mathbb{R}^{H \times W \times D}$$，其中$$c=1,2,\cdots,C$$，$$H,W$$是图片尺寸，$$D$$是特征维度。对于text-embedding来说，假设有$$N$$个token，这样所有的token的embedding集合是$$F_{T} \in \mathbb{R}^{N \times D}$$。从而就可以计算pixel-level的text和image pixel的相似度：$$M_c \mathbb{R}_{+}^{H \times W \times N}$$。最后的相似度矩阵$$M$$是先将$$M_c$$沿着$$N$$维度作softmax，再对于所有的head进行一个average，最后再在$$L$$层上选出几层出来average。

在得到了相似度矩阵$$M \mathbb{R}_{+}^{H \times W \times N}$$之后，其loss是对于每一个$$M_i \mathbb{R}_{+}^{H \times W \times N}$$，$$i=1,2,\cdots,N$$，先找到该$$M_i$$最大值所在的坐标，以该坐标为中心，在$$H \times W$$的2维grid上构建一个高斯分布（方差是超参数），然后计算$$M_i$$与该高斯分布之间的损失。

有几点值得注意的技术细节：
* 首先，在训练的时候，除了上述的损失函数，还需要加一个transformation损失，也就是对于手动transform之后的输入图片，其得到的相似矩阵，也应该加上同样的transformation（也就是之前那些unsupervised keypoint inference方法里的equivariance loss）
* 其次，因为keypoints存在遮挡的情况，所以对于某些图片来说，可能计算出来的相似矩阵是比较弥散的（即不能对应一个尖峰高斯，因为其本来就检测不到）。为了解决这个问题，作者提出的engineering的方法是先设定一个较大的keypoint值，比如说50，最后选择那些和高斯分布的KL散度最小的相似矩阵对应的那些keypoint作为输出，比如说25。
* 最后，为了实现所得到的keypoints确实能够覆盖object，作者还在上一步所得到的那些keypoint里，继续使用fartest point sampling选择出一个subset作为最终的输出，比如说15。

和autolink相比，本文的方法对于pose比较大的情况，确实能够正确识别，比如下图：

![unsuper1]({{ '/assets/images/diffusion2dkp2.png' | relative_url }}){: width=800px style="float:center"} 

但是对于某些情况，比如说人体，该方法仍然不能正确的区分正反面（可能是因为人脸这种用于区分正反面的特征在图片里的区域太小了，导致特征信息不够）：

![unsuper1]({{ '/assets/images/diffusion2dkp3.png' | relative_url }}){: width=800px style="float:center"} 


## 3D Keypoint Detection from Images

### Supervised 3D Keypoint Detection
### Unsupervised 3D Keypoint Detection

## 3D Keypoint Detection from Point Clouds
