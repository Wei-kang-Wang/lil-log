---
layout: post
comments: True
title: "关键点检测"
date: 2024-07-30 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 2D Keypoint Detection from Images

### Supervised 2D keypoint Detection

#### \[**ECCV 2016**\] [Stacked hourglass networks for human pose estimation](https://arxiv.org/pdf/1603.06937.pdf)

*Alejandro Newell, Kaiyu Yang, Jia Deng*

[CODE](http://www-personal.umich.edu/~alnewell/pose)

![hourglass0]({{ '/assets/images/HOURGLASS-0.PNG' | relative_url }}){: width=400px style="float:center"} 

这篇文章是2D keypoints detection的经典之一（因为它的Hourglass网络结构）。

Hourglass网络的设计受启发于获取各个尺度的信息的需求。尽管局部信息对于识别比如faces或者hands等局部物体来说很重要，但最终的pose estimation需要对整个human body有一个全面的理解。人的orientations，四肢的arrangements，以及相邻关节的关系等等，这些信息都需要同一张图片不同尺度的信息才能良好的获得。Hourglass网络能够将各个尺度的信息综合起来从而输出pixel-wise的预测。
这样的网络必须要有某些机制来有效的处理和合并不同尺度的特征。之前有些工作使用独立的不同的网络来处理不同分辨率的图片输入，再将这些输入综合起来处理。但Hourglass network使用的是一个单一的pipeline，使用skip connections来在各个分辨率下保存空间信息。

Hourglass网络的结构如下：卷积层和max pooling层将输入图片的分辨率降低到一个很低的值。在每个max pooling层，在做max pooling之前，输入分叉为两部分，不进入max pooling的那部分会做更多的卷积操作，留着之后使用。最后在达到了最低分辨率之后，网络开始进行upsampling以及结合之前不同分辨率下的features的操作。为了将两个相邻分辨率的特征结合起来，作者对较低分辨率的feature map使用了最近邻upsampling方法，再加上较高分辨率的那个feature map，从而得到输出。hourglass网络的结构是对称的，之前每用max pooling降低分辨率一次，之后就使用upsampling提升分辨率一次，所以最终的分辨率会和输入图片分辨率一样。在之后，再加上两个卷积大小为$$1 \times 1$$的卷积层，从而得到网络最终的输出。网络的最终输出是一系列的heatmaps，每个heatmap表示的是每个keypoint在图片中每个像素点出现的概率值的大小。

初始输入的图片分辨率为$$256 \times 256$$，为了节约计算成本，作者在将图片输入hourglass模块之前，先通过了一个带有残差链接的$$stride=2$$，$$padding=3$$，卷积核为7的卷积层，和一个max pooling层，从而分辨率变为$$64 \times 64$$，之后在进入上述所说的hourglass模块里。对于hourglass里所有层输出的feature maps，其通道数都是256。

本文提出的最终的网络结果是多个hourglass模块堆叠而成，前一个hourglass模块的输出是后一个hourglass模块的输入。这样的结构使得网络可以重新衡量初始的估计值，改进效果。而且使用这种结构，网络中间某个hourglass模块的输入也可以拿来计算loss，具体做法是，对于中间某个hourglass模块的输出，其经过$$1 \times 1$$卷积得到heatmap输出，从而可以拿来被计算loss，而这个heatmaps再经过$$1 \times 1$$的卷积层回到操作之前的通道数，再和这个hourglass的输出加起来，作为下一个hourglass模块的输入。每个hourglass模块的权重并不共享，而且每个hourglass拿来计算的loss使用的是同一种loss以及同一个ground truth。

本文的evaluation是根据标准的Percentage of Correct Keypoints（PCK）metric来计算的，给定一个normalized的distance，落在ground truth这个distance以内的都算判断正确，而PCK metric计算的是判断正确的keypoints占的比例。本文使用了FLIC和MPII两个数据集来测试效果，对于FLIC数据集，这个distance是利用躯干的大小进行了normalize，而对于MPII数据集，这个distance是根据头的大小进行了normalize。

> 这篇文章并不能解决多人的问题，对于多人的情况，只会考虑最靠近图片中心的那个。如果要看多人的算法，可以去看OpenPose那篇论文。

> 本文的方法不能检测到被遮挡住的keypoints，这仍然是一个需要被解决的问题。


#### \[**CVPR 2017** $$\&$$  **TPAMI 2019**\] [OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)
*Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh*

[post](https://github.com/CMU-Perceptual-Computing-Lab/openpose)

OpenPose是一个成熟的可以进行multi-person的2D关键点检测的算法（已商业化）

对于单人的pose estimation有很多论文已经做得很好了，但对于多人来说，有以下几个困难的地方：1）首先，我们并不知道图里到底有几个人，而且每个人所在的位置、大小都不清楚；2）其次，人与人之间可能存在干涉，比如说遮挡、关节的旋转等等，很难分清楚到底哪部分属于哪一个人；3）以往的论文里的算法的复杂度都会随着图里人数量的增加而增加，从而很难实现实时。

对于multi-person 2D pose estimation，top-down的方式很常见，也就是先检测图片里有几个人，然后对每个人实行pose estimation，因为单人的pose estimation已经做得很好了，这个算法并不复杂。但它存在着两个很大的问题：首先如果一开始检测人的时候就检测错了或者遗漏了，那之后是没有补救办法的；其次，这样的方法需要对检测出来的每个人都做单人pose estimation，这会使得算法的复杂度和人的数量成正比。所以说，bottom-up的方法也被提了出来，这种方法有能够解决上述两个问题的潜力。但之前的bottom-up方法仍然效率不高，因为它们在最后还是需要利用全局信息来辅助判断，从而要花不少时间。

这篇文章里利用Part Affinity Field实现了实时的multi-person 2D pose estimation。Part Affinity Field是一个2D向量的集合，表示的是四肢的位置和方向信息。利用bottom-up的方式，将detection和association结合起来（模型有两个主要部分，一个是PAF refinement，另一个是body part prediction refinement，而PAF就是association）逐步推进，可以在利用很小的计算资源的情况下达到很好的效果。

![overview]({{ '/assets/images/OPENPOSE-1.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 1. 算法流程。输入是一张大小为$$w \times h$$的RGB图片，而输出为图中每个人的生理结构上的2D keypoints的位置。首先，一个feedforward网络输出一个集合$$S = (S_1, S_2, ..., S_J)$$有J个confidence maps，每个对应一个身体部位，其中$$S_j \in R^{w \times h}$$，$$j = \lbrace 1,2,...,J \rbrace$$，用来表示各个身体部位位置的2D confidence maps，和一个part affinity fields（也就是2D的向量）的集合$$L= (L_1, L_2, ..., L_C)$$，每个$$L_c \in R^{w \times h \times 2}$$对应一个肢体，$$c \in \lbrace 1,...,C \rbrace$$，用来表示各个身体部位之间的从属程度。最终，confidence maps和PAFs（也就是集合S和集合L）通过greedy inference联系了起来用于输出图中所有人的2D keypoints位置。*

![algorithm]({{ '/assets/images/OPENPOSE-2.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 2. 上方：多人pose estimation。同一个人的身体部分被连了起来，也包括了脚的keypoints（大脚趾，小脚趾和脚后跟）。下左：关于连接右手肘和手腕的肢体的PAFs。颜色表明了方向。下右：在关于连接右手肘和手腕的肢体的PAFs的每个像素点处的2D向量包含了肢体的位置和方向信息。*

![architecture]({{ '/assets/images/OPENPOSE-3.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 3. multi-stage CNN的一个stage。stage的前一部分用来预测PAFs $$L^t$$, 而后一部分用来预测confidence maps $$S^t$$。每个stage的输出和图片feature连接起来，再作为下一个stage的输入。注意每个stage的CNN并不共享参数。*

![PAF]({{ '/assets/images/OPENPOSE-4.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 4. 右前小臂肢体的PAF。尽管在一开始的stage里还有一些不清晰，但在之后的stage里可以看到PAF很清晰。*

对于PAF的stage $$t_i$$和confidence map的stage $$t_k$$的loss function是：

$$ \mathcal{L}_{P}^{t_i} = \Sigma_{c=1}^C \Sigma_p W(p) ||L_c^{t_i}(p) - L_c^{\ast}(p)||^2_2 $$

$$ \mathcal{L}_{S}^{t_k} = \Sigma_{j=1}^J \Sigma_p W(p) ||S_j^{t_k}(p) - S_j^{\ast}(p)||^2_2 $$

其中$$L_c^{\ast}$$是第$$c$$个肢体的PAF的ground truth，$$S_j^{\ast}$$是第$$j$$个keypoint的confidence map的ground truth，上述两个求和都是对所有像素进行的，$$p$$指的就是像素点。$$W(p)$$如果某个位置没有标注就是0，有标注就是1。这个$$W$$是用来避免因为没有标注而导致的错误训练。我们在每个stage都使用loss function，用来解决梯度消失的问题，因为每个stage结尾都有loss function，对梯度的值进行了补充。从而整体的的loss function就是：

$$ \mathcal{L} = \Sigma_{t=1}^{T_p} \mathcal{L}_{P}^{t} + \Sigma_{t=T_p + 1}^{T_p + T_C} \mathcal{L}^t $$


对于每个人$$k$$生成confidence map，$$S_{j,k}^{\ast}$$。$$x_{j,k} \in R^2$$表示人$$k$$的身体部分$$j$$在图中位置的ground truth。从而$$S_{j,k}^{\ast}的位置$$p \in R^2$$处的值就是：

$$ S_{j,k}^{\ast}(p) = exp(-||p-x_{j,k}||^2_2 / \sigma^2) $$

其中$$\sigma$$控制峰的大小。从而整个图片（可能包含多个人）的ground truth就是：

$$ S_j^{\ast}(p) = max_k S_{j,k}^{\ast}(p) $$


![cal]({{ '/assets/images/OPENPOSE-5.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 5. 身体部位association策略。（a) 几个人的两种身体部分（也就是keypoint）分别用红蓝点表示，而所有的点之间都连上了线。（b) 利用中间点进行连线。黑线是正确的，绿线是错误的，但他们都满足连接了一个中间点。(c) 利用PAF来连接，黄色的箭头就是PAF的结果。利用肢体来表示keypoint的位置和keypoint之间的方向信息，PAF减少了错误association的可能性。.*

给定一些已经检测到了的body parts（Fig 5里的红色和蓝色的点），那我们该如何将它们组合起来从而构建未知数量的人的肢体呢？我们需要对每一对body part keypoints都有一个confidence measure，也就是说，measure它们是否属于同一个人。一种可能的方式就是检测这一对keypoints的中间是否还有附加的midpoint。但是当人聚集在一起的时候，很容易出错。这种方式之所以不好是因为1）它仅仅有位置信息，并没有一对keypoint之间的方向信息；2）它仅仅用了midpoint，而不是这两个keypoints之间的所有部分当成一个肢体来使用。

Part Affinity Fieds (PAFs)解决了这些问题。它对于每一对keypoints构成的肢体提供了位置和方向信息。每一个PAF都是一个2D的vector field，在Fig 2里有显示。对于每个肢体的PAF的每个位置的值，其都是一个2D的向量，包含了位置和这个肢体一个keypoint指向另一个keypoint的方向。每个类别的肢体都有一个对应的PAF（由对应的body part keypoints对组成）。

![fig]({{ '/assets/images/OPENPOSE-6.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}

考虑fig 6所示的一个简单的肢体。$$x_{j_1, k}$$和$$x_{j_2, k}$$是人$$k$$的身体部位$$j_1$$和$$j_2$$的ground truth，而这两个部位组成了肢体$$c$$。对于肢体$$c$$上的一点$$p$$，$$L_{c,k}^{\ast}(p)$$是一个单位向量，从$$j_1$$指向$$j_2$$；对于其它的点，$$L_{c,k}^{\ast}(p)$$的值都是0。

为了能在训练过程中计算$$f_L$$的值，我们需要定义PAF的ground truth，也就是对于人$$k$$，$$L_{c,k}^{\ast}$$在$$p$$点的值为$$L_{c,k}^{\ast}(p) = v$$ if $$p$$ on limb $$c, k$$ and $$0$$ otherwise。

这里

$$v = (x_{j_2, k} - x_{j_1, l}) / ||x_{j_2, k} - x_{j_1, l}||$$

是肢体的有方向的单位向量。一个肢体上的点不仅仅只有两个keypoints连线上的，而是有一个距离阈值，比如说：

$$ 0 \leq v (p - x_{j_1,k}) \leq l_{c,k}$$ 和 $$|v_{verticle} (p - x_{j_1,k})| \leq \sigma_l$$

其中肢体宽度$$\sigma_l$$自定义的，肢体长度由两个keypoints决定，也就是

$$l_{c,k} = ||x_{j_2, k} - x_{j_1, k}||$$

$$v_{verticle}$$是垂直于$$v$$的。

而整个图片的PAF的ground truth是对于所有人取了均值：

$$ L_c^{\ast}(p) = 1/n_c(p) \Sigma_k L_{c,k}^{\ast}(p) $$

在测试过程中，我们通过计算连接两个keypoints的线段间的PAF的积分来衡量这两个keypoints是否构成了一个肢体。对于两个身体部分$$d_{j_1}$$和$$d_{j_2}$$，我们计算：

$$ E = \int_{u=0}^{u=1} L_c(p(u)) (d_{j_2} - d_{j_1})/||d_{j_2} - d_{j_1}|| du $$

其中$$p(u) = (1-u) d_{j_1} + u d_{j_2}$$。在实践中，我们通过等距离采样来近似这个积分值。


对于每个身体部位keypoint的location，我们都有好几个备选的值，这是因为图中有多个人或者因为计算错误。而这些keypoints组成的肢体就会有很多种可能了。我们用5.4里定义的积分来计算每个肢体的积分值。从而问题变成了，如何在众多的有着不同积分值（也就是score）的肢体集合中，选择合适的肢体并将其正确连接起来，而这是个NP-hard的问题，如fig 7所示。

![fig]({{ '/assets/images/OPENPOSE-7.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 7. Graph matching。(a) 原始的图片，已经有了身体部位keypoint标注了。(b) K-partite graph。(c) 树状结构。(d) 二分图。*

在这篇文章里，我们使用一种greedy relaxation的方法，持续性的产生高质量的匹配。我们猜测这种方法有效的原因是上述计算的积分值（也就是每个肢体的score）潜在的含有global信息，因为PAF的框架具有较大的感受野。

具体来说，首先，我们获得整张图片身体部分keypoint的集合，$$D_J$$，其中$$D_J = \lbrace d_{j_m}: j \in \lbrace 1, ..., J \rbrace, m \in \lbrace 1, ..., N_j \rbrace \rbrace$$，$$N_j$$是身体部位$$j$$的候选数量，而$$d_j^m \in R^2$$是身体部位$$j$$的第$$m$$个候选位置。我们需要将每个身体部位keypoint连接到属于同一个人的同一个肢体的其它身体部位keypoint上，也就是说，我们还需要找到正确的肢体。我们定义$$z_{j_1, j_2}^{m,n} \in \{0, 1\}$$来表示两个身体部位keypoint的候选，$$d_{j_1}^m$$和$$d_{j_2}^n$$是否连在一起，我们的目标是为$$Z = \{z_{j_1, j_2}^{m,n}: j_1, j_2 \in \{1, ..., J\}, m \in \{1, ..., N_{j_1}\}, n \in \{1, ..., N_{j_2}\}$$找到最优的值。

如果我们考虑一个特定的keypoint的pair $$j_1$$和$$j_2$$（比如说neck和right-hip），叫做c-肢体，而我们的目标是：

$$ \max\limits_{Z_c} E_c = \max\limits_{Z_c} \Sigma_{m \in D_{j_1}} \Sigma_{n \in D_{j_2}} E_{m,n} z_{j_1, j_2}^{m,n}$$

$$s.t., \forall m \in D_{j_1}, \Sigma_{n \in D_{j_2}} z_{j_1, j_2}^{m, n} \leq 1$$

$$ \forall n \in D_{j_2}, \Sigma_{m \in D_{j_1}} z_{j_1, j_2}^{m,n} \leq 1$$

其中，$$E_c$$是所有的c-肢体的积分值的和（可能有多个c-肢体，因为可能有多个人），$$Z_c$$是$$Z$$的只关于c-肢体的子集，$$E_{m,n}$$是keypoint $$d_{j_1}^m$$和$$d_{j_2}^n$$之间的定义的积分值，上述要优化的目标的条件，使得我们所学习到的结果里不会有两个肢体公用同一个keypoint。我们可以用Hungarian算法来获取上述优化的结果。

现在我们考虑所有的肢体，那么上述优化的式子即是需要考虑整个$$Z$$并且需要计算所有肢体的所有可能结果，计算$$Z$$是一个K-维的匹配问题（K是肢体的数量）。这个问题是个NP-hard的问题，有很多relaxations的算法存在。在我们这篇论文中，我们添加了两个relaxation。首先，完整的图会对于每两个不同类别的keypoint都有edge，而我们将这个图简化为其能表示人的pose的spanning tree就可以，而多余的edge就不要了。其次，我们将上述K-维的匹配问题解构为一系列二分匹配的子问题并且独立的解决这些问题，所利用的就是每个spanning tree的相邻的两个node所对应的值以及它们之间的连线，所以说是独立的。第二个relaxation之所以可行，直觉上来说，spanning tree里相邻的两个node之间的关系是由PAF网络学习到的，而非相邻的两个node之间的关系是由CNN网络学习到的。

有了上述两个relaxations，我们的问题被简化为：

$$ \max\limits_{Z} E = \Sigma_{c=1}^C \max\limits_{Z_c} E_c $$

从而我们将这个优化问题分解为独立的每个pair的优化问题，而这个在之前所述，可以用Hungarian算法解决。我们再将有共同keypoint的肢体联合起来，这样其就表示出了一个完整的人的pose，或者说骨架。我们的第一个relaxation，将完整的图简化为spanning tree使得整个算法获得了很大程度的加快。

我们目前的模型仍然有多余的PAF连接（比如说耳朵和肩膀的连接，手腕和肩膀的连接等）。这样冗余的连接使得我们的算法对于人群很密集的时候准确度较高。对于冗余的PAF连接，也就是有冗余的肢体，我们在parsing算法进行一些简单的修改就行。


### Unsupervised 2D Keypoint Detection

#### \[**ICCV 2017**\] [Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)

[CODE](https://github.com/alldbi/Factorized-Spatial-Embeddings)

方法基于factorizing image deformations，这种deformation可能是由viewpoint变化引起的，或者是因为物体本身的形变引起的。作者提出的方法是认为deformation前后的图片的keypoints具有consistency的性质。作者还表明，用这种方法所学习到的keypoints，即使不需要加上限制条件使得同一个类别的不同图片（也就是不同物体）之间的keypoints具有对应关系，它们就已经自动对应上了。

![fse1]({{ '/assets/images/fse1.png' | relative_url }}){: width=400px style="float:center"}
*fig1. 作者提出了一个新的方法，可以在没有任何标注的情况下学习视角无关的keypoints。这个方法使用了一个viewpoint factorization的过程，其学习了一个可以进行image deformation的keypoint detector。这个模型可以用来学习rigid或者deformable objects。*

$$S \subset \mathbb R^3$$表示一个物体的3维surface，比如说一只鸟，$$\pmb x: \Lambda \rightarrow \mathbb R$$表示的是这个物体的一张图片，其中$$\Lambda \subset \mathbb R^2$$是image domain，如fig2所示。$$S$$是物体的一个固有的特性，和$$\pmb x$$是无关的。作者考虑的是学习一个函数$$q = \Phi_S(p; \pmb x)$$将$$S$$上的点$$p \in S$$映射到对应的图片$$\pmb x$$上的某个点$$q \in \Lambda$$上去。

![fse2]({{ '/assets/images/fse2.png' | relative_url }}){: width=400px style="float:center"}
*fig2. 物体结构的建模过程。。*

作者提出了一个利用viewpoint factorization来自动学习$$\Phi_S$$的方法。为了实现这个目标，我们考虑从另一个角度来看这个物体从而获得的另一张图片$$\pmb{x^{'}}$$。利用$$g$$来表示由于视角变化导致的图片歪曲，从而$$\pmb{x^{'}} \approx \pmb x \circ g$$。利用$$\Phi_S$$，我们可以将$$g$$$分解为：

$$g = \Phi_S(\dot; \pmb{x^{'}}) \circ \Phi_S(\dot; \pmb x)^{-1} \tag{1}$$

也就是说，我们将这个歪曲$$g: q \rightarrow q^{'}$$分解为先找到图片$$\pmb x$$里的点$$q$$对应在$$S$$上的点$$p=\Phi_S^{-1}(q ;\pmb x)$$，然后再找到这个点$$p$$在图片$$\pmb{x^{'}}$$上对应的点$$q^{'} = \Phi_S(p;\pmb{x^{'}})$$。

公式1表示的factorization也可以用下述等价的约束条件来表示：

$$\forall q \in S: \Phi_S(p; \pmb x \circ g) = g(\Phi_S(p; \pmb x)) \tag{2}$$

这个约束条件表明$$S$$上所检测到的点$$p$$需要随着视角的变化仍然保持一致。

为了学习这个函数$$\Phi_S$$，作者将这个函数利用一个深度神经网络来表示，并采用一种Siamese的结构，输入是$$(\pmb x, \pmb{x^{'}}, g)$$。注意到，如果我们任意给定一个物体的两个视角的图片，那么$$g$$一般是不知道的，所以说与其再利用某种方法来估计$$g$$，不如直接随机的构造出$$g$$，再利用$$g$$从$$\pmb x$$上构造出$$\pmb{x^{'}}$$。

>尽管训练的过程使用的是同一张图片和这张图片经过transformation之后得到的歪曲的图片作为训练图片对，但是训练好的模型仍然能够在同一种类物体的不同图片之间学习到keypoints的对应关系。对于那些角度变化特别大的情况，则不能保证效果，这就是future works了。


上述描述的是rigid物体的建模，也就是说$$S$$是不变的，而其也可以直接拓展到deformable的物体上去。假设$$S$$经过了某些deformation $$w$: \mathbb R^3 \rightarrow \mathbb R^3$$。从而形变后的surface就是$$S_{'} = \lbrace w(p):p \in S \rbrace$$。作者同时还提出了一个共用的surface $$S_0$$，其表示的是这个类别的object的一个标准的surface，也就是说其它的surface都可以从这个surface变形而来。将从$$S_0$$到$$S$$的deformation记为$$\pi_S$$，那么对于$$S_0$$上任意一个点$$r$$，就有$$\pi_S(r) \in S$$，而且$$\forall w: w(\pi_S(r)) = \pi_{S^{'}}(r)$$。然后，对于$$S_0$$上的点$$r$$到图片$$\pmb x$$上的点$$q$$的映射$$\Phi(r, \pmb x)$$，可以用$$\Phi_S$$来表示：$$\Phi(r; \pmb x) = \Phi_S(\pi_S(r); \pmb x)$$。从而公式2就写成：

$$\forall r \in S_0: \Phi(r; \pmb x \circ g) = g(\Phi(r; \pmb x)) \tag{3}$$

这也就是说即使是由形变造成的物体变化，变化前后所检测到的物体的keypoints也是要对应的。


除了物体的形变，上述的构造对于同一个类别的不同物体之间的差异也是可以解释的。为了实现这种解释，只需要做出一个假设：同一个种类的所有物体的surfaces对于一个category-specific surface $$S_0$$来说，都是isomorphic的，如fig2所示。

和derformable object的情况不一样，几何性质（也就是之前说的$$g$$）是不足以使得从$$S_0$$映射到不同object $$S$$的映射$$\pi_S$$能够相关联的（因为这个时候$$g$$已经无法简单的定义出来了），从而不同图片的keypoints之间的对应关系就不好建立了。然而，我们还希望这些这些$$\pi_S$$具有semantically consistent的性质，也就是还存在对应关系，比如说$$\pi_S(r)$$表示的是图片$$S$$里右眼的位置，而$$\pi_{S^{'}}(r)$$表示的也是图片$$S^{'}$$里右眼的位置。本文的一个重要的贡献就在于，如果将模型按照3.1里所说的那样进行训练，那么训练好的模型对于一个种类的不同图片，能够自动检测到semantically consistent的keypoints。


我们需要确定如何将从$$S_0$$映射到图片$$\Lambda$$的函数$$\Phi(\dot; \pmb x)$$表示为一个神经网络的输出。作者的方法是在$$K$$个离散的点采样来表示：$$\Phi(\pmb x) = (\Phi(r_1; \pmb x), \cdots, \Phi(r_K; \pmb x))$$。在这样的设定下，函数$$\Phi(\pmb x)$$就可以被理解为检测的是图片$$\pmb x$$的$$K$$个keypoints，$$p_k = \Phi(r_k; \pmb x)$$。作者并没有对这些keypoints加以别的约束。

如果$$\Phi$$是由一个神经网络所表示的，我们可以使用现有的任何一种用于keypoint detection的框架来实现它（比如HRnet，Hourglass等）。绝大多数这种框架都是预测的score maps $$\Psi(\pmb x) \in \mathbb R^{H \times W \times K}$$，对于每个keypoint $$r_k$$和图片的像素点位置$$u \in \lbrace 1, \cdots, H \rbrace \times \lbrace 1, \cdots, W \rbrace \subset \mathbb R^2$$，$$\Psi_{u r_k}(\pmb x)$$表示了一个得分。之后，可以对于每个通道，将每个score map利用softmax进行处理。从而得到了概率分布的score map。再之后，就可以利用求均值的方式求出每个keypoints的位置了。

主要的loss如下：

$$\mathcal L_{align} = \frac{1}{K} \sum_{r=1}^K \lVert \Phi(\pmb x \circ g) - g(\Phi(\pmb x)) \rVert^2 \tag{4}$$

上述的loss会使得网络学习到的keypoints都是consistent的，但并没有阻止网络所学习到的都是同一个点，也就是这些点全都聚集到了一起。

为了避免这个现象，作者加了一个diversity loss来使得这些probability maps对应到图片的不同区域。最直接的方式就是直接对两个probability相互重合的区域增加惩罚：

$$\mathcal L_{div}(\pmb x) = \frac{1}{K^2} \sum_{r=1} \sum_{r^{'}=1} \sum_u p(u \vert \pmb x, r) p(u \vert \pmb x, r^{'})$$

上述公式的一个缺点就是计算量是quadratic的。一个简便的方法是：

$$\mathcal L_{div}^{'} (\pmb x) = \sum_u (\sum_{r=1}^K p(u \vert \pmb x, r) - max_{r=1, \cdots, K} p(u \vert \pmb x, r))$$

对于一张图片$$\pmb x$$和用thin-plate splines（薄板样条插值）TPS实现的transformation $$g_1, g_2$$，网络的输入图片对为：$$g_1 \circ \pmb x$$和$$g_2 \circ (g_1 \circ \pmb x)$$。



## 3D Keypoint Detection from Images

### Supervised 3D Keypoint Detection
### Unsupervised 3D Keypoint Detection

## 3D Keypoint Detection from Point Clouds
