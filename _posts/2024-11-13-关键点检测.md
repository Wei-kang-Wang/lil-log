---
layout: post
comments: True
title: "关键点检测"
date: 2024-07-30 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 2D Keypoint Detection from Images

### Supervised 2D keypoint Detection

#### \[**ECCV 2016**\] [Stacked hourglass networks for human pose estimation](https://arxiv.org/pdf/1603.06937.pdf)

*Alejandro Newell, Kaiyu Yang, Jia Deng*

[CODE](http://www-personal.umich.edu/~alnewell/pose)

![hourglass0]({{ '/assets/images/HOURGLASS-0.PNG' | relative_url }}){: width=200px style="float:center"} 

这篇文章是2D keypoints detection的经典之一（因为它的Hourglass网络结构）。

Hourglass网络的设计受启发于获取各个尺度的信息的需求。尽管局部信息对于识别比如faces或者hands等局部物体来说很重要，但最终的pose estimation需要对整个human body有一个全面的理解。人的orientations，四肢的arrangements，以及相邻关节的关系等等，这些信息都需要同一张图片不同尺度的信息才能良好的获得。Hourglass网络能够将各个尺度的信息综合起来从而输出pixel-wise的预测。
这样的网络必须要有某些机制来有效的处理和合并不同尺度的特征。之前有些工作使用独立的不同的网络来处理不同分辨率的图片输入，再将这些输入综合起来处理。但Hourglass network使用的是一个单一的pipeline，使用skip connections来在各个分辨率下保存空间信息。

Hourglass网络的结构如下：卷积层和max pooling层将输入图片的分辨率降低到一个很低的值。在每个max pooling层，在做max pooling之前，输入分叉为两部分，不进入max pooling的那部分会做更多的卷积操作，留着之后使用。最后在达到了最低分辨率之后，网络开始进行upsampling以及结合之前不同分辨率下的features的操作。为了将两个相邻分辨率的特征结合起来，作者对较低分辨率的feature map使用了最近邻upsampling方法，再加上较高分辨率的那个feature map，从而得到输出。hourglass网络的结构是对称的，之前每用max pooling降低分辨率一次，之后就使用upsampling提升分辨率一次，所以最终的分辨率会和输入图片分辨率一样。在之后，再加上两个卷积大小为$$1 \times 1$$的卷积层，从而得到网络最终的输出。网络的最终输出是一系列的heatmaps，每个heatmap表示的是每个keypoint在图片中每个像素点出现的概率值的大小。

初始输入的图片分辨率为$$256 \times 256$$，为了节约计算成本，作者在将图片输入hourglass模块之前，先通过了一个带有残差链接的$$stride=2$$，$$padding=3$$，卷积核为7的卷积层，和一个max pooling层，从而分辨率变为$$64 \times 64$$，之后在进入上述所说的hourglass模块里。对于hourglass里所有层输出的feature maps，其通道数都是256。

本文提出的最终的网络结果是多个hourglass模块堆叠而成，前一个hourglass模块的输出是后一个hourglass模块的输入。这样的结构使得网络可以重新衡量初始的估计值，改进效果。而且使用这种结构，网络中间某个hourglass模块的输入也可以拿来计算loss，具体做法是，对于中间某个hourglass模块的输出，其经过$$1 \times 1$$卷积得到heatmap输出，从而可以拿来被计算loss，而这个heatmaps再经过$$1 \times 1$$的卷积层回到操作之前的通道数，再和这个hourglass的输出加起来，作为下一个hourglass模块的输入。每个hourglass模块的权重并不共享，而且每个hourglass拿来计算的loss使用的是同一种loss以及同一个ground truth。

本文的evaluation是根据标准的Percentage of Correct Keypoints（PCK）metric来计算的，给定一个normalized的distance，落在ground truth这个distance以内的都算判断正确，而PCK metric计算的是判断正确的keypoints占的比例。本文使用了FLIC和MPII两个数据集来测试效果，对于FLIC数据集，这个distance是利用躯干的大小进行了normalize，而对于MPII数据集，这个distance是根据头的大小进行了normalize。

> 这篇文章并不能解决多人的问题，对于多人的情况，只会考虑最靠近图片中心的那个。如果要看多人的算法，可以去看OpenPose那篇论文。

> 本文的方法不能检测到被遮挡住的keypoints，这仍然是一个需要被解决的问题。


#### \[**CVPR 2017** $$\&$$  **TPAMI 2019**\] [OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)
*Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh*

[post](https://github.com/CMU-Perceptual-Computing-Lab/openpose)

OpenPose是一个成熟的可以进行multi-person的2D关键点检测的算法（已商业化）

对于单人的pose estimation有很多论文已经做得很好了，但对于多人来说，有以下几个困难的地方：1）首先，我们并不知道图里到底有几个人，而且每个人所在的位置、大小都不清楚；2）其次，人与人之间可能存在干涉，比如说遮挡、关节的旋转等等，很难分清楚到底哪部分属于哪一个人；3）以往的论文里的算法的复杂度都会随着图里人数量的增加而增加，从而很难实现实时。

对于multi-person 2D pose estimation，top-down的方式很常见，也就是先检测图片里有几个人，然后对每个人实行pose estimation，因为单人的pose estimation已经做得很好了，这个算法并不复杂。但它存在着两个很大的问题：首先如果一开始检测人的时候就检测错了或者遗漏了，那之后是没有补救办法的；其次，这样的方法需要对检测出来的每个人都做单人pose estimation，这会使得算法的复杂度和人的数量成正比。所以说，bottom-up的方法也被提了出来，这种方法有能够解决上述两个问题的潜力。但之前的bottom-up方法仍然效率不高，因为它们在最后还是需要利用全局信息来辅助判断，从而要花不少时间。

这篇文章里利用Part Affinity Field实现了实时的multi-person 2D pose estimation。Part Affinity Field是一个2D向量的集合，表示的是四肢的位置和方向信息。利用bottom-up的方式，将detection和association结合起来（模型有两个主要部分，一个是PAF refinement，另一个是body part prediction refinement，而PAF就是association）逐步推进，可以在利用很小的计算资源的情况下达到很好的效果。

![overview]({{ '/assets/images/OPENPOSE-1.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 1. 算法流程。输入是一张大小为$$w \times h$$的RGB图片，而输出为图中每个人的生理结构上的2D keypoints的位置。首先，一个feedforward网络输出一个集合$$S = (S_1, S_2, ..., S_J)$$有J个confidence maps，每个对应一个身体部位，其中$$S_j \in R^{w \times h}$$，$$j = \lbrace 1,2,...,J \rbrace$$，用来表示各个身体部位位置的2D confidence maps，和一个part affinity fields（也就是2D的向量）的集合$$L= (L_1, L_2, ..., L_C)$$，每个$$L_c \in R^{w \times h \times 2}$$对应一个肢体，$$c \in \lbrace 1,...,C \rbrace$$，用来表示各个身体部位之间的从属程度。最终，confidence maps和PAFs（也就是集合S和集合L）通过greedy inference联系了起来用于输出图中所有人的2D keypoints位置。*

![architecture]({{ '/assets/images/OPENPOSE-3.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 3. OpenPose里的stages。上图左边部分用来预测PAFs $$L^t$$, 而右侧部分用来预测confidence maps $$S^t$$。每个stage的输出和图片feature连接起来，再作为下一个stage的输入。*

> 注意每个stage的CNN并不共享参数
> OpenPose有两个版本：TPAMI版本和CVPR版本，在CVPR版本里，每个stage都会同时输出PAF和confidence maps，然后结合图片features作为下个stage的输入。而在TPAMI版本里，前$$T_P$$个stages仅仅输出PAF，而后$$T_C$$个stages仅输出confidence map，这样可以大大减少计算量。这样的改进是因为作者通过实验注意到，减少每个stage的输出，并不会太多影响实验结果，而且需要将PAF的stages放在前面。这也符合intuition，因为如果给定一张图片的PAF，很容易猜到每个keypoint在哪里，但如果只是给一系列的keypoints，不仅无法知道单个人的keypoints怎么连接，更不知道keypoints都分别是属于哪个人的。

对于PAF的stage $$t_i$$和confidence map的stage $$t_k$$的loss function是：

$$ \mathcal{L}_{P}^{t_i} = \Sigma_{c=1}^C \Sigma_p W(p) \lvert L_c^{t_i}(p) - L_c^{\ast}(p) \rvert^2_2, \mathcal{L}_{C}^{t_k} = \Sigma_{j=1}^J \Sigma_p W(p) \lvert S_j^{t_k}(p) - S_j^{\ast}(p) \rvert^2_2 $$

其中$$L_c^{\ast}$$是第$$c$$个肢体的PAF的ground truth，$$S_j^{\ast}$$是第$$j$$个keypoint的confidence map的ground truth，上述两个求和都是对所有像素进行的，$$p$$指的就是像素点。$$W(p)$$是图片的mask。

我们在每个stage都使用loss function，用来解决梯度消失的问题，因为每个stage结尾都有loss function，对梯度的值进行了补充。从而整体的的loss function就是$$ \mathcal{L} = \Sigma_{t=1}^{T_P} \mathcal{L}_{P}^{t} + \Sigma_{t=T_P + 1}^{T_P + T_C} \mathcal{L}_{C}^t $$

Confidence maps GT，$$S_j^{\ast}$$的计算方式是，先对于图片里的每个人$$k$$生成confidence map，$$S_{j,k}^{\ast}$$。$$x_{j,k} \in \mathbb{R}^2$$表示人$$k$$的keypoint$$j$$的ground truth。从而$$ S_{j,k}^{\ast}(p) = exp(-\lvert p-x_{j,k} \rvert^2_2 / \sigma^2) $$，其中$$\sigma$$控制峰的大小。从而整个图片（可能包含多个人）的ground truth就是：$$ S_j^{\ast}(p) = \max_k S_{j,k}^{\ast}(p) $$

Part Affinity Fieds (PAFs) GT的计算方式较为复杂一点。每一个PAF都是一个2D的vector field。对于每个肢体的PAF的每个位置的值，其都是一个2D的向量，包含了位置和这个肢体一个keypoint指向另一个keypoint的方向。预先定义好的body part keypoints对决定了哪些有哪些keypoints对构成肢体，而每个肢体都有一个PAF。

对于某张图片，其可能含有多个人，假设$$x_{j_1, k}$$和$$x_{j_2, k}$$是人$$k$$的两个keypoints，这keypoints对（有序）组成了肢体$$c$$。对于肢体$$c$$上的一点$$p$$，$$L_{c,k}^{\ast}(p)$$是一个单位向量，从$$j_1$$指向$$j_2$$，对于其它的点，$$L_{c,k}^{\ast}(p)$$的值都是0。但肢体不仅是一条直线，而是一个具有一定宽度的长方形。具体来说：$$v = (x_{j_2, k} - x_{j_1, l}) / \lvert x_{j_2, k} - x_{j_1, l} \rvert$$是从$$x_{j_1, k}$$指向$$x_{j_2, k}$$的单位向量，$$v_{\bot}$$是垂直于$$v$$的一个单位向量，而对于任意的像素点$$p$$，如果其满足$$ 0 \leq v (p - x_{j_1,k}) \leq \lVert x_{j_2, k} - x_{j_1, k} \rVert$$ 和 $$\lvert v_{\perp} (p - x_{j_1,k}) \rvert \leq \sigma_l$$，那么$$L_{c,k}^{\ast}(p) = v$$，别的像素点位置的值就是0（长度为2的0向量）。

而整个图片的PAF的ground truth是对于所有人取了均值：$$ L_c^{\ast}(p) = 1/n_c(p) \Sigma_k L_{c,k}^{\ast}(p) $$，其中$$n_c(p)$$统计的是在点$$p$$处非零向量的PAF的个数，也就是对应着有相交的肢体的情况。

有了上述confidence map和PAF的GT的定义，就可以开始网络的训练了。

而在测试过程中，对于输入的图片，对于每个keypoint，能得到一个预测的confidence map，使用non-maximum suppresion（非极大抑制）可以得到该keypoints可能出现的多个位置，这些多个位置可能对应着图片里多个人的该keypoint，但也可能是假阳性。为了进一步筛选，就需要使用预测的PAF。对于每张图片，我们也得到了$$C$$个PAF，对于每个PAF，我们计算连接该PAF对应的两个keypoints之间的线段上PAF的积分来衡量这两个keypoints是否构成了一个肢体（如果这两个keypoints按照之前的non-maximum suppression的计算，都有好几个预测值，那么对于每对都需要计算该积分）。这个积分具体来说是这样算的，对于两个检测到的keypoints，$$d_{j_1}$$和$$d_{j_2}$$：

$$ E = \int_{u=0}^{u=1} L_c(p(u)) (d_{j_2} - d_{j_1})/||d_{j_2} - d_{j_1}|| du , p(u) = (1-u) d_{j_1} + u d_{j_2}$$。

对于每个PAF对应的两个keypoints的所有预选值计算出来的上述所有的积分，如果想要找到最优的，这是个NP-hard的问题。这篇文章使用一种greedy relaxation的方法，持续性的产生高质量的匹配。

> 文章猜测这种方法有效的原因是上述计算的积分值（也就是每个肢体的score）潜在的含有global信息，因为PAF的框架具有较大的感受野。

具体来说，首先，我们获得整张图片keypoints的集合，$$D_J$$，其中$$D_J = \lbrace d_{j_m}: j \in \lbrace 1, ..., J \rbrace, m \in \lbrace 1, ..., N_j \rbrace \rbrace$$，$$N_j$$是keypoint $$j$$的候选数量，而$$d_j^m \in R^2$$是keypoint $$j$$的第$$m$$个候选位置。定义$$z_{j_1, j_2}^{m,n} \in \{0, 1\}$$来表示两个keypoint的两个候选$$d_{j_1}^m$$和$$d_{j_2}^n$$是否构成肢体。从而keypoints和肢体的候选集合为$$Z = \{z_{j_1, j_2}^{m,n}: j_1, j_2 \in \{1, ..., J\}, m \in \{1, ..., N_{j_1}\}, n \in \{1, ..., N_{j_2}\}$$，而目标函数是使得该集合对于上述描述的PAF积分的和最大。

严格来描述这个问题，对于keypoint对$$j_1$$和$$j_2$$（比如说neck和right-hip），以及它们组成的肢体$$c$$，目标是：

$$ \max\limits_{Z_c} E_c = \max\limits_{Z_c} \Sigma_{m \in D_{j_1}} \Sigma_{n \in D_{j_2}} E_{m,n} z_{j_1, j_2}^{m,n}, \quad s.t., \forall m \in D_{j_1}, \Sigma_{n \in D_{j_2}} z_{j_1, j_2}^{m, n} \leq 1, \quad \text{and} \quad \forall n \in D_{j_2}, \Sigma_{m \in D_{j_1}} z_{j_1, j_2}^{m,n} \leq 1$$

上述要优化的目标的条件，使得我们所学习到的结果里不会有两个$$c$$肢体公用同一个keypoint。而对于该优化目标$$E_c$$，可以用Hungarian算法来获取上述优化的结果。

但问题的目标$$Z$$是所有的肢体的目标$$Z_c$$的和，计算$$Z$$的最优值是一个$$K-Matching$$问题（$$K$$是肢体的数量），这个问题是个NP-hard的。对于该问题，有很多relaxations的算法存在。在这篇论文中，作者添加了两个relaxation。首先，完整的图会对于每两个不同类别的keypoint都有edge，而我们将这个图简化为其能表示人的pose的spanning tree就可以，而多余的edge就不要了。其次，我们将上述K-维的匹配问题解构为一系列二分匹配的子问题并且独立的解决这些问题，所利用的就是每个spanning tree的相邻的两个node所对应的值以及它们之间的连线，所以说是独立的。

> 第二个relaxation之所以可行，直觉上来说，spanning tree里相邻的两个node之间的关系是由预测PAF的网络学习到的，而非相邻的两个node之间的关系是由预测Keypoint的网络学习到的。

有了上述两个relaxations，我们的问题被简化为：

$$ \max\limits_{Z} E = \Sigma_{c=1}^C \max\limits_{Z_c} E_c $$

从而我们将这个优化问题分解为独立的每个pair的优化问题，而这个在之前所述，可以用Hungarian算法解决。我们再将有共同keypoint的肢体联合起来，这样其就表示出了一个完整的人的pose，或者说骨架。我们的第一个relaxation，将完整的图简化为spanning tree使得整个算法获得了很大程度的加快。


### Unsupervised 2D Keypoint Detection

#### \[**ICCV 2017**\] [Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)

[CODE](https://github.com/alldbi/Factorized-Spatial-Embeddings)

方法基于factorizing image deformations，这种deformation可能是由viewpoint变化引起的，或者是因为物体本身的形变引起的。作者提出的方法是认为deformation前后的图片的keypoints具有consistency的性质。作者还表明，用这种方法所学习到的keypoints，即使不需要加上限制条件使得同一个类别的不同图片（也就是不同物体）之间的keypoints具有对应关系，它们就已经自动对应上了。

![fse1]({{ '/assets/images/fse1.png' | relative_url }}){: width=400px style="float:center"}
*fig1. 作者提出了一个新的方法，可以在没有任何标注的情况下学习视角无关的keypoints。这个方法使用了一个viewpoint factorization的过程，其学习了一个可以进行image deformation的keypoint detector。这个模型可以用来学习rigid或者deformable objects。*

$$S \subset \mathbb R^3$$表示一个物体的3维surface，比如说一只鸟，$$\pmb x: \Lambda \rightarrow \mathbb R$$表示的是这个物体的一张图片，其中$$\Lambda \subset \mathbb R^2$$是image domain，如fig2所示。$$S$$是物体的一个固有的特性，和$$\pmb x$$是无关的。作者考虑的是学习一个函数$$q = \Phi_S(p; \pmb x)$$将$$S$$上的点$$p \in S$$映射到对应的图片$$\pmb x$$上的某个点$$q \in \Lambda$$上去。

![fse2]({{ '/assets/images/fse2.png' | relative_url }}){: width=400px style="float:center"}
*fig2. 物体结构的建模过程。。*

作者提出了一个利用viewpoint factorization来自动学习$$\Phi_S$$的方法。为了实现这个目标，我们考虑从另一个角度来看这个物体从而获得的另一张图片$$\pmb{x^{'}}$$。利用$$g$$来表示由于视角变化导致的图片歪曲，从而$$\pmb{x^{'}} \approx \pmb x \circ g$$。利用$$\Phi_S$$，我们可以将$$g$$$分解为：

$$g = \Phi_S(\dot; \pmb{x^{'}}) \circ \Phi_S(\dot; \pmb x)^{-1} \tag{1}$$

也就是说，我们将这个歪曲$$g: q \rightarrow q^{'}$$分解为先找到图片$$\pmb x$$里的点$$q$$对应在$$S$$上的点$$p=\Phi_S^{-1}(q ;\pmb x)$$，然后再找到这个点$$p$$在图片$$\pmb{x^{'}}$$上对应的点$$q^{'} = \Phi_S(p;\pmb{x^{'}})$$。

公式1表示的factorization也可以用下述等价的约束条件来表示：

$$\forall q \in S: \Phi_S(p; \pmb x \circ g) = g(\Phi_S(p; \pmb x)) \tag{2}$$

这个约束条件表明$$S$$上所检测到的点$$p$$需要随着视角的变化仍然保持一致。

为了学习这个函数$$\Phi_S$$，作者将这个函数利用一个深度神经网络来表示，并采用一种Siamese的结构，输入是$$(\pmb x, \pmb{x^{'}}, g)$$。注意到，如果我们任意给定一个物体的两个视角的图片，那么$$g$$一般是不知道的，所以说与其再利用某种方法来估计$$g$$，不如直接随机的构造出$$g$$，再利用$$g$$从$$\pmb x$$上构造出$$\pmb{x^{'}}$$。

>尽管训练的过程使用的是同一张图片和这张图片经过transformation之后得到的歪曲的图片作为训练图片对，但是训练好的模型仍然能够在同一种类物体的不同图片之间学习到keypoints的对应关系。对于那些角度变化特别大的情况，则不能保证效果，这就是future works了。


上述描述的是rigid物体的建模，也就是说$$S$$是不变的，而其也可以直接拓展到deformable的物体上去。假设$$S$$经过了某些deformation $$w$: \mathbb R^3 \rightarrow \mathbb R^3$$。从而形变后的surface就是$$S_{'} = \lbrace w(p):p \in S \rbrace$$。作者同时还提出了一个共用的surface $$S_0$$，其表示的是这个类别的object的一个标准的surface，也就是说其它的surface都可以从这个surface变形而来。将从$$S_0$$到$$S$$的deformation记为$$\pi_S$$，那么对于$$S_0$$上任意一个点$$r$$，就有$$\pi_S(r) \in S$$，而且$$\forall w: w(\pi_S(r)) = \pi_{S^{'}}(r)$$。然后，对于$$S_0$$上的点$$r$$到图片$$\pmb x$$上的点$$q$$的映射$$\Phi(r, \pmb x)$$，可以用$$\Phi_S$$来表示：$$\Phi(r; \pmb x) = \Phi_S(\pi_S(r); \pmb x)$$。从而公式2就写成：

$$\forall r \in S_0: \Phi(r; \pmb x \circ g) = g(\Phi(r; \pmb x)) \tag{3}$$

这也就是说即使是由形变造成的物体变化，变化前后所检测到的物体的keypoints也是要对应的。


除了物体的形变，上述的构造对于同一个类别的不同物体之间的差异也是可以解释的。为了实现这种解释，只需要做出一个假设：同一个种类的所有物体的surfaces对于一个category-specific surface $$S_0$$来说，都是isomorphic的，如fig2所示。

和derformable object的情况不一样，几何性质（也就是之前说的$$g$$）是不足以使得从$$S_0$$映射到不同object $$S$$的映射$$\pi_S$$能够相关联的（因为这个时候$$g$$已经无法简单的定义出来了），从而不同图片的keypoints之间的对应关系就不好建立了。然而，我们还希望这些这些$$\pi_S$$具有semantically consistent的性质，也就是还存在对应关系，比如说$$\pi_S(r)$$表示的是图片$$S$$里右眼的位置，而$$\pi_{S^{'}}(r)$$表示的也是图片$$S^{'}$$里右眼的位置。本文的一个重要的贡献就在于，如果将模型按照3.1里所说的那样进行训练，那么训练好的模型对于一个种类的不同图片，能够自动检测到semantically consistent的keypoints。


我们需要确定如何将从$$S_0$$映射到图片$$\Lambda$$的函数$$\Phi(\dot; \pmb x)$$表示为一个神经网络的输出。作者的方法是在$$K$$个离散的点采样来表示：$$\Phi(\pmb x) = (\Phi(r_1; \pmb x), \cdots, \Phi(r_K; \pmb x))$$。在这样的设定下，函数$$\Phi(\pmb x)$$就可以被理解为检测的是图片$$\pmb x$$的$$K$$个keypoints，$$p_k = \Phi(r_k; \pmb x)$$。作者并没有对这些keypoints加以别的约束。

如果$$\Phi$$是由一个神经网络所表示的，我们可以使用现有的任何一种用于keypoint detection的框架来实现它（比如HRnet，Hourglass等）。绝大多数这种框架都是预测的score maps $$\Psi(\pmb x) \in \mathbb R^{H \times W \times K}$$，对于每个keypoint $$r_k$$和图片的像素点位置$$u \in \lbrace 1, \cdots, H \rbrace \times \lbrace 1, \cdots, W \rbrace \subset \mathbb R^2$$，$$\Psi_{u r_k}(\pmb x)$$表示了一个得分。之后，可以对于每个通道，将每个score map利用softmax进行处理。从而得到了概率分布的score map。再之后，就可以利用求均值的方式求出每个keypoints的位置了。

主要的loss如下：

$$\mathcal L_{align} = \frac{1}{K} \sum_{r=1}^K \lVert \Phi(\pmb x \circ g) - g(\Phi(\pmb x)) \rVert^2 \tag{4}$$

上述的loss会使得网络学习到的keypoints都是consistent的，但并没有阻止网络所学习到的都是同一个点，也就是这些点全都聚集到了一起。

为了避免这个现象，作者加了一个diversity loss来使得这些probability maps对应到图片的不同区域。最直接的方式就是直接对两个probability相互重合的区域增加惩罚：

$$\mathcal L_{div}(\pmb x) = \frac{1}{K^2} \sum_{r=1} \sum_{r^{'}=1} \sum_u p(u \vert \pmb x, r) p(u \vert \pmb x, r^{'})$$

上述公式的一个缺点就是计算量是quadratic的。一个简便的方法是：

$$\mathcal L_{div}^{'} (\pmb x) = \sum_u (\sum_{r=1}^K p(u \vert \pmb x, r) - max_{r=1, \cdots, K} p(u \vert \pmb x, r))$$

对于一张图片$$\pmb x$$和用thin-plate splines（薄板样条插值）TPS实现的transformation $$g_1, g_2$$，网络的输入图片对为：$$g_1 \circ \pmb x$$和$$g_2 \circ (g_1 \circ \pmb x)$$。



## 3D Keypoint Detection from Images

### Supervised 3D Keypoint Detection
### Unsupervised 3D Keypoint Detection

## 3D Keypoint Detection from Point Clouds
